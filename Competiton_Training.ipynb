{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THis line tests my commitment issues\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import calendar\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('tableau-colorblind10')\n",
    "\n",
    "import sys\n",
    "sys.path.append('/data/Hydra_Work/Competition_Functions') \n",
    "from Processing_Functions import process_forecast_date, process_seasonal_forecasts\n",
    "from ML_Functions import Hydra_LSTM_Block, PinballLoss, SumPinballLoss, EarlyStopper, Model_Run\n",
    "from Data_Transforming import read_nested_csvs, generate_daily_flow, use_USGS_flow_data, USGS_to_daily_df_yearly\n",
    "# sys.path.append('/data/gbmc/Functions')\n",
    "# from Efficiency_Functions import kling_gupta_efficiency, nash_sutcliffe_efficiency, KGE_Loss, NSE_Loss, Combined_Loss\n",
    "\n",
    "\n",
    "sys.path.append('/data/Hydra_Work/Pipeline_Functions')\n",
    "from Folder_Work import filter_rows_by_year, csv_dictionary, add_day_of_year_column\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Folder_Work import extract_from_individual_zip, extract_from_main_zip, remove_redundant_folders\n",
    "# USGS = '/data/Hydra_Work/Rodeo_Data/streamflows.zip'\n",
    "# USGS_Path = '/data/Hydra_Work/Rodeo_Data/'\n",
    "# extract_from_individual_zip(USGS, USGS_Path)\n",
    "\n",
    "# extract_from_individual_zip('/data/Hydra_Work/Rodeo_Data/digital_elevation_models.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lists of  basins and their quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_basins = ['animas_r_at_durango', 'boise_r_nr_boise', 'boysen_reservoir_inflow', 'colville_r_at_kettle_falls', 'detroit_lake_inflow', 'dillon_reservoir_inflow',\n",
    "    'fontenelle_reservoir_inflow', 'green_r_bl_howard_a_hanson_dam', 'hungry_horse_reservoir_inflow', 'libby_reservoir_inflow',\n",
    "    'missouri_r_at_toston','owyhee_r_bl_owyhee_dam', 'pecos_r_nr_pecos', 'pueblo_reservoir_inflow',\n",
    "    'ruedi_reservoir_inflow', 'skagit_ross_reservoir', 'snake_r_nr_heise', 'stehekin_r_at_stehekin', 'sweetwater_r_nr_alcova',\n",
    "    'taylor_park_reservoir_inflow', 'virgin_r_at_virtin', 'weber_r_nr_oakley', 'yampa_r_nr_maybell',\n",
    "]\n",
    "\n",
    "\n",
    "USGS_basins = ['animas_r_at_durango', 'boise_r_nr_boise', 'boysen_reservoir_inflow', 'colville_r_at_kettle_falls', 'detroit_lake_inflow', 'dillon_reservoir_inflow',   \n",
    "    'green_r_bl_howard_a_hanson_dam', 'hungry_horse_reservoir_inflow', 'libby_reservoir_inflow', 'merced_river_yosemite_at_pohono_bridge', 'missouri_r_at_toston',\n",
    "    'owyhee_r_bl_owyhee_dam', 'pecos_r_nr_pecos', 'pueblo_reservoir_inflow',    'san_joaquin_river_millerton_reservoir', 'snake_r_nr_heise', 'stehekin_r_at_stehekin',\n",
    "    'sweetwater_r_nr_alcova', 'taylor_park_reservoir_inflow', 'virgin_r_at_virtin', 'weber_r_nr_oakley', 'yampa_r_nr_maybell',\n",
    "]\n",
    "\n",
    "basins = list(set(monthly_basins + USGS_basins))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Historic Quantiles\n",
    "\n",
    "quantiles = {\n",
    "    'american_river_folsom_lake': {'10': 8.25890931, '50': 16.15167713, '90': 82.37816025},\n",
    "    'animas_r_at_durango': {'10': 148.99123825, '50': 279.32773825, '90': 373.75623825},\n",
    "    'boise_r_nr_boise': {'10': 775.93723825, '50': 1033.72273825, '90': 1478.30723825},\n",
    "    'boysen_reservoir_inflow': {'10': 204.87023825, '50': 438.94273825, '90': 841.13573825},\n",
    "    'colville_r_at_kettle_falls': {'10': 57.98373825, '50': 116.11323825, '90': 187.70223825},\n",
    "    'detroit_lake_inflow': {'10': 6.86947102, '50': 13.29894623, '90': 30.09213939},\n",
    "    'dillon_reservoir_inflow': {'10': 54.85473825, '50': 117.36573825, '90': 142.10273825},\n",
    "    'fontenelle_reservoir_inflow': {'10': 286.99973825, '50': 420.65873825, '90': 828.92423825},\n",
    "    'green_r_bl_howard_a_hanson_dam': {'10': 153.56873825, '50': 233.51123825, '90': 325.42373825},\n",
    "    'hungry_horse_reservoir_inflow': {'10': 1437.45473825, '50': 1808.90123825, '90': 2254.66223825},\n",
    "    'libby_reservoir_inflow': {'10': 3174.74673825, '50': 4428.96523825, '90': 5197.77823825},\n",
    "    'missouri_r_at_toston': {'10': 851.17273825, '50': 1533.77423825, '90': 1883.71223825},\n",
    "    'owyhee_r_bl_owyhee_dam': {'10': 103.47323825, '50': 160.73323825, '90': 499.06673825},\n",
    "    'pecos_r_nr_pecos': {'10': 5.90123825, '50': 32.88623825, '90': 46.72873825},\n",
    "    'pueblo_reservoir_inflow': {'10': 123.28373825, '50': 201.71073825, '90': 297.95423825},\n",
    "    'ruedi_reservoir_inflow': {'10': 58.50173825, '50': 96.82073825, '90': 130.72273825},\n",
    "    'skagit_ross_reservoir': {'10': 928.05423825, '50': 1117.87923825, '90': 1351.08423825},\n",
    "    'snake_r_nr_heise': {'10': 1936.54673825, '50': 2373.29973825, '90': 3291.39823825},\n",
    "    'stehekin_r_at_stehekin': {'10': 492.02873825, '50': 557.46223825, '90': 658.41673825},\n",
    "    'sweetwater_r_nr_alcova': {'10': 14.10623825, '50': 30.35373825, '90': 74.62323825},\n",
    "    'taylor_park_reservoir_inflow': {'10': 39.50873825, '50': 60.36173825, '90': 85.32823825},\n",
    "    'virgin_r_at_virtin': {'10': 14.43423825, '50': 29.26573825, '90': 57.64473825},\n",
    "    'weber_r_nr_oakley': {'10': 57.79523825, '50': 83.71873825, '90': 115.95923825},\n",
    "    'yampa_r_nr_maybell': {'10': 420.85673825, '50': 802.86223825, '90': 1050.70273825},\n",
    "    'san_joaquin_river_millerton_reservoir': {'10': 8.25890931, '50': 16.15167713, '90': 82.37816025},\n",
    "    'merced_river_yosemite_at_pohono_bridge': {'10': 6.1894047, '50': 16.65905307, '90': 33.48977683},\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Data\n",
    "selected_years = range(2000,2024,2)\n",
    "\n",
    "era5_folder = '/data/Hydra_Work/Rodeo_Data/era5'\n",
    "era5 = csv_dictionary(era5_folder, basins, years=selected_years)\n",
    "era5 = add_day_of_year_column(era5)\n",
    "\n",
    "flow_folder = '/data/Hydra_Work/Rodeo_Data/train_monthly_naturalized_flow'\n",
    "flow = csv_dictionary(flow_folder, monthly_basins)\n",
    "flow = filter_rows_by_year(flow, 1998)\n",
    "\n",
    "climatology_file_path = '/data/Hydra_Work/Rodeo_Data/climate_indices.csv'\n",
    "climate_indices = pd.read_csv(climatology_file_path)\n",
    "climate_indices['date'] = pd.to_datetime(climate_indices['date'])\n",
    "climate_indices.set_index('date', inplace = True)\n",
    "climate_indices.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "climate_indices = climate_indices[~climate_indices.index.duplicated(keep='first')]\n",
    "\n",
    "root_folder = '/data/Hydra_Work/Rodeo_Data/seasonal_forecasts'\n",
    "seasonal_forecasts = read_nested_csvs(root_folder)\n",
    "\n",
    "USGS_flow_folder = '/data/Hydra_Work/Rodeo_Data/USGS_streamflows'\n",
    "USGS_flow = csv_dictionary(USGS_flow_folder, USGS_basins)\n",
    "\n",
    "Static_variables = pd.read_csv('/data/Hydra_Work/Rodeo_Data/static_indices.csv', index_col= 'site_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Flow Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert monthly flow values to daily flow estimates\n",
    "daily_flow = {}\n",
    "\n",
    "# Iterate through the dictionary and apply generate_daily_flow to each DataFrame\n",
    "for key, df in flow.items():\n",
    "    daily_flow[key] = generate_daily_flow(df, persistence_factor=0.7)\n",
    "\n",
    "# Replacing monhtly data for normalised USGS when available\n",
    "daily_flow = use_USGS_flow_data(daily_flow, USGS_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introducing the data from San_jaoqin and Merced, normalised by the yearly flow given\n",
    "path = '/data/Hydra_Work/Rodeo_Data/USGS_streamflows/san_joaquin_river_millerton_reservoir.csv'\n",
    "name = 'san_joaquin_river_millerton_reservoir'\n",
    "normalising_path = '/data/Hydra_Work/Rodeo_Data/train_yearly/san_joaquin_river_millerton_reservoir.csv'\n",
    "\n",
    "USGS_to_daily_df_yearly(daily_flow, path, name, normalising_path)\n",
    "\n",
    "path = '/data/Hydra_Work/Rodeo_Data/USGS_streamflows/merced_river_yosemite_at_pohono_bridge.csv'\n",
    "name = 'merced_river_yosemite_at_pohono_bridge'\n",
    "normalising_path = '/data/Hydra_Work/Rodeo_Data/train_yearly/merced_river_yosemite_at_pohono_bridge.csv'\n",
    "\n",
    "USGS_to_daily_df_yearly(daily_flow, path, name, normalising_path)\n",
    "\n",
    "path = '/data/Hydra_Work/Rodeo_Data/USGS_streamflows/detroit_lake_inflow.csv'\n",
    "name = 'detroit_lake_inflow'\n",
    "normalising_path = '/data/Hydra_Work/Rodeo_Data/train_yearly/detroit_lake_inflow.csv'\n",
    "\n",
    "USGS_to_daily_df_yearly(daily_flow, path, name, normalising_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detroit and sweetwater train monthly are fucked\n",
    "for key, df in daily_flow.items():\n",
    "    length = len(df)\n",
    "    nan_count = df['daily_flow'].isna().sum()\n",
    "    \n",
    "    # Check if the DataFrame is not empty\n",
    "    if not df.empty:\n",
    "        latest_date = df.index.max()\n",
    "        print(f\"{key}: Length = {length}, NaN count in 'daily_flow' = {nan_count}, Latest Date = {latest_date}\")\n",
    "    else:\n",
    "        print(f\"{key}: DataFrame is empty, Length = {length}, NaN count in 'daily_flow' = {nan_count}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "Static = pd.read_csv('/data/Hydra_Work/Rodeo_Data/static_indices.csv')\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(Static)\n",
    "scaled_dataframe = pd.DataFrame(scaled_data, columns=Static.columns)\n",
    "joblib.dump(scaler, f'/data/Hydra_Work/Rodeo_Data/scalers/static_scaler.save')\n",
    "\n",
    "# for basin, dataframe in daily_flow.items(): \n",
    "#     scaler = StandardScaler()\n",
    "#     scaled_data = scaler.fit_transform(dataframe)\n",
    "#     scaled_dataframe = pd.DataFrame(scaled_data, columns=dataframe.columns)\n",
    "#     joblib.dump(scaler, f'/data/Hydra_Work/Rodeo_Data/scalers/flows/{basin}_flow_scaler.save')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Variable Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_scaler_filename = '/data/Hydra_Work/Rodeo_Data/scalers/climate_normalization_scaler.save'\n",
    "climate_scaler = joblib.load(climate_scaler_filename) \n",
    "climate_indices = pd.DataFrame(climate_scaler.transform(climate_indices), columns=climate_indices.columns, index=climate_indices.index)\n",
    "\n",
    "era5_scaler_filename = '/data/Hydra_Work/Rodeo_Data/scalers/era5_scaler.save'\n",
    "era5_scaler = joblib.load(era5_scaler_filename) \n",
    "era5 = {key: pd.DataFrame(era5_scaler.transform(df), columns=df.columns, index=df.index) for key, df in era5.items()}\n",
    "\n",
    "for basin, df in daily_flow.items(): \n",
    "    flow_scaler_filename = f'/data/Hydra_Work/Rodeo_Data/scalers/flows/{basin}_flow_scaler.save'\n",
    "    flow_scaler = joblib.load(flow_scaler_filename) \n",
    "    daily_flow[basin] = pd.DataFrame(flow_scaler.transform(df), columns=df.columns, index=df.index)\n",
    "\n",
    "seasonal_scaler_filename = \"/data/Hydra_Work/Rodeo_Data/scalers/seasonal_scaler.save\"\n",
    "seasonl_scaler = joblib.load(seasonal_scaler_filename)\n",
    "seasonal_forecasts = {key: pd.DataFrame(seasonl_scaler.transform(df), columns=df.columns, index=df.index ) for key, df in seasonal_forecasts.items()}\n",
    "\n",
    "static_scaler_filename = '/data/Hydra_Work/Rodeo_Data/scalers/static_scaler.save'\n",
    "static_scaler = joblib.load(static_scaler_filename) \n",
    "Static_variables = pd.DataFrame(static_scaler.transform(Static_variables), columns=Static_variables.columns, index=Static_variables.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scaled climatology for each day\n",
    "\n",
    "climatological_flows = {}\n",
    "\n",
    "for basin, df in daily_flow.items():\n",
    "    # Extract day of year and flow values\n",
    "    df['day_of_year'] = df.index.dayofyear\n",
    "\n",
    "    grouped = df.groupby('day_of_year')['daily_flow'].quantile([0.1, 0.5, 0.9]).unstack(level=1)\n",
    "\n",
    "    climatological_flows[basin] = pd.DataFrame({\n",
    "        'day_of_year': grouped.index,\n",
    "        '10th_percentile_flow': grouped[0.1],\n",
    "        '50th_percentile_flow': grouped[0.5],\n",
    "        '90th_percentile_flow': grouped[0.9]\n",
    "    })\n",
    "    \n",
    "    climatological_flows[basin].set_index('day_of_year', inplace=True)\n",
    "\n",
    "    # Drop the temporary 'day_of_year' column from the original dataframe\n",
    "    df.drop(columns='day_of_year', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/data/Hydra_Work/Rodeo_Data/climatological_flows'\n",
    "\n",
    "\n",
    "for basin, df in climatological_flows.items():\n",
    "    csv_name = f'{basin}_climatological_flows.csv'\n",
    "    csv_path = os.path.join(folder_path, csv_name)\n",
    "    df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_quantiles = {}\n",
    "\n",
    "# Iterate through the dictionary and scale each value\n",
    "for key, inner_dict in quantiles.items():\n",
    "    values = list(inner_dict.values())    \n",
    "    values = [[value] for value in values]\n",
    "    scaled_values = flow_scaler.fit_transform(values)\n",
    "    scaled_values = [value[0] for value in scaled_values]\n",
    "    scaled_inner_dict = dict(zip(inner_dict.keys(), scaled_values))\n",
    "    scaled_quantiles[key] = scaled_inner_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for basin, dataframe in daily_flow.items(): \n",
    "#     print(np.mean(dataframe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('/data/Hydra_Work/Performance_Evaluation') \n",
    "from Evaluation_Functions import test_performance_for_basin_and_season\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_models_optimizers(basins, body_input_size, body_hidden_size, body_num_layers, body_output_size, body_dropout, body_bidirectional,\n",
    "                                 head_input_size, head_hidden_size, head_num_layers, head_output_size, head_dropout, head_bidirectional,\n",
    "                                 days, hidden_variables_size, learning_rate_body, learning_rate_head, learning_rate_general_head, LR, device):\n",
    "    \n",
    "    Hydra_Body = Hydra_LSTM_Block(body_input_size, body_hidden_size, body_num_layers, body_output_size, H0_sequences_size=days * (hidden_variables_size - 1), dropout=body_dropout, bidirectional=body_bidirectional)\n",
    "    Hydra_Body.to(device)\n",
    "\n",
    "    model_heads = {}\n",
    "    for basin in basins:\n",
    "        model_heads[f'{basin}'] = Hydra_LSTM_Block(head_input_size, head_hidden_size, head_num_layers, head_output_size, H0_sequences_size=days * hidden_variables_size, dropout=head_dropout, bidirectional=head_bidirectional)\n",
    "        model_heads[f'{basin}'].to(device)\n",
    "\n",
    "    General_Hydra_Head = Hydra_LSTM_Block(head_input_size, head_hidden_size, head_num_layers, head_output_size, H0_sequences_size=days * (hidden_variables_size - 1), dropout=head_dropout, bidirectional=head_bidirectional)\n",
    "    General_Hydra_Head.to(device)\n",
    "\n",
    "    params_to_optimize = list(General_Hydra_Head.parameters()) + list(Hydra_Body.parameters())\n",
    "\n",
    "    # Add parameters from specific heads\n",
    "    for basin, model in model_heads.items():\n",
    "        params_to_optimize += list(model.parameters())\n",
    "\n",
    "    optimizer = torch.optim.Adam(params_to_optimize, lr=LR, weight_decay = 1e-1)\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=1e4)\n",
    "\n",
    "    return Hydra_Body, model_heads, General_Hydra_Head, optimizer, scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight between body and head is unclear, how they should be balanced\n",
    "\n",
    "LR = 1e-4\n",
    "static_size = np.shape(Static_variables)[1]\n",
    "forecast_size = np.shape(seasonal_forecasts['american_river_folsom_lake_2000_apr'])[1]\n",
    "History_Fourier_in_forcings = 0 #2*3*(6 - 1)\n",
    "Climate_guess = 3\n",
    "History_Statistics_in_forcings = 5*2\n",
    "body_input_size = forecast_size + static_size + History_Fourier_in_forcings + History_Statistics_in_forcings  + Climate_guess + 3\n",
    "body_output_size, body_hidden_size, body_num_layers =  16, 64, 2\n",
    "body_dropout, body_bidirectional, learning_rate_body = 0.5, True, LR\n",
    "head_input_size, head_output_size, head_hidden_size, head_num_layers =  body_output_size , 3, 64, 1\n",
    "head_dropout, head_bidirectional, learning_rate_head, learning_rate_general_head = 0.1, True, LR, LR\n",
    "\n",
    "days  = 90\n",
    "hidden_variables_size = 17\n",
    "\n",
    "# Randomness\n",
    "seed = 42 ; torch.manual_seed(seed) ; random.seed(seed) ; np.random.seed(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "Hydra_Body, model_heads, General_Hydra_Head, optimizer, scheduler = initialize_models_optimizers(basins, body_input_size, body_hidden_size, body_num_layers, body_output_size, body_dropout, body_bidirectional,\n",
    "                    head_input_size, head_hidden_size, head_num_layers, head_output_size, head_dropout, head_bidirectional,\n",
    "                    days, hidden_variables_size, learning_rate_body, learning_rate_head, learning_rate_general_head, LR, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probably SumPinballLoss initially\n",
    "criterion = SumPinballLoss(quantiles = [0.1, 0.5, 0.9])\n",
    "\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max= 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First build list of all possible forecast dates, can use any random basin to initialise this\n",
    "basin = 'animas_r_at_durango' \n",
    "All_Dates = daily_flow[basin].index[\n",
    "    ((daily_flow[basin].index.month < 6) | ((daily_flow[basin].index.month == 6) & (daily_flow[basin].index.day < 25))) &\n",
    "    ((daily_flow[basin].index.year % 2 == 0) | ((daily_flow[basin].index.month > 10) | ((daily_flow[basin].index.month == 10) & (daily_flow[basin].index.day >= 1))))\n",
    "]\n",
    "All_Dates = All_Dates[All_Dates.year > 1998]\n",
    "\n",
    "\n",
    "# Validation Year\n",
    "Val_Dates = All_Dates[All_Dates.year == 2022]\n",
    "All_Dates = All_Dates[All_Dates.year < 2022]\n",
    "\n",
    "\n",
    "basin_to_remove = 'sweetwater_r_nr_alcova'\n",
    "\n",
    "if basin_to_remove in basins:\n",
    "    basins.remove(basin_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-2\n",
    "# Add parameters from specific heads\n",
    "params_to_optimize = list([])#list(General_Hydra_Head.parameters()) # + list(Hydra_Body.parameters())\n",
    "\n",
    "for basin, model in model_heads.items():\n",
    "    params_to_optimize += list(model.parameters())\n",
    "\n",
    "optimizer = torch.optim.Adam(params_to_optimize, lr=LR, weight_decay = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs, batch_size = 30, 1\n",
    "group_lengths = [7, 10, 20, 30, 50, 60, 70, 120, 150]\n",
    "group_lengths = [10, 20, 30,110, 120, 150]\n",
    "group_lengths = np.arange(180)\n",
    "Train_Mode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopper = None\n",
    "#Basin,Climate = Model_Run(All_Dates, basins, Hydra_Body, General_Hydra_Head, model_heads, era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts, Static_variables, optimizer, scheduler, criterion, early_stopper, n_epochs , batch_size, group_lengths, Train_Mode, device, feed_forcing= False)\n",
    "\n",
    "# params_to_optimize =  list(Hydra_Body.parameters()) \n",
    "params_to_optimize = list(General_Hydra_Head.parameters()) \n",
    "\n",
    "# Add parameters from specific heads\n",
    "for basin, model in model_heads.items():\n",
    "    params_to_optimize += list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Loop\n",
    "# Work on owyhee, colville, dillon, animas\n",
    "total_epochs = n_epochs\n",
    "feed_forcing = False\n",
    "early_stopper = EarlyStopper(patience=10, min_delta=0.01)\n",
    "daily_flow['dillon_reservoir_inflow']\n",
    "#'dillon_reservoir_inflow',\n",
    "train_basins = ['dillon_reservoir_inflow']\n",
    "general_losses, specific_losses, val_general_losses, val_specific_losses, climate_losses = [[] for _ in range(5)]\n",
    "for epoch in range(total_epochs):  # Use 'range' to iterate over epochs\n",
    "    # Training\n",
    "    print('True Epoch is', epoch)\n",
    "    train_general_loss, train_specific_loss, climate_loss = Model_Run(All_Dates, train_basins, Hydra_Body, General_Hydra_Head, model_heads, era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts, Static_variables, optimizer, scheduler, criterion, n_epochs=1, batch_size=batch_size, group_lengths=group_lengths, Train_Mode=True, device=device, feed_forcing = feed_forcing)\n",
    "    general_losses.append(train_general_loss)\n",
    "    specific_losses.append(train_specific_loss)\n",
    "\n",
    "    # Validation\n",
    "    val_general_loss, val_specific_loss, climate_loss = Model_Run(Val_Dates, basins, Hydra_Body, General_Hydra_Head, model_heads, era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts, Static_variables, optimizer, scheduler, criterion, early_stopper = None,n_epochs=1, batch_size=batch_size, group_lengths=group_lengths, Train_Mode=False, device=device, feed_forcing = feed_forcing)\n",
    "    val_general_losses.append(val_general_loss)\n",
    "    val_specific_losses.append(val_specific_loss)\n",
    "    climate_losses.append(climate_loss)\n",
    "\n",
    "    if early_stopper.early_stop(0.5*(np.array(val_specific_loss) + np.array(val_general_loss))):\n",
    "        stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_season_date = '2022-02-01'\n",
    "end_season_date = '2022-05-31'\n",
    "years = [2000,2002,2004,2006,2008,2010,2012,2014,2016,2018,2020, 2022]\n",
    "year = 2022\n",
    "daily_flow['dillon_reservoir_inflow']\n",
    "for basin in basins:\n",
    "#for year in years:\n",
    "    # start_season_date = f'{year}-02-01'\n",
    "    # end_season_date = f'{year}-05-31'\n",
    "\n",
    "    #basin = 'dillon_reservoir_inflow'\n",
    "\n",
    "    flow_scaler_filename = f'/data/Hydra_Work/Rodeo_Data/scalers/flows/{basin}_flow_scaler.save'\n",
    "    flow_scaler = joblib.load(flow_scaler_filename) \n",
    "\n",
    "    Specific, General, Truth, Climatology_10, Climatology_50, Climatology_90  = test_performance_for_basin_and_season(basin, Hydra_Body, General_Hydra_Head, model_heads, era5, seasonal_forecasts, daily_flow, climatological_flows, climate_indices, Static_variables, device, end_season_date, start_season_date, flow_scaler, furthest_distance=120, feed_forcing= feed_forcing)\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Plot for Specific\n",
    "    No_Days = len(Specific[:, 0])\n",
    "    Days_Til_Season = No_Days - np.arange(1, No_Days + 1)\n",
    "\n",
    "    axes[0].plot(Truth, label='Truth')\n",
    "    axes[0].plot(Days_Til_Season, Specific[:, 0], label='10 Percentile Specific')\n",
    "    axes[0].plot(Days_Til_Season, Specific[:, 1], label='50 Percentile Specific')\n",
    "    axes[0].plot(Days_Til_Season, Specific[:, 2], label='90 Percentile Specific')\n",
    "    # axes[0].plot(Days_Til_Season, Climatology_10, label='10 Percentile Climatology')\n",
    "    # axes[0].plot(Days_Til_Season, Climatology_50, label='50 Percentile Climatology')\n",
    "    # axes[0].plot(Days_Til_Season, Climatology_90, label='90 Percentile Climatology')\n",
    "\n",
    "    axes[0].legend()\n",
    "    axes[0].set_title('Specific Percentiles')\n",
    "\n",
    "    # Plot for General\n",
    "    axes[1].plot(Truth, label='Truth')\n",
    "    axes[1].plot(Days_Til_Season, General[:, 0], label='10 Percentile General')\n",
    "    axes[1].plot(Days_Til_Season, General[:, 1], label='50 Percentile General')\n",
    "    axes[1].plot(Days_Til_Season, General[:, 2], label='90 Percentile General')\n",
    "\n",
    "\n",
    "    axes[1].legend()\n",
    "    axes[1].set_title('General Percentiles')\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_ylabel('Seasonal Discharge ($m^3/s$)')\n",
    "        ax.invert_xaxis()\n",
    "        ax.set_xlabel('Days Till End of Forecast')\n",
    "\n",
    "\n",
    "\n",
    "    plt.suptitle(f'Model performance at {basin} in year {year}', fontsize=16)\n",
    "\n",
    "    Clmiatology = True\n",
    "    # Need to make code to get climatology of the season of interest\n",
    "    if Clmiatology == True:\n",
    "        # Add horizontal dashed lines for quantiles\n",
    "        axes[0].plot(Days_Til_Season, Climatology_10, linestyle='dashed', color='gray', label=f'10 Climatology')\n",
    "        axes[0].plot(Days_Til_Season, Climatology_50, linestyle='dashed', color='gray', label=f'50 Climatology')\n",
    "        axes[0].plot(Days_Til_Season, Climatology_90, linestyle='dashed', color='gray', label=f'90 Climatology')\n",
    "\n",
    "        axes[1].plot(Days_Til_Season,Climatology_10, linestyle='dashed', color='gray', label=f'10 Climatology')\n",
    "        axes[1].plot(Days_Til_Season,Climatology_50, linestyle='dashed', color='gray', label=f'50 Climatology')\n",
    "        axes[1].plot(Days_Til_Season,Climatology_90, linestyle='dashed', color='gray', label=f'90 Climatology')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signed off on: Hungry Horse, Colville, Pueblo, ruedi, dillon, boise, missouri, libby, pecos, virgin, yampa, merced, animas, detroit\n",
    "# not great but improved: snake, taylor, owyhee, fontenelle\n",
    "model_dir = '/data/Hydra_Work/Models/10_01_Models/'\n",
    "daily_flow['dillon_reservoir_inflow']\n",
    "basin = 'dillon_reservoir_inflow'\n",
    "\n",
    "# Construct the full path for saving the model\n",
    "save_path = os.path.join(model_dir, f'{basin}_Head.pth')\n",
    "# Save the model \n",
    "torch.save(model_heads[f'{basin}'], save_path)\n",
    "\n",
    "\n",
    "# save_path = os.path.join(model_dir, f'General_Head.pth')\n",
    "# # Save the model\n",
    "# torch.save(General_Hydra_Head, save_path)\n",
    "\n",
    "# save_path = os.path.join(model_dir, f'General_Body.pth')\n",
    "# # Save the model\n",
    "# torch.save(Hydra_Body, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of indices as the x-values\n",
    "indices = np.arange(len(general_losses))\n",
    "skip = 0\n",
    "# Fit linear regression models\n",
    "general_coef = np.polyfit(indices[skip:], val_general_losses[skip:], 1)\n",
    "gen_line_of_best_fit = np.polyval(general_coef[skip:], indices[skip:])\n",
    "specific_coef = np.polyfit(indices[skip:], val_specific_losses[skip:], 1)\n",
    "spec_line_of_best_fit = np.polyval(specific_coef[skip:], indices[skip:])\n",
    "\n",
    "# Plot the loss values\n",
    "plt.plot(val_general_losses[skip:], label='General Losses', color='blue')\n",
    "plt.plot(val_specific_losses[skip:], label='Specific Losses', color='green')\n",
    "plt.plot(climate_losses[skip:], label='Climate Losses', linestyle = '--', color='grey')\n",
    "# Plot the lines of best fit\n",
    "plt.plot(gen_line_of_best_fit, label='General Line of Best Fit', linestyle='--', color='red')\n",
    "plt.plot(spec_line_of_best_fit, label='Specific Line of Best Fit', linestyle='--', color='orange')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "# 500 vs 1500 currently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of indices as the x-values\n",
    "indices = np.arange(len(general_losses))\n",
    "plt.plot( ( ( np.array(climate_losses[skip:]) - np.array(val_general_losses[skip:]) )/np.array(climate_losses[skip:]) ), label='General Losses', color='blue')\n",
    "plt.plot( ( ( np.array(climate_losses[skip:]) - np.array(val_specific_losses[skip:]) )/np.array(climate_losses[skip:]) ), label='Specific Losses', color='green')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "# 500 vs 1500 currently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking distribution of basins and days used in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Basin_Count(All_Dates, basins, Hydra_Body, General_Hydra_Head, model_heads, era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts, static_indices, optimizer, scheduler, criterion, early_stopper = None, n_epochs = 20, batch_size = 2, group_lengths = [89, 90, 91, 92] , Train_Mode=True, device = 'cpu', feed_forcing = True):\n",
    "    basin_usage_counter = defaultdict(int)\n",
    "\n",
    "    Size = len(All_Dates)\n",
    "\n",
    "    # Set models to train mode if Train_Mode is True, else set to evaluation mode\n",
    "    Hydra_Body.train(Train_Mode)\n",
    "    General_Hydra_Head.train(Train_Mode)\n",
    "    [model_heads[f'{basin}'].train(Train_Mode) for basin in basins]\n",
    "\n",
    "    for epoch in range(n_epochs):  \n",
    "        permutation = torch.randperm(len(All_Dates))\n",
    "\n",
    "        for i in range(0, Size, batch_size):\n",
    "            indices = permutation[i:i + batch_size]\n",
    "            batch_dates = All_Dates[indices.cpu().numpy().astype(int)]\n",
    "            basin = np.random.choice(basins)\n",
    "\n",
    "            min_day = min(180 - forecast_datetime.dayofyear for forecast_datetime in batch_dates) + 1\n",
    "            final_forcing_distance = np.random.choice(range(round(0.49*min_day), min_day + 1) )\n",
    "            final_forcing_distance = min_day\n",
    "\n",
    "            basin_key = f\"{basin}_{final_forcing_distance}\"\n",
    "            basin_usage_counter[basin_key] += 1\n",
    "        \n",
    "        return basin_usage_counter\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract basins and distances from keys\n",
    "n_epochs = 3000\n",
    "basin_usage_counter = Basin_Count(All_Dates, basins, Hydra_Body, General_Hydra_Head, model_heads, era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts, Static_variables, optimizer, scheduler, criterion, n_epochs=n_epochs, batch_size= 1, group_lengths=group_lengths, Train_Mode=True, device=device, feed_forcing = False)\n",
    "\n",
    "basin_info = [key.rsplit('_', 1) for key in basin_usage_counter.keys()]\n",
    "\n",
    "# Convert to numeric values if needed\n",
    "basins, distances = zip(*[(info[0], int(info[1])) for info in basin_info])\n",
    "\n",
    "# Get unique basins and distances\n",
    "unique_basins = list(set(basins))\n",
    "unique_distances = list(set(distances))\n",
    "\n",
    "# Create a matrix of counts\n",
    "counts_matrix = [\n",
    "    [basin_usage_counter[f\"{basin}_{distance}\"] for distance in unique_distances] for basin in unique_basins\n",
    "]\n",
    "\n",
    "# Create a heatmap using Matplotlib\n",
    "plt.imshow(counts_matrix, cmap=\"YlGnBu\", interpolation=\"nearest\", aspect=\"auto\")\n",
    "plt.colorbar(label=\"Counts\")\n",
    "plt.yticks(range(len(unique_basins)), unique_basins)\n",
    "plt.xlabel('Final Forcing Distance')\n",
    "plt.ylabel('Basin')\n",
    "plt.title('Basin and Final Forcing Distance Usage Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# green pueblo colvilli\n",
    "# green need works, so does pueblo in their issues when getting close\n",
    "\n",
    "year = 2012\n",
    "start_season_date = f'{year}-01-31' \n",
    "end_season_date = f'{year}-05-31'\n",
    "flow['pueblo_reservoir_inflow']\n",
    "basin = 'pueblo_reservoir_inflow'\n",
    "\n",
    "flow_scaler_filename = f'/data/Hydra_Work/Rodeo_Data/scalers/flows/{basin}_flow_scaler.save'\n",
    "flow_scaler = joblib.load(flow_scaler_filename) \n",
    "\n",
    "Specific, General, Truth, Climatology_10, Climatology_50, Climatology_90  = test_performance_for_basin_and_season(basin, Hydra_Body, General_Hydra_Head, model_heads, era5, seasonal_forecasts, daily_flow, climatological_flows, climate_indices, Static_variables, device, end_season_date, start_season_date, flow_scaler, furthest_distance=120, feed_forcing= feed_forcing)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot for Specific\n",
    "No_Days = len(Specific[:, 0])\n",
    "Days_Til_Season = No_Days - np.arange(1, No_Days + 1)\n",
    "\n",
    "axes[0].plot(Truth, label='Truth')\n",
    "axes[0].plot(Days_Til_Season, Specific[:, 0], label='10 Percentile Specific')\n",
    "axes[0].plot(Days_Til_Season, Specific[:, 1], label='50 Percentile Specific')\n",
    "axes[0].plot(Days_Til_Season, Specific[:, 2], label='90 Percentile Specific')\n",
    "# axes[0].plot(Days_Til_Season, Climatology_10, label='10 Percentile Climatology')\n",
    "# axes[0].plot(Days_Til_Season, Climatology_50, label='50 Percentile Climatology')\n",
    "# axes[0].plot(Days_Til_Season, Climatology_90, label='90 Percentile Climatology')\n",
    "\n",
    "axes[0].legend()\n",
    "axes[0].set_title('Specific Percentiles')\n",
    "\n",
    "# Plot for General\n",
    "axes[1].plot(Truth, label='Truth')\n",
    "axes[1].plot(Days_Til_Season, General[:, 0], label='10 Percentile General')\n",
    "axes[1].plot(Days_Til_Season, General[:, 1], label='50 Percentile General')\n",
    "axes[1].plot(Days_Til_Season, General[:, 2], label='90 Percentile General')\n",
    "\n",
    "\n",
    "axes[1].legend()\n",
    "axes[1].set_title('General Percentiles')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_ylabel('Seasonal Discharge ($m^3/s$)')\n",
    "    ax.invert_xaxis()\n",
    "    ax.set_xlabel('Days Till End of Forecast')\n",
    "\n",
    "\n",
    "\n",
    "plt.suptitle(f'Model performance at {basin}', fontsize=16)\n",
    "\n",
    "Clmiatology = True\n",
    "# Need to make code to get climatology of the season of interest\n",
    "if Clmiatology == True:\n",
    "    # Add horizontal dashed lines for quantiles\n",
    "    axes[0].plot(Days_Til_Season, Climatology_10, linestyle='dashed', color='gray', label=f'10 Climatology')\n",
    "    axes[0].plot(Days_Til_Season, Climatology_50, linestyle='dashed', color='gray', label=f'50 Climatology')\n",
    "    axes[0].plot(Days_Til_Season, Climatology_90, linestyle='dashed', color='gray', label=f'90 Climatology')\n",
    "\n",
    "    axes[1].plot(Days_Til_Season,Climatology_10, linestyle='dashed', color='gray', label=f'10 Climatology')\n",
    "    axes[1].plot(Days_Til_Season,Climatology_50, linestyle='dashed', color='gray', label=f'50 Climatology')\n",
    "    axes[1].plot(Days_Til_Season,Climatology_90, linestyle='dashed', color='gray', label=f'90 Climatology')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all models to a model path\n",
    "\n",
    "# model_dir = '/data/Hydra_Work/Models/10_01_Models/'\n",
    "\n",
    "# for basin in basins:\n",
    "#     # Construct the full path for saving the model\n",
    "#     save_path = os.path.join(model_dir, f'{basin}_Head.pth')\n",
    "#     # Save the model \n",
    "#     torch.save(model_heads[f'{basin}'], save_path)\n",
    "\n",
    "\n",
    "# save_path = os.path.join(model_dir, f'General_Head.pth')\n",
    "# # Save the model\n",
    "# torch.save(General_Hydra_Head, save_path)\n",
    "\n",
    "# save_path = os.path.join(model_dir, f'General_Body.pth')\n",
    "# # Save the model\n",
    "# torch.save(Hydra_Body, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rodeo_V2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
