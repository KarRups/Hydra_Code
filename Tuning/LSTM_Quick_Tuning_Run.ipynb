{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('tableau-colorblind10')\n",
    "\n",
    "import sys\n",
    "sys.path.append('/data/Hydra_Work/Competition_Functions') \n",
    "from Processing_Functions import process_forecast_date, process_seasonal_forecasts\n",
    "from Data_Transforming import read_nested_csvs, generate_daily_flow, use_USGS_flow_data, USGS_to_daily_df_yearly\n",
    "\n",
    "sys.path.append('/data/Hydra_Work/Pipeline_Functions')\n",
    "from Folder_Work import filter_rows_by_year, csv_dictionary, add_day_of_year_column\n",
    "\n",
    "sys.path.append('/data/Hydra_Work/Post_Rodeo_Work/ML_Functions.py')\n",
    "from Full_LSTM_ML_Functions import Specific_Heads, Google_Model_Block, SumPinballLoss, EarlyStopper, Model_Run, No_Body_Model_Run\n",
    "\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the prep\n",
    "monthly_basins = ['animas_r_at_durango', 'boise_r_nr_boise', 'boysen_reservoir_inflow', 'colville_r_at_kettle_falls', 'detroit_lake_inflow', 'dillon_reservoir_inflow',\n",
    "    'fontenelle_reservoir_inflow', 'green_r_bl_howard_a_hanson_dam', 'hungry_horse_reservoir_inflow', 'libby_reservoir_inflow',\n",
    "    'missouri_r_at_toston','owyhee_r_bl_owyhee_dam', 'pecos_r_nr_pecos', 'pueblo_reservoir_inflow',\n",
    "    'ruedi_reservoir_inflow', 'skagit_ross_reservoir', 'snake_r_nr_heise', 'stehekin_r_at_stehekin', 'sweetwater_r_nr_alcova',\n",
    "    'taylor_park_reservoir_inflow', 'virgin_r_at_virtin', 'weber_r_nr_oakley', 'yampa_r_nr_maybell',\n",
    "]\n",
    "\n",
    "\n",
    "USGS_basins = ['animas_r_at_durango', 'boise_r_nr_boise', 'boysen_reservoir_inflow', 'colville_r_at_kettle_falls', 'detroit_lake_inflow', 'dillon_reservoir_inflow',   \n",
    "    'green_r_bl_howard_a_hanson_dam', 'hungry_horse_reservoir_inflow', 'libby_reservoir_inflow', 'merced_river_yosemite_at_pohono_bridge', 'missouri_r_at_toston',\n",
    "    'owyhee_r_bl_owyhee_dam', 'pecos_r_nr_pecos', 'pueblo_reservoir_inflow',    'san_joaquin_river_millerton_reservoir', 'snake_r_nr_heise', 'stehekin_r_at_stehekin',\n",
    "    'sweetwater_r_nr_alcova', 'taylor_park_reservoir_inflow', 'virgin_r_at_virtin', 'weber_r_nr_oakley', 'yampa_r_nr_maybell',\n",
    "]\n",
    "\n",
    "basins = list(set(monthly_basins + USGS_basins))\n",
    "\n",
    "\n",
    "selected_years = range(2000,2024,2)\n",
    "\n",
    "era5_folder = '/data/Hydra_Work/Rodeo_Data/era5'\n",
    "era5 = csv_dictionary(era5_folder, basins, years=selected_years)\n",
    "era5 = add_day_of_year_column(era5)\n",
    "\n",
    "flow_folder = '/data/Hydra_Work/Rodeo_Data/train_monthly_naturalized_flow'\n",
    "flow = csv_dictionary(flow_folder, monthly_basins)\n",
    "flow = filter_rows_by_year(flow, 1998)\n",
    "\n",
    "climatology_file_path = '/data/Hydra_Work/Rodeo_Data/climate_indices.csv'\n",
    "climate_indices = pd.read_csv(climatology_file_path)\n",
    "climate_indices['date'] = pd.to_datetime(climate_indices['date'])\n",
    "climate_indices.set_index('date', inplace = True)\n",
    "climate_indices.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "climate_indices = climate_indices[~climate_indices.index.duplicated(keep='first')]\n",
    "\n",
    "root_folder = '/data/Hydra_Work/Rodeo_Data/seasonal_forecasts'\n",
    "seasonal_forecasts = read_nested_csvs(root_folder)\n",
    "\n",
    "USGS_flow_folder = '/data/Hydra_Work/Rodeo_Data/USGS_streamflows'\n",
    "USGS_flow = csv_dictionary(USGS_flow_folder, USGS_basins)\n",
    "\n",
    "Static_variables = pd.read_csv('/data/Hydra_Work/Rodeo_Data/static_indices.csv', index_col= 'site_id')\n",
    "\n",
    "# Convert monthly flow values to daily flow estimates\n",
    "daily_flow = {}\n",
    "\n",
    "# Iterate through the dictionary and apply generate_daily_flow to each DataFrame\n",
    "for key, df in flow.items():\n",
    "    daily_flow[key] = generate_daily_flow(df, persistence_factor=0.7)\n",
    "\n",
    "# Replacing monhtly data for normalised USGS when available\n",
    "daily_flow = use_USGS_flow_data(daily_flow, USGS_flow)\n",
    "\n",
    "\n",
    "normalising_basins = ['san_joaquin_river_millerton_reservoir', 'merced_river_yosemite_at_pohono_bridge', 'detroit_lake_inflow']\n",
    "\n",
    "for basin in normalising_basins:\n",
    "    path = f'/data/Hydra_Work/Rodeo_Data/USGS_streamflows/{basin}.csv' \n",
    "    normalising_path = f'/data/Hydra_Work/Rodeo_Data/train_yearly/{basin}.csv'\n",
    "    USGS_to_daily_df_yearly(daily_flow, path, basin, normalising_path)\n",
    "\n",
    "climate_scaler_filename = '/data/Hydra_Work/Rodeo_Data/scalers/climate_normalization_scaler.save'\n",
    "climate_scaler = joblib.load(climate_scaler_filename) \n",
    "climate_indices = pd.DataFrame(climate_scaler.transform(climate_indices), columns=climate_indices.columns, index=climate_indices.index)\n",
    "\n",
    "era5_scaler_filename = '/data/Hydra_Work/Rodeo_Data/scalers/era5_scaler.save'\n",
    "era5_scaler = joblib.load(era5_scaler_filename) \n",
    "era5 = {key: pd.DataFrame(era5_scaler.transform(df), columns=df.columns, index=df.index) for key, df in era5.items()}\n",
    "\n",
    "for basin, df in daily_flow.items(): \n",
    "    flow_scaler_filename = f'/data/Hydra_Work/Rodeo_Data/scalers/flows/{basin}_flow_scaler.save'\n",
    "    flow_scaler = joblib.load(flow_scaler_filename) \n",
    "    daily_flow[basin] = pd.DataFrame(flow_scaler.transform(df), columns=df.columns, index=df.index)\n",
    "\n",
    "seasonal_scaler_filename = \"/data/Hydra_Work/Rodeo_Data/scalers/seasonal_scaler.save\"\n",
    "seasonal_scaler = joblib.load(seasonal_scaler_filename)\n",
    "seasonal_forecasts = {key: pd.DataFrame(seasonal_scaler.transform(df), columns=df.columns, index=df.index ) for key, df in seasonal_forecasts.items()}\n",
    "\n",
    "static_scaler_filename = '/data/Hydra_Work/Rodeo_Data/scalers/static_scaler.save'\n",
    "static_scaler = joblib.load(static_scaler_filename) \n",
    "Static_variables = pd.DataFrame(static_scaler.transform(Static_variables), columns=Static_variables.columns, index=Static_variables.index)\n",
    "\n",
    "climatological_flows = {}\n",
    "\n",
    "for basin, df in daily_flow.items():\n",
    "    # Extract day of year and flow values\n",
    "    df['day_of_year'] = df.index.dayofyear\n",
    "\n",
    "    grouped = df.groupby('day_of_year')['daily_flow'].quantile([0.1, 0.5, 0.9]).unstack(level=1)\n",
    "\n",
    "    climatological_flows[basin] = pd.DataFrame({\n",
    "        'day_of_year': grouped.index,\n",
    "        '10th_percentile_flow': grouped[0.1],\n",
    "        '50th_percentile_flow': grouped[0.5],\n",
    "        '90th_percentile_flow': grouped[0.9]\n",
    "    })\n",
    "    \n",
    "    climatological_flows[basin].set_index('day_of_year', inplace=True)\n",
    "\n",
    "    # Drop the temporary 'day_of_year' column from the original dataframe\n",
    "    df.drop(columns='day_of_year', inplace=True)\n",
    "\n",
    "criterion = SumPinballLoss(quantiles = [0.1, 0.5, 0.9])\n",
    "\n",
    "basin = 'animas_r_at_durango' \n",
    "All_Dates = daily_flow[basin].index[\n",
    "    ((daily_flow[basin].index.month < 6) | ((daily_flow[basin].index.month == 6) & (daily_flow[basin].index.day < 25))) &\n",
    "    ((daily_flow[basin].index.year % 2 == 0) | ((daily_flow[basin].index.month > 10) | ((daily_flow[basin].index.month == 10) & (daily_flow[basin].index.day >= 1))))\n",
    "]\n",
    "All_Dates = All_Dates[All_Dates.year > 1998]\n",
    "\n",
    "\n",
    "# Validation Year\n",
    "Val_Dates = All_Dates[All_Dates.year == 2022]\n",
    "All_Dates = All_Dates[All_Dates.year < 2022]\n",
    "\n",
    "\n",
    "basin_to_remove = 'sweetwater_r_nr_alcova'\n",
    "\n",
    "if basin_to_remove in basins:\n",
    "    basins.remove(basin_to_remove)\n",
    "\n",
    "\n",
    "seed = 42 ; torch.manual_seed(seed) ; random.seed(seed) ; np.random.seed(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "days  = 90\n",
    "hindcast_input_size = 17\n",
    "\n",
    "LR = 1e-3\n",
    "static_size = np.shape(Static_variables)[1]\n",
    "forecast_size = np.shape(seasonal_forecasts['american_river_folsom_lake_2000_apr'])[1]\n",
    "History_Fourier_in_forcings = 0 #2*3*(6 - 1)\n",
    "Climate_guess = 3\n",
    "History_Statistics_in_forcings = 5*2\n",
    "\n",
    "head_input_size = forecast_size + static_size + History_Fourier_in_forcings + History_Statistics_in_forcings  + Climate_guess + 3\n",
    "head_output_size = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning individual basins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "static_size = np.shape(Static_variables)[1]\n",
    "forecast_size = np.shape(seasonal_forecasts['american_river_folsom_lake_2000_apr'])[1]\n",
    "History_Fourier_in_forcings = 0 #2*3*(6 - 1)\n",
    "Climate_guess = 3\n",
    "History_Statistics_in_forcings = 5*2\n",
    "\n",
    "forecast_input_size = forecast_size + static_size + History_Fourier_in_forcings + History_Statistics_in_forcings  + Climate_guess + 3\n",
    "output_size, head_hidden_size, head_num_layers =  3, 64, 3\n",
    "hindcast_input_size = 17\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we want hindcast and forecast num-layers to be different?\n",
    "def define_models(hindcast_input_size, forecast_input_size, hidden_size, num_layers, dropout, bidirectional, learning_rate, copies = 3, forecast_output_size = 3, device = device):\n",
    "    models = {}\n",
    "    params_to_optimize = {}\n",
    "    optimizers = {}\n",
    "    schedulers = {}\n",
    "    \n",
    "    hindcast_output_size = forecast_output_size\n",
    "    for copy in range(copies):\n",
    "        models[copy] = Google_Model_Block(hindcast_input_size, forecast_input_size, hindcast_output_size, forecast_output_size, hidden_size, num_layers, device, dropout, bidirectional)\n",
    "        \n",
    "        models[copy].to(device)\n",
    "        params_to_optimize[copy] = list(models[copy].parameters())\n",
    "\n",
    "        optimizers[copy] = torch.optim.Adam(params_to_optimize[copy], lr= learning_rate, weight_decay = 1e-3)\n",
    "        schedulers[copy] = lr_scheduler.CosineAnnealingLR(optimizers[copy], T_max=1e4)\n",
    "\n",
    "    return models, params_to_optimize, optimizers, schedulers\n",
    "\n",
    "def update_final_parameters(Final_Parameters, basin, min_val_loss_parameters, min_val_loss):\n",
    "    Final_Parameters['basin'].append(basin)\n",
    "    Final_Parameters['hidden_size'].append(min_val_loss_parameters[0])\n",
    "    Final_Parameters['num_layers'].append(min_val_loss_parameters[1])\n",
    "    Final_Parameters['dropout'].append(min_val_loss_parameters[2])\n",
    "    Final_Parameters['bidirectional'].append(min_val_loss_parameters[3])\n",
    "    Final_Parameters['learning_rate'].append(min_val_loss_parameters[4])\n",
    "    Final_Parameters['val_loss'].append(min_val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.stopper import TrialPlateauStopper\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "import optuna\n",
    "\n",
    "# Fixed parameters\n",
    "total_epochs = 15\n",
    "n_epochs = 1  # Epochs between tests\n",
    "group_lengths = np.arange(180)\n",
    "batch_size = 1\n",
    "copies = 3\n",
    "\n",
    "# parameters to tune\n",
    "hidden_sizes = [64, 128]\n",
    "num_layers =  [1,3]\n",
    "dropout = [0.1, 0.4, 0.7]\n",
    "bidirectional = [True]\n",
    "learning_rate = [1e-3, 1e-5]\n",
    "\n",
    "# Set up configuration space\n",
    "config_space = {\n",
    "    \"hidden_size\": tune.grid_search(hidden_sizes),\n",
    "    \"num_layers\": tune.grid_search(num_layers),\n",
    "    \"dropout\": tune.grid_search(dropout),\n",
    "    \"bidirectional\": tune.grid_search(bidirectional),\n",
    "    \"learning_rate\": tune.grid_search(learning_rate)\n",
    "}\n",
    "\n",
    "def define_optuna_search_space(trial: optuna.Trial):\n",
    "    trial.suggest_categorical(\"hidden_size\", hidden_sizes)\n",
    "    trial.suggest_categorical(\"num_layers\", num_layers)\n",
    "    trial.suggest_categorical(\"dropout\", dropout)\n",
    "    trial.suggest_categorical(\"bidirectional\", bidirectional)\n",
    "    trial.suggest_categorical(\"learning_rate\", learning_rate)\n",
    "\n",
    "optuna_config_space = {\n",
    "    \"hidden_size\": tune.lograndint(16,128),\n",
    "    \"num_layers\": tune.randint(1,3),\n",
    "    \"dropout\": tune.uniform(0.1,0.7),\n",
    "    \"bidirectional\": tune.choice(bidirectional),\n",
    "    \"learning_rate\": tune.loguniform(1e-3, 1e-5)\n",
    "}\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "\n",
    "    All_Dates = ray.get(All_Dates_id)  \n",
    "    Val_Dates = ray.get(Val_Dates_id)  \n",
    "    era5 = ray.get(era5_id)  \n",
    "    daily_flow = ray.get(daily_flow_id)  \n",
    "    climatological_flows = ray.get(climatological_flows_id)\n",
    "    climate_indices = ray.get(climate_indices_id)\n",
    "    seasonal_forecasts = ray.get(seasonal_forecasts_id)\n",
    "    Static_variables = ray.get(Static_variables_id)\n",
    "\n",
    "    val_loss = 1000\n",
    "\n",
    "    copies = 3\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.\n",
    "                    is_available() else 'cpu')\n",
    "    \n",
    "    models, params_to_optimize, optimizers, schedulers = define_models(hindcast_input_size, forecast_input_size,\n",
    "    config[\"hidden_size\"], config[\"num_layers\"], config[\"dropout\"],\n",
    "    config[\"bidirectional\"], config[\"learning_rate\"], copies=copies, device = device)\n",
    "\n",
    "\n",
    "    losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(total_epochs):\n",
    "\n",
    "        train_losses = {}\n",
    "        epoch_val_losses = {}\n",
    "\n",
    "        for copy in range(copies):\n",
    "\n",
    "             # Need to fix the outputs of No_Body_Model_Run\n",
    "            train_losses[copy], Climate_Loss = No_Body_Model_Run(All_Dates, [basin], models[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts,\n",
    "                Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper= None, n_epochs=n_epochs,\n",
    "                batch_size=batch_size, group_lengths=group_lengths, Train_Mode=True, device=device, specialised=False)\n",
    "            epoch_val_losses[copy], Climate_Loss = No_Body_Model_Run(Val_Dates, [basin], models[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts,\n",
    "                Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper= None, n_epochs=n_epochs,\n",
    "                batch_size=batch_size, group_lengths=group_lengths, Train_Mode=False, device=device, specialised=False)\n",
    "\n",
    "        loss = np.mean(list(train_losses.values())) - Climate_Loss\n",
    "        val_loss = np.min(val_loss, 10 + (np.mean(list(epoch_val_losses.values())).mean() - Climate_Loss)[0])\n",
    "        ray.train.report({'val_loss' : val_loss})\n",
    "\n",
    "        losses.append(loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 14:38:15,910\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "from ray import train, tune\n",
    "\n",
    "\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(runtime_env = { \"env_vars\":   {\"PYTHONPATH\": '/data/Hydra_Work/Competition_Functions/' } } )\n",
    "         \n",
    "All_Dates_id = ray.put(All_Dates)  \n",
    "Val_Dates_id = ray.put(Val_Dates)  \n",
    "era5_id = ray.put(era5)  \n",
    "daily_flow_id = ray.put(daily_flow)  \n",
    "climatological_flows_id = ray.put(climatological_flows)\n",
    "climate_indices_id = ray.put(climate_indices)\n",
    "seasonal_forecasts_id = ray.put(seasonal_forecasts)\n",
    "Static_variables_id = ray.put(Static_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "asha_scheduler = ASHAScheduler(\n",
    "    time_attr='training_iteration',\n",
    "    metric='val_loss',\n",
    "    mode='min',\n",
    "    max_t=100,\n",
    "    grace_period=3,\n",
    "    reduction_factor=2,\n",
    "    brackets=1,\n",
    ")\n",
    "\n",
    "\n",
    "plateau_stopper = TrialPlateauStopper(\n",
    "    metric=\"val_loss\",\n",
    "    num_results = 3,\n",
    "    grace_period=4,\n",
    "    mode=\"min\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=1071532)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "\u001b[36m(objective pid=1071532)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=1071567)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=1071567)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 13x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Stehekin gives :True\t0.4\t64\t0.001\t3\t11\t\n",
    "# T-tests suggests: Bidirectional good, dropout unimportant, 16 bad, 64 vs 128 unimportant\n",
    "\n",
    "def objective(config):   \n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.\n",
    "                      is_available() else 'cpu')\n",
    "    \n",
    "    #print('Device available is', device)\n",
    "    \n",
    "\n",
    "    score = train_model(config) # Have training loop in here that outputs loss of model\n",
    "    return {\"val_loss\": score}\n",
    "\n",
    "basin = 'stehekin_r_at_stehekin'\n",
    "basin = 'animas_r_at_durango'\n",
    "\n",
    "#, search_alg = optuna_search\n",
    "optuna_tune_config = tune.TuneConfig(scheduler=asha_scheduler)\n",
    "tune_config = tune.TuneConfig(scheduler=asha_scheduler)\n",
    "run_config=train.RunConfig(stop= plateau_stopper)\n",
    "\n",
    "# Note using < 1gb per run stops pylance from crashing I think\n",
    "# Without Optun\n",
    "tuner = tune.Tuner(tune.with_resources(tune.with_parameters(objective), resources={\"cpu\": 9/10, \"gpu\": 1/17}), param_space=config_space, tune_config = tune_config, run_config = run_config) \n",
    "# With Optuna\n",
    "#tuner = tune.Tuner(tune.with_resources(tune.with_parameters(objective), resources={\"cpu\": 1, \"gpu\": 1/16}), param_space = optuna_config_space, tune_config = optuna_tune_config, run_config = run_config) \n",
    "\n",
    "results = tuner.fit()\n",
    "results_df = results.get_dataframe()\n",
    "best_config = results.get_best_result(metric=\"val_loss\", mode=\"min\").config\n",
    "print(best_config)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the file path where you want to save the best configuration\n",
    "file_path = f\"/data/Hydra_Work/Tuning/Config_Text/{basin}_best_config.txt\"\n",
    "\n",
    "# Open the file in write mode and save the configuration\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(str(best_config))\n",
    "\n",
    "print(\"Best configuration saved to:\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss</th>\n",
       "      <th>config/hidden_size</th>\n",
       "      <th>config/num_layers</th>\n",
       "      <th>config/dropout</th>\n",
       "      <th>config/bidirectional</th>\n",
       "      <th>config/learning_rate</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.904474</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>a9c9c_00030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>7.149454</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>a9c9c_00031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>8.761816</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>a9c9c_00035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>7.934672</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>a9c9c_00045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    val_loss  config/hidden_size  config/num_layers  config/dropout  \\\n",
       "30  8.904474                  64                  3             0.4   \n",
       "31  7.149454                  64                  3             0.4   \n",
       "35  8.761816                 128                  3             0.4   \n",
       "45  7.934672                 128                  3             0.1   \n",
       "\n",
       "    config/bidirectional  config/learning_rate       logdir  \n",
       "30                 False               0.00100  a9c9c_00030  \n",
       "31                  True               0.00100  a9c9c_00031  \n",
       "35                  True               0.00100  a9c9c_00035  \n",
       "45                  True               0.00001  a9c9c_00045  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[results_df['val_loss'] < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Statistic: -0.3305082237200267\n",
      "P-Value: 0.7425166701300201\n",
      "The difference in mean val_loss is not statistically significant.\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "results_df = results.get_dataframe()\n",
    "columns_to_drop = ['timestamp', 'checkpoint_dir_name', 'done', 'training_iteration', \n",
    "                   'trial_id', 'date', 'time_this_iter_s', 'time_total_s', 'pid', \n",
    "                   'hostname', 'node_ip', 'time_since_restore', 'iterations_since_restore']\n",
    "\n",
    "# Drop the columns\n",
    "results_df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "val_loss_bidirectional_true = results_df[results_df['config/num_layers'] == 3]['val_loss']\n",
    "val_loss_bidirectional_false = results_df[results_df['config/num_layers'] == 1]['val_loss']\n",
    "\n",
    "# Perform a t-test\n",
    "t_statistic, p_value = stats.ttest_ind(val_loss_bidirectional_true, val_loss_bidirectional_false)\n",
    "\n",
    "# Print the results\n",
    "print(\"T-Statistic:\", t_statistic)\n",
    "print(\"P-Value:\", p_value)\n",
    "\n",
    "# Check if the difference in means is statistically significant\n",
    "alpha = 0.05  # Significance level\n",
    "if p_value < alpha:\n",
    "    print(\"The difference in mean val_loss is statistically significant.\")\n",
    "else:\n",
    "    print(\"The difference in mean val_loss is not statistically significant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading models\n",
    "Tuned_Models = {}\n",
    "for basin in basins:\n",
    "    Tuned_Models[basin] = torch.load(f'/data/Hydra_Work/Post_Rodeo_Work/Tuned_Single_Models/basin.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning General Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "static_size = np.shape(Static_variables)[1]\n",
    "forecast_size = np.shape(seasonal_forecasts['american_river_folsom_lake_2000_apr'])[1]\n",
    "History_Fourier_in_forcings = 0 #2*3*(6 - 1)\n",
    "Climate_guess = 3\n",
    "History_Statistics_in_forcings = 5*2\n",
    "\n",
    "input_size = forecast_size + static_size + History_Fourier_in_forcings + History_Statistics_in_forcings  + Climate_guess + 3\n",
    "output_size, head_hidden_size, head_num_layers =  3, 64, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_final_parameters_general(Final_Parameters, min_val_loss_parameters, min_val_loss):\n",
    "    Final_Parameters['hidden_size'].append(min_val_loss_parameters[0])\n",
    "    Final_Parameters['num_layers'].append(min_val_loss_parameters[1])\n",
    "    Final_Parameters['dropout'].append(min_val_loss_parameters[2])\n",
    "    Final_Parameters['bidirectional'].append(min_val_loss_parameters[3])\n",
    "    Final_Parameters['learning_rate'].append(min_val_loss_parameters[4])\n",
    "    Final_Parameters['val_loss'].append(min_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.stopper import TrialPlateauStopper\n",
    "\n",
    "# Fixed parameters\n",
    "total_epochs = 15\n",
    "n_epochs = 1 # Epochs between tests\n",
    "group_lengths = np.arange(180)\n",
    "batch_size = 1\n",
    "copies = 2\n",
    "\n",
    "# parameters to tune\n",
    "# I tuned to 128,2,0.1,False,1e-3 \n",
    "hidden_sizes = [64, 128, 256]\n",
    "num_layers = [1,3]\n",
    "dropout = [0.1, 0.4]\n",
    "bidirectional =  [False, True]\n",
    "learning_rate = [1e-3, 1e-5]\n",
    "\n",
    "config_space = {\n",
    "    \"hidden_size\": tune.grid_search(hidden_sizes),\n",
    "    \"num_layers\": tune.grid_search(num_layers),\n",
    "    \"dropout\": tune.grid_search(dropout),\n",
    "    \"bidirectional\": tune.grid_search(bidirectional),\n",
    "    \"learning_rate\": tune.grid_search(learning_rate)\n",
    "}\n",
    "\n",
    "\n",
    "# Places to save info\n",
    "model_dir = '/data/Hydra_Work/Post_Rodeo_Work/Tuned_General_Model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_general(config):\n",
    "\n",
    "    All_Dates = ray.get(All_Dates_id)  \n",
    "    Val_Dates = ray.get(Val_Dates_id)  \n",
    "    era5 = ray.get(era5_id)  \n",
    "    daily_flow = ray.get(daily_flow_id)  \n",
    "    climatological_flows = ray.get(climatological_flows_id)\n",
    "    climate_indices = ray.get(climate_indices_id)\n",
    "    seasonal_forecasts = ray.get(seasonal_forecasts_id)\n",
    "    Static_variables = ray.get(Static_variables_id)\n",
    "\n",
    "    copies = 3\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.\n",
    "                    is_available() else 'cpu')\n",
    "   \n",
    "    models, params_to_optimize, optimizers, schedulers = define_models(hindcast_input_size, forecast_input_size,\n",
    "    config[\"hidden_size\"], config[\"num_layers\"], config[\"dropout\"],\n",
    "    config[\"bidirectional\"], config[\"learning_rate\"], copies=copies, device = device)\n",
    "\n",
    "    losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(total_epochs):\n",
    "\n",
    "        train_losses = {}\n",
    "        epoch_val_losses = {}\n",
    "\n",
    "        for copy in range(copies):\n",
    "\n",
    "             # Need to fix the outputs of No_Body_Model_Run\n",
    "            train_losses[copy], Climate_Loss = No_Body_Model_Run(All_Dates, basins, models[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts,\n",
    "                Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper= None, n_epochs=n_epochs,\n",
    "                batch_size=batch_size, group_lengths=group_lengths, Train_Mode=True, device=device, specialised=False)\n",
    "            epoch_val_losses[copy], Climate_Loss = No_Body_Model_Run(Val_Dates, basins, models[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts,\n",
    "                Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper= None, n_epochs=n_epochs,\n",
    "                batch_size=batch_size, group_lengths=group_lengths, Train_Mode=False, device=device, specialised=False)\n",
    "\n",
    "        loss = np.mean(list(train_losses.values()))\n",
    "        val_loss = (np.mean(list(epoch_val_losses.values())) - Climate_Loss)[0]\n",
    "\n",
    "        ray.train.report({'val_loss' : val_loss})\n",
    "\n",
    "        losses.append(loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 07:39:25,321\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "from ray import train, tune\n",
    "\n",
    "\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(runtime_env = { \"env_vars\":   {\"PYTHONPATH\": '/data/Hydra_Work/Competition_Functions/' } } )\n",
    "         \n",
    "All_Dates_id = ray.put(All_Dates)  \n",
    "Val_Dates_id = ray.put(Val_Dates)  \n",
    "era5_id = ray.put(era5)  \n",
    "daily_flow_id = ray.put(daily_flow)  \n",
    "climatological_flows_id = ray.put(climatological_flows)\n",
    "climate_indices_id = ray.put(climate_indices)\n",
    "seasonal_forecasts_id = ray.put(seasonal_forecasts)\n",
    "Static_variables_id = ray.put(Static_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-09 07:40:11,873] A new study created in memory with name: optuna\n"
     ]
    }
   ],
   "source": [
    "asha_scheduler = ASHAScheduler(\n",
    "    time_attr='training_iteration',\n",
    "    metric='val_loss',\n",
    "    mode='min',\n",
    "    max_t=100,\n",
    "    grace_period=4,\n",
    "    reduction_factor=2,\n",
    "    brackets=1,\n",
    ")\n",
    "\n",
    "\n",
    "optuna_search = OptunaSearch(\n",
    "    define_optuna_search_space,\n",
    "    metric=\"val_loss\",\n",
    "    mode=\"min\")\n",
    "\n",
    "plateau_stopper = TrialPlateauStopper(\n",
    "    metric=\"val_loss\",\n",
    "    num_results = 3,\n",
    "    grace_period=4,\n",
    "    mode=\"min\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-04-09 14:15:59</td></tr>\n",
       "<tr><td>Running for: </td><td>06:35:46.15        </td></tr>\n",
       "<tr><td>Memory:      </td><td>10.0/125.9 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=41<br>Bracket: Iter 64.000: None | Iter 32.000: None | Iter 16.000: -0.009507081622171398 | Iter 8.000: -1.111638052889278 | Iter 4.000: -0.4610652891227165<br>Logical resource usage: 0.8333333333333334/16 CPUs, 0.0625/1 GPUs (0.0/1.0 accelerator_type:A100D)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc                   </th><th>bidirectional  </th><th style=\"text-align: right;\">  dropout</th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  num_layers</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  val_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_66552_00000</td><td>TERMINATED</td><td>136.156.133.98:1063152</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         8377.03</td><td style=\"text-align: right;\">-0.0186945</td></tr>\n",
       "<tr><td>objective_66552_00001</td><td>TERMINATED</td><td>136.156.133.98:1063153</td><td>True           </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         1867.53</td><td style=\"text-align: right;\"> 0.187761 </td></tr>\n",
       "<tr><td>objective_66552_00002</td><td>TERMINATED</td><td>136.156.133.98:1063154</td><td>False          </td><td style=\"text-align: right;\">      0.4</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         1916.27</td><td style=\"text-align: right;\"> 1.07532  </td></tr>\n",
       "<tr><td>objective_66552_00003</td><td>TERMINATED</td><td>136.156.133.98:1063155</td><td>True           </td><td style=\"text-align: right;\">      0.4</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         4157.62</td><td style=\"text-align: right;\"> 2.36957  </td></tr>\n",
       "<tr><td>objective_66552_00004</td><td>TERMINATED</td><td>136.156.133.98:1063156</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         8551.36</td><td style=\"text-align: right;\">-0.394257 </td></tr>\n",
       "<tr><td>objective_66552_00005</td><td>TERMINATED</td><td>136.156.133.98:1063157</td><td>True           </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         2142.7 </td><td style=\"text-align: right;\"> 1.58483  </td></tr>\n",
       "<tr><td>objective_66552_00006</td><td>TERMINATED</td><td>136.156.133.98:1063158</td><td>False          </td><td style=\"text-align: right;\">      0.4</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         4130.47</td><td style=\"text-align: right;\"> 2.32046  </td></tr>\n",
       "<tr><td>objective_66552_00007</td><td>TERMINATED</td><td>136.156.133.98:1063159</td><td>True           </td><td style=\"text-align: right;\">      0.4</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         4536.17</td><td style=\"text-align: right;\"> 1.26398  </td></tr>\n",
       "<tr><td>objective_66552_00008</td><td>TERMINATED</td><td>136.156.133.98:1063160</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         2380.91</td><td style=\"text-align: right;\"> 0.650762 </td></tr>\n",
       "<tr><td>objective_66552_00009</td><td>TERMINATED</td><td>136.156.133.98:1063161</td><td>True           </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         5145.2 </td><td style=\"text-align: right;\"> 3.06394  </td></tr>\n",
       "<tr><td>objective_66552_00010</td><td>TERMINATED</td><td>136.156.133.98:1063162</td><td>False          </td><td style=\"text-align: right;\">      0.4</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         5133.92</td><td style=\"text-align: right;\"> 1.8052   </td></tr>\n",
       "<tr><td>objective_66552_00011</td><td>TERMINATED</td><td>136.156.133.98:1063163</td><td>True           </td><td style=\"text-align: right;\">      0.4</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         5150.49</td><td style=\"text-align: right;\"> 2.86569  </td></tr>\n",
       "<tr><td>objective_66552_00012</td><td>TERMINATED</td><td>136.156.133.98:1063164</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          1e-05</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         3918.45</td><td style=\"text-align: right;\"> 1.87978  </td></tr>\n",
       "<tr><td>objective_66552_00013</td><td>TERMINATED</td><td>136.156.133.98:1063165</td><td>True           </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          1e-05</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         8966.47</td><td style=\"text-align: right;\"> 1.73463  </td></tr>\n",
       "<tr><td>objective_66552_00014</td><td>TERMINATED</td><td>136.156.133.98:1063166</td><td>False          </td><td style=\"text-align: right;\">      0.4</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          1e-05</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         1892.79</td><td style=\"text-align: right;\"> 1.27379  </td></tr>\n",
       "<tr><td>objective_66552_00015</td><td>TERMINATED</td><td>136.156.133.98:1063167</td><td>True           </td><td style=\"text-align: right;\">      0.4</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          1e-05</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         1887.19</td><td style=\"text-align: right;\"> 3.81824  </td></tr>\n",
       "<tr><td>objective_66552_00016</td><td>TERMINATED</td><td>136.156.133.98:1063153</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          1e-05</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         2063.04</td><td style=\"text-align: right;\"> 0.445607 </td></tr>\n",
       "<tr><td>objective_66552_00017</td><td>TERMINATED</td><td>136.156.133.98:1063167</td><td>True           </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          1e-05</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         2238.59</td><td style=\"text-align: right;\"> 1.31725  </td></tr>\n",
       "<tr><td>objective_66552_00018</td><td>TERMINATED</td><td>136.156.133.98:1063166</td><td>False          </td><td style=\"text-align: right;\">      0.4</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          1e-05</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         2168.99</td><td style=\"text-align: right;\">-0.0134639</td></tr>\n",
       "<tr><td>objective_66552_00019</td><td>TERMINATED</td><td>136.156.133.98:1063154</td><td>True           </td><td style=\"text-align: right;\">      0.4</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          1e-05</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         2361.23</td><td style=\"text-align: right;\"> 1.55218  </td></tr>\n",
       "<tr><td>objective_66552_00020</td><td>TERMINATED</td><td>136.156.133.98:1063157</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          1e-05</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         3309.32</td><td style=\"text-align: right;\"> 1.57201  </td></tr>\n",
       "<tr><td>objective_66552_00021</td><td>TERMINATED</td><td>136.156.133.98:1063160</td><td>True           </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          1e-05</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         2789.98</td><td style=\"text-align: right;\"> 0.780592 </td></tr>\n",
       "<tr><td>objective_66552_00022</td><td>TERMINATED</td><td>136.156.133.98:1063164</td><td>False          </td><td style=\"text-align: right;\">      0.4</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          1e-05</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         3568.51</td><td style=\"text-align: right;\"> 2.12026  </td></tr>\n",
       "<tr><td>objective_66552_00023</td><td>TERMINATED</td><td>136.156.133.98:1063153</td><td>True           </td><td style=\"text-align: right;\">      0.4</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          1e-05</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         3142.2 </td><td style=\"text-align: right;\"> 1.13952  </td></tr>\n",
       "<tr><td>objective_66552_00024</td><td>TERMINATED</td><td>136.156.133.98:1063166</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         2769.45</td><td style=\"text-align: right;\"> 0.454278 </td></tr>\n",
       "<tr><td>objective_66552_00025</td><td>TERMINATED</td><td>136.156.133.98:1063167</td><td>True           </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        11223.9 </td><td style=\"text-align: right;\"> 1.26541  </td></tr>\n",
       "<tr><td>objective_66552_00026</td><td>TERMINATED</td><td>136.156.133.98:1063158</td><td>False          </td><td style=\"text-align: right;\">      0.4</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         2788.19</td><td style=\"text-align: right;\"> 0.467853 </td></tr>\n",
       "<tr><td>objective_66552_00027</td><td>TERMINATED</td><td>136.156.133.98:1063155</td><td>True           </td><td style=\"text-align: right;\">      0.4</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        12241.1 </td><td style=\"text-align: right;\"> 0.70837  </td></tr>\n",
       "<tr><td>objective_66552_00028</td><td>TERMINATED</td><td>136.156.133.98:1063154</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        11866.6 </td><td style=\"text-align: right;\"> 0.298834 </td></tr>\n",
       "<tr><td>objective_66552_00029</td><td>TERMINATED</td><td>136.156.133.98:1063159</td><td>True           </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         3904.18</td><td style=\"text-align: right;\"> 1.10603  </td></tr>\n",
       "<tr><td>objective_66552_00030</td><td>TERMINATED</td><td>136.156.133.98:1063162</td><td>False          </td><td style=\"text-align: right;\">      0.4</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         3099.58</td><td style=\"text-align: right;\"> 2.77195  </td></tr>\n",
       "<tr><td>objective_66552_00031</td><td>TERMINATED</td><td>136.156.133.98:1063161</td><td>True           </td><td style=\"text-align: right;\">      0.4</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         3596.14</td><td style=\"text-align: right;\"> 0.511365 </td></tr>\n",
       "<tr><td>objective_66552_00032</td><td>TERMINATED</td><td>136.156.133.98:1063163</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         4117.45</td><td style=\"text-align: right;\"> 0.76781  </td></tr>\n",
       "<tr><td>objective_66552_00033</td><td>TERMINATED</td><td>136.156.133.98:1063160</td><td>True           </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        16184.2 </td><td style=\"text-align: right;\">-2.75208  </td></tr>\n",
       "<tr><td>objective_66552_00034</td><td>TERMINATED</td><td>136.156.133.98:1063157</td><td>False          </td><td style=\"text-align: right;\">      0.4</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         6299.64</td><td style=\"text-align: right;\"> 3.74267  </td></tr>\n",
       "<tr><td>objective_66552_00035</td><td>TERMINATED</td><td>136.156.133.98:1063166</td><td>True           </td><td style=\"text-align: right;\">      0.4</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        16910.2 </td><td style=\"text-align: right;\"> 0.0377086</td></tr>\n",
       "<tr><td>objective_66552_00036</td><td>TERMINATED</td><td>136.156.133.98:1063158</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          1e-05</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         3116.25</td><td style=\"text-align: right;\"> 2.34717  </td></tr>\n",
       "<tr><td>objective_66552_00037</td><td>TERMINATED</td><td>136.156.133.98:1063153</td><td>True           </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          1e-05</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         3195.81</td><td style=\"text-align: right;\"> 1.79848  </td></tr>\n",
       "<tr><td>objective_66552_00038</td><td>TERMINATED</td><td>136.156.133.98:1063164</td><td>False          </td><td style=\"text-align: right;\">      0.4</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          1e-05</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         3176.79</td><td style=\"text-align: right;\"> 1.31598  </td></tr>\n",
       "<tr><td>objective_66552_00039</td><td>TERMINATED</td><td>136.156.133.98:1063162</td><td>True           </td><td style=\"text-align: right;\">      0.4</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          1e-05</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        10163   </td><td style=\"text-align: right;\">-1.27969  </td></tr>\n",
       "<tr><td>objective_66552_00040</td><td>TERMINATED</td><td>136.156.133.98:1063152</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          1e-05</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         3471.05</td><td style=\"text-align: right;\"> 2.04107  </td></tr>\n",
       "<tr><td>objective_66552_00041</td><td>TERMINATED</td><td>136.156.133.98:1063159</td><td>True           </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          1e-05</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         4247.71</td><td style=\"text-align: right;\"> 2.14048  </td></tr>\n",
       "<tr><td>objective_66552_00042</td><td>TERMINATED</td><td>136.156.133.98:1063156</td><td>False          </td><td style=\"text-align: right;\">      0.4</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          1e-05</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         3477.6 </td><td style=\"text-align: right;\"> 1.68551  </td></tr>\n",
       "<tr><td>objective_66552_00043</td><td>TERMINATED</td><td>136.156.133.98:1063161</td><td>True           </td><td style=\"text-align: right;\">      0.4</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          1e-05</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        11148.3 </td><td style=\"text-align: right;\"> 0.400127 </td></tr>\n",
       "<tr><td>objective_66552_00044</td><td>TERMINATED</td><td>136.156.133.98:1063165</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          1e-05</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        10491.7 </td><td style=\"text-align: right;\"> 1.64731  </td></tr>\n",
       "<tr><td>objective_66552_00045</td><td>TERMINATED</td><td>136.156.133.98:1063163</td><td>True           </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          1e-05</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        13033.6 </td><td style=\"text-align: right;\">-0.834869 </td></tr>\n",
       "<tr><td>objective_66552_00046</td><td>TERMINATED</td><td>136.156.133.98:1063158</td><td>False          </td><td style=\"text-align: right;\">      0.4</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          1e-05</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         5651.12</td><td style=\"text-align: right;\"> 1.47152  </td></tr>\n",
       "<tr><td>objective_66552_00047</td><td>TERMINATED</td><td>136.156.133.98:1063153</td><td>True           </td><td style=\"text-align: right;\">      0.4</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          1e-05</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        12203.1 </td><td style=\"text-align: right;\">-0.402955 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=1063153)\u001b[0m Device available is cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=1063153)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=1063153)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=1063152)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=1063152)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 18x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=1063153)\u001b[0m Device available is cuda\u001b[32m [repeated 16x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=1063153)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=1063153)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 14x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=1063167)\u001b[0m Device available is cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=1063167)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=1063167)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=1063166)\u001b[0m Device available is cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=1063166)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "\u001b[36m(objective pid=1063166)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=1063154)\u001b[0m Device available is cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=1063154)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "\u001b[36m(objective pid=1063154)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=1063157)\u001b[0m Device available is cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=1063157)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=1063157)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=1063160)\u001b[0m Device available is cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=1063160)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=1063160)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=1063164)\u001b[0m Device available is cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=1063164)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "\u001b[36m(objective pid=1063164)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=1063153)\u001b[0m Device available is cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=1063153)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "\u001b[36m(objective pid=1063153)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=1063166)\u001b[0m Device available is cuda\n",
      "\u001b[36m(objective pid=1063167)\u001b[0m Device available is cuda\n",
      "\u001b[36m(objective pid=1063158)\u001b[0m Device available is cuda\n",
      "\u001b[36m(objective pid=1063155)\u001b[0m Device available is cuda\n",
      "\u001b[36m(objective pid=1063154)\u001b[0m Device available is cuda\n",
      "\u001b[36m(objective pid=1063159)\u001b[0m Device available is cuda\n",
      "\u001b[36m(objective pid=1063162)\u001b[0m Device available is cuda\n",
      "\u001b[36m(objective pid=1063161)\u001b[0m Device available is cuda\n",
      "\u001b[36m(objective pid=1063163)\u001b[0m Device available is cuda\n",
      "\u001b[36m(objective pid=1063160)\u001b[0m Device available is cuda\n",
      "\u001b[36m(objective pid=1063157)\u001b[0m Device available is cuda\n",
      "\u001b[36m(objective pid=1063166)\u001b[0m Device available is cuda\n",
      "\u001b[36m(objective pid=1063158)\u001b[0m Device available is cuda\n",
      "\u001b[36m(objective pid=1063153)\u001b[0m Device available is cuda\n",
      "\u001b[36m(objective pid=1063164)\u001b[0m Device available is cuda\n",
      "\u001b[36m(objective pid=1063162)\u001b[0m Device available is cuda\n",
      "\u001b[36m(objective pid=1063152)\u001b[0m Device available is cuda\n",
      "\u001b[36m(objective pid=1063159)\u001b[0m Device available is cuda\n",
      "\u001b[36m(objective pid=1063156)\u001b[0m Device available is cuda\n",
      "\u001b[36m(objective pid=1063161)\u001b[0m Device available is cuda\n",
      "\u001b[36m(objective pid=1063165)\u001b[0m Device available is cuda\n",
      "\u001b[36m(objective pid=1063163)\u001b[0m Device available is cuda\n",
      "\u001b[36m(objective pid=1063158)\u001b[0m Device available is cuda\n",
      "\u001b[36m(objective pid=1063153)\u001b[0m Device available is cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 14:15:59,172\tINFO tune.py:1042 -- Total run time: 23746.17 seconds (23746.14 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_size': 256, 'num_layers': 3, 'dropout': 0.1, 'bidirectional': True, 'learning_rate': 0.001}\n",
      "Best configuration saved to: /data/Hydra_Work/Tuning/Config_Text/General_Model_best_config.txt\n"
     ]
    }
   ],
   "source": [
    "# {'hidden_size': 256, 'num_layers': 3, 'dropout': 0.1, 'bidirectional': True, 'learning_rate': 0.001}\n",
    "def objective(config):  \n",
    "    device = torch.device('cuda' if torch.cuda.\n",
    "                      is_available() else 'cpu')\n",
    "    \n",
    "    print('Device available is', device)\n",
    "    \n",
    "\n",
    "    score = train_model_general(config) # Have training loop in here that outputs loss of model\n",
    "    return {\"val_loss\": score}\n",
    "\n",
    "\n",
    "#, search_alg = optuna_search\n",
    "optuna_tune_config = tune.TuneConfig(scheduler=asha_scheduler)\n",
    "tune_config = tune.TuneConfig(scheduler=asha_scheduler)\n",
    "run_config=train.RunConfig(stop= plateau_stopper)\n",
    "\n",
    "# Without Optuna\n",
    "tuner = tune.Tuner(tune.with_resources(tune.with_parameters(objective), resources={\"cpu\": 5/6, \"gpu\": 1/16}), param_space=config_space, tune_config = tune_config, run_config = run_config) \n",
    "# With Optuna\n",
    "#tuner = tune.Tuner(tune.with_resources(tune.with_parameters(objective), resources={\"cpu\": 1, \"gpu\": 1/16}), param_space = optuna_config_space, tune_config = optuna_tune_config, run_config = run_config) \n",
    "\n",
    "results = tuner.fit()\n",
    "# try get_best_checkpoint, or change val to be maximum of current val_loss and previous ones\n",
    "best_config = results.get_best_result(metric=\"val_loss\", mode=\"min\").config\n",
    "print(best_config)\n",
    "file_path = f\"/data/Hydra_Work/Tuning/Config_Text/General_Model_best_config.txt\"\n",
    "\n",
    "# Open the file in write mode and save the configuration\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(str(best_config))\n",
    "\n",
    "print(\"Best configuration saved to:\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss</th>\n",
       "      <th>config/hidden_size</th>\n",
       "      <th>config/num_layers</th>\n",
       "      <th>config/dropout</th>\n",
       "      <th>config/bidirectional</th>\n",
       "      <th>config/learning_rate</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.018694</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>66552_00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.187761</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>66552_00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.075316</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>66552_00002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.369570</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>66552_00003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.394257</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>66552_00004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.584831</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>66552_00005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.320457</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>66552_00006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.263984</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>66552_00007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.650762</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>66552_00008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.063937</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>66552_00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.805204</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>66552_00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.865686</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>66552_00011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.879778</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>66552_00012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.734627</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>66552_00013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.273789</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>66552_00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.818238</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>66552_00015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.445607</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>66552_00016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.317251</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>66552_00017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.013464</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>66552_00018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.552181</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>66552_00019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.572007</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>66552_00020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.780592</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>66552_00021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.120258</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>66552_00022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.139519</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>66552_00023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.454278</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>66552_00024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.265408</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>66552_00025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.467853</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>66552_00026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.708370</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>66552_00027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.298834</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>66552_00028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.106034</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>66552_00029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2.771953</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>66552_00030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.511365</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>66552_00031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.767810</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>66552_00032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-2.752084</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>66552_00033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3.742673</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>66552_00034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.037709</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>66552_00035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2.347165</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>66552_00036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.798485</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>66552_00037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.315977</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>66552_00038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-1.279688</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>66552_00039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2.041072</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>66552_00040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2.140484</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>66552_00041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.685515</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>66552_00042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.400127</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>66552_00043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.647309</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>66552_00044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.834869</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>66552_00045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.471520</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>66552_00046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.402955</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>66552_00047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    val_loss  config/hidden_size  config/num_layers  config/dropout  \\\n",
       "0  -0.018694                  64                  1             0.1   \n",
       "1   0.187761                  64                  1             0.1   \n",
       "2   1.075316                  64                  1             0.4   \n",
       "3   2.369570                  64                  1             0.4   \n",
       "4  -0.394257                 128                  1             0.1   \n",
       "5   1.584831                 128                  1             0.1   \n",
       "6   2.320457                 128                  1             0.4   \n",
       "7   1.263984                 128                  1             0.4   \n",
       "8   0.650762                 256                  1             0.1   \n",
       "9   3.063937                 256                  1             0.1   \n",
       "10  1.805204                 256                  1             0.4   \n",
       "11  2.865686                 256                  1             0.4   \n",
       "12  1.879778                  64                  1             0.1   \n",
       "13  1.734627                  64                  1             0.1   \n",
       "14  1.273789                  64                  1             0.4   \n",
       "15  3.818238                  64                  1             0.4   \n",
       "16  0.445607                 128                  1             0.1   \n",
       "17  1.317251                 128                  1             0.1   \n",
       "18 -0.013464                 128                  1             0.4   \n",
       "19  1.552181                 128                  1             0.4   \n",
       "20  1.572007                 256                  1             0.1   \n",
       "21  0.780592                 256                  1             0.1   \n",
       "22  2.120258                 256                  1             0.4   \n",
       "23  1.139519                 256                  1             0.4   \n",
       "24  0.454278                  64                  3             0.1   \n",
       "25  1.265408                  64                  3             0.1   \n",
       "26  0.467853                  64                  3             0.4   \n",
       "27  0.708370                  64                  3             0.4   \n",
       "28  0.298834                 128                  3             0.1   \n",
       "29  1.106034                 128                  3             0.1   \n",
       "30  2.771953                 128                  3             0.4   \n",
       "31  0.511365                 128                  3             0.4   \n",
       "32  0.767810                 256                  3             0.1   \n",
       "33 -2.752084                 256                  3             0.1   \n",
       "34  3.742673                 256                  3             0.4   \n",
       "35  0.037709                 256                  3             0.4   \n",
       "36  2.347165                  64                  3             0.1   \n",
       "37  1.798485                  64                  3             0.1   \n",
       "38  1.315977                  64                  3             0.4   \n",
       "39 -1.279688                  64                  3             0.4   \n",
       "40  2.041072                 128                  3             0.1   \n",
       "41  2.140484                 128                  3             0.1   \n",
       "42  1.685515                 128                  3             0.4   \n",
       "43  0.400127                 128                  3             0.4   \n",
       "44  1.647309                 256                  3             0.1   \n",
       "45 -0.834869                 256                  3             0.1   \n",
       "46  1.471520                 256                  3             0.4   \n",
       "47 -0.402955                 256                  3             0.4   \n",
       "\n",
       "    config/bidirectional  config/learning_rate       logdir  \n",
       "0                  False               0.00100  66552_00000  \n",
       "1                   True               0.00100  66552_00001  \n",
       "2                  False               0.00100  66552_00002  \n",
       "3                   True               0.00100  66552_00003  \n",
       "4                  False               0.00100  66552_00004  \n",
       "5                   True               0.00100  66552_00005  \n",
       "6                  False               0.00100  66552_00006  \n",
       "7                   True               0.00100  66552_00007  \n",
       "8                  False               0.00100  66552_00008  \n",
       "9                   True               0.00100  66552_00009  \n",
       "10                 False               0.00100  66552_00010  \n",
       "11                  True               0.00100  66552_00011  \n",
       "12                 False               0.00001  66552_00012  \n",
       "13                  True               0.00001  66552_00013  \n",
       "14                 False               0.00001  66552_00014  \n",
       "15                  True               0.00001  66552_00015  \n",
       "16                 False               0.00001  66552_00016  \n",
       "17                  True               0.00001  66552_00017  \n",
       "18                 False               0.00001  66552_00018  \n",
       "19                  True               0.00001  66552_00019  \n",
       "20                 False               0.00001  66552_00020  \n",
       "21                  True               0.00001  66552_00021  \n",
       "22                 False               0.00001  66552_00022  \n",
       "23                  True               0.00001  66552_00023  \n",
       "24                 False               0.00100  66552_00024  \n",
       "25                  True               0.00100  66552_00025  \n",
       "26                 False               0.00100  66552_00026  \n",
       "27                  True               0.00100  66552_00027  \n",
       "28                 False               0.00100  66552_00028  \n",
       "29                  True               0.00100  66552_00029  \n",
       "30                 False               0.00100  66552_00030  \n",
       "31                  True               0.00100  66552_00031  \n",
       "32                 False               0.00100  66552_00032  \n",
       "33                  True               0.00100  66552_00033  \n",
       "34                 False               0.00100  66552_00034  \n",
       "35                  True               0.00100  66552_00035  \n",
       "36                 False               0.00001  66552_00036  \n",
       "37                  True               0.00001  66552_00037  \n",
       "38                 False               0.00001  66552_00038  \n",
       "39                  True               0.00001  66552_00039  \n",
       "40                 False               0.00001  66552_00040  \n",
       "41                  True               0.00001  66552_00041  \n",
       "42                 False               0.00001  66552_00042  \n",
       "43                  True               0.00001  66552_00043  \n",
       "44                 False               0.00001  66552_00044  \n",
       "45                  True               0.00001  66552_00045  \n",
       "46                 False               0.00001  66552_00046  \n",
       "47                  True               0.00001  66552_00047  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "General_Model = torch.load('/data/Hydra_Work/Post_Rodeo_Work/Tuned_General_Model/General_model.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Hydra Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_models_hydra(body_hindcast_input_size, body_forecast_input_size, body_output_size, body_hidden_size, body_num_layers, body_dropout,\n",
    "                        head_hidden_size, head_num_layers, head_forecast_output_size, head_dropout, bidirectional, basins,\n",
    "                        learning_rate_general_head, learning_rate_head, learning_rate_body, LR = 1e-3, \n",
    "                        additional_specific_head_hindcast_input_size = 1, additional_specific_head_forecast_input_size = 0,\n",
    "                        copies=3, device=None):\n",
    "    Hydra_Bodys = {}\n",
    "    Basin_Heads = {}\n",
    "    General_Heads = {}   \n",
    "    general_optimizers = {}\n",
    "    optimizers = {}\n",
    "    schedulers = {}\n",
    "    \n",
    "    body_forecast_output_size = body_output_size\n",
    "    body_hindcast_output_size = body_output_size\n",
    "    \n",
    "    # Define head hindcast size as head-forecast for simplicty\n",
    "    head_hindcast_output_size = head_forecast_output_size\n",
    "    specific_head_hindcast_output_size = head_forecast_output_size\n",
    "    specific_head_forecast_output_size = head_forecast_output_size\n",
    "    specific_head_hidden_size = head_hidden_size\n",
    "    specific_head_num_layers = head_num_layers\n",
    "    \n",
    "    # Head takes Body as inputs\n",
    "    #head_hindcast_input_size = body_hindcast_input_size \n",
    "    head_hindcast_input_size = body_hindcast_output_size\n",
    "    head_forecast_input_size = body_forecast_output_size\n",
    "    \n",
    "    # Specific input size\n",
    "    specific_head_hindcast_input_size = head_hindcast_input_size + additional_specific_head_hindcast_input_size\n",
    "    specific_head_forecast_input_size = head_forecast_input_size + additional_specific_head_forecast_input_size\n",
    "    \n",
    "\n",
    "    \n",
    "    for copy in range(copies):\n",
    "        Hydra_Bodys[copy] = Google_Model_Block(body_hindcast_input_size, body_forecast_input_size, body_hindcast_output_size, body_forecast_output_size, body_hidden_size, body_num_layers, device, body_dropout, bidirectional)\n",
    "        General_Heads[copy] = Google_Model_Block(head_hindcast_input_size, head_forecast_input_size, head_hindcast_output_size, head_forecast_output_size, head_hidden_size, head_num_layers, device, head_dropout, bidirectional)\n",
    "        Basin_Heads[copy] = Specific_Heads(basins, specific_head_hindcast_input_size, specific_head_forecast_input_size, specific_head_hindcast_output_size, specific_head_forecast_output_size, specific_head_hidden_size, specific_head_num_layers, device, head_dropout, bidirectional)\n",
    "\n",
    "\n",
    "        specific_head_parameters = list()\n",
    "        for basin, model in Basin_Heads[copy].items():\n",
    "            specific_head_parameters += list(model.parameters())\n",
    "\n",
    "        optimizers[copy] = torch.optim.Adam(\n",
    "        # Extra LR is the global learning rate, not really important\n",
    "        [\n",
    "            {\"params\": General_Heads[copy].parameters(), \"lr\": learning_rate_general_head},\n",
    "            {\"params\": specific_head_parameters, \"lr\": learning_rate_head},\n",
    "            {\"params\": Hydra_Bodys[copy].parameters(), \"lr\": learning_rate_body},\n",
    "        ],\n",
    "        lr=LR, )\n",
    "\n",
    "        general_optimizers[copy] = torch.optim.Adam(\n",
    "        # Extra LR is the global learning rate, not really important\n",
    "        [\n",
    "            {\"params\": General_Heads[copy].parameters(), \"lr\": learning_rate_general_head},\n",
    "            {\"params\": Hydra_Bodys[copy].parameters(), \"lr\": learning_rate_body},\n",
    "        ],\n",
    "        lr=LR, )\n",
    "        schedulers[copy] = lr_scheduler.CosineAnnealingLR(optimizers[copy], T_max=1e4)\n",
    "\n",
    "    return Hydra_Bodys, General_Heads, Basin_Heads, optimizers, schedulers, general_optimizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "static_size = np.shape(Static_variables)[1]\n",
    "forecast_size = np.shape(seasonal_forecasts['american_river_folsom_lake_2000_apr'])[1]\n",
    "History_Fourier_in_forcings = 0 #2*3*(6 - 1)\n",
    "Climate_guess = 3\n",
    "History_Statistics_in_forcings = 5*2\n",
    "\n",
    "forecast_input_size = forecast_size + static_size + History_Fourier_in_forcings + History_Statistics_in_forcings  + Climate_guess + 3\n",
    "output_size, head_hidden_size, head_num_layers =  3, 64, 3\n",
    "body_hindcast_input_size = 16\n",
    "body_forecast_input_size = forecast_input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.stopper import TrialPlateauStopper\n",
    "\n",
    "# Fixed parameters\n",
    "total_epochs = 2\n",
    "n_epochs = 1 # Epochs between tests\n",
    "group_lengths = np.arange(180)\n",
    "batch_size = 1\n",
    "copies = 3\n",
    "head_output_size = 3\n",
    "\n",
    "# parameters to tune\n",
    "# chose 128, 2, 0.1, 1e-3, 6, 32, 1, 0.4, 1e-3\n",
    "body_hidden_sizes = [64, 128, 256]\n",
    "body_num_layers = [2] #  [1, 3]\n",
    "body_dropouts = [0.4] #[0.1, 0.4]\n",
    "body_learning_rates = [1e-4] # [1e-3, 1e-5]\n",
    "body_outputs = [6] #[3, 6, 10] # Say hindcast and forecasts have same outputrs body_hindcast_output_size\n",
    "\n",
    "\n",
    "head_hidden_sizes = [16, 32, 64]\n",
    "head_num_layers = [1] #[1, 3]\n",
    "head_dropouts = [0.2] #[0.1, 0.4, 0.7]\n",
    "head_learning_rates = [1e-4] #[1e-3, 1e-5]\n",
    "LR = 1e-3\n",
    "bidirectionals = [True] #[False, True]\n",
    "\n",
    "config_space = {\n",
    "    \"body_hidden_size\": tune.grid_search(body_hidden_sizes),\n",
    "    \"body_num_layer\": tune.grid_search(body_num_layers),\n",
    "    \"body_dropout\": tune.grid_search(body_dropouts),\n",
    "    \"bidirectional\": tune.grid_search(bidirectionals),\n",
    "    \"body_output\": tune.grid_search(body_outputs),\n",
    "    \"body_learning_rate\": tune.grid_search(body_learning_rates),\n",
    "    \"head_hidden_size\": tune.grid_search(head_hidden_sizes),\n",
    "    \"head_num_layer\": tune.grid_search(head_num_layers),\n",
    "    \"head_dropout\": tune.grid_search(head_dropouts),\n",
    "    \"head_learning_rate\": tune.grid_search(head_learning_rates),\n",
    "    \"general_head_learning_rate\": tune.grid_search(head_learning_rates),\n",
    "}\n",
    "\n",
    "# Places to save info\n",
    "model_dir = '/data/Hydra_Work/Post_Rodeo_Work/Tuned_Hydra_Model/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_hydra(config):\n",
    "\n",
    "    All_Dates = ray.get(All_Dates_id)  \n",
    "    Val_Dates = ray.get(Val_Dates_id)  \n",
    "    era5 = ray.get(era5_id)  \n",
    "    daily_flow = ray.get(daily_flow_id)  \n",
    "    climatological_flows = ray.get(climatological_flows_id)\n",
    "    climate_indices = ray.get(climate_indices_id)\n",
    "    seasonal_forecasts = ray.get(seasonal_forecasts_id)\n",
    "    Static_variables = ray.get(Static_variables_id)\n",
    "\n",
    "    copies = 3\n",
    "    warmup = 0    \n",
    "    device = torch.device('cuda' if torch.cuda.\n",
    "                    is_available() else 'cpu')\n",
    "   \n",
    "\n",
    "    Hydra_Bodys, General_Hydra_Heads, model_heads, optimizers, schedulers, general_optimizers  = define_models_hydra(body_hindcast_input_size, body_forecast_input_size, config['body_output'],\n",
    "                                config['body_hidden_size'], config['body_num_layer'], config['body_dropout'], \n",
    "                                config['head_hidden_size'], config['head_num_layer'], 3, config['head_dropout'], config['bidirectional'], basins,\n",
    "                                config['general_head_learning_rate'], config['head_learning_rate'], config['body_learning_rate'], LR, device = device\n",
    "                                )\n",
    "     \n",
    "\n",
    "    general_losses, specific_losses, general_val_losses, specific_val_losses, val_losses = [], [], [], [], []\n",
    "\n",
    "\n",
    "    for epoch in range(total_epochs):\n",
    "        train_general_losses = {}\n",
    "        train_specific_losses = {}\n",
    "        epoch_val_general_losses = {}\n",
    "        epoch_val_specific_losses = {}\n",
    "        climate_losses = {}\n",
    "        \n",
    "        for copy in range(copies):\n",
    "            # Initialise\n",
    "            train_general_losses[copy], train_specific_losses[copy], climate_losses[copy] = Model_Run(All_Dates, basins, Hydra_Bodys[copy], General_Hydra_Heads[copy], model_heads[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts,\n",
    "                Static_variables, general_optimizers[copy], schedulers[copy], criterion, early_stopper= None, n_epochs= warmup,\n",
    "                batch_size=batch_size, group_lengths=group_lengths, Train_Mode=True, device=device, feed_forcing = False)\n",
    "                        \n",
    "\n",
    "            # Full Training\n",
    "            train_general_losses[copy], train_specific_losses[copy], climate_losses[copy] = Model_Run(All_Dates, basins, Hydra_Bodys[copy], General_Hydra_Heads[copy], model_heads[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts,\n",
    "                Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper= None, n_epochs=n_epochs,\n",
    "                batch_size=batch_size, group_lengths=group_lengths, Train_Mode=True, device=device, feed_forcing = False)\n",
    "            epoch_val_general_losses[copy], epoch_val_specific_losses[copy], climate_losses[copy] = Model_Run(Val_Dates, basins, Hydra_Bodys[copy], General_Hydra_Heads[copy], model_heads[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts,\n",
    "                Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper= None, n_epochs=n_epochs,\n",
    "                batch_size=batch_size, group_lengths=group_lengths, Train_Mode=False, device=device, feed_forcing = False)\n",
    "\n",
    "        general_loss = np.mean(list(train_general_losses.values()))\n",
    "        specific_loss = np.mean(list(train_specific_losses.values()))\n",
    "        climate_loss = np.mean(list(climate_losses.values()))\n",
    "        \n",
    "        epoch_val_general_loss = np.mean(list(epoch_val_general_losses.values())).mean()\n",
    "        epoch_val_specific_loss = np.mean(list(epoch_val_specific_losses.values())).mean()\n",
    "        \n",
    "        \n",
    "        general_losses.append(general_loss)\n",
    "        specific_losses.append(specific_loss)\n",
    "        specific_val_losses.append(epoch_val_specific_loss)\n",
    "        general_val_losses.append(epoch_val_general_loss)\n",
    "\n",
    "        val_loss = (0.5*(epoch_val_general_loss + epoch_val_specific_loss) - climate_loss)\n",
    "        ray.train.report({'val_loss' : val_loss})\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "    return val_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asha_scheduler = ASHAScheduler(\n",
    "    time_attr='training_iteration',\n",
    "    metric='val_loss',\n",
    "    mode='min',\n",
    "    max_t=100,\n",
    "    grace_period=10,\n",
    "    reduction_factor=3,\n",
    "    brackets=1,\n",
    ")\n",
    "\n",
    "\n",
    "optuna_search = OptunaSearch(\n",
    "    define_optuna_search_space,\n",
    "    metric=\"val_loss\",\n",
    "    mode=\"min\")\n",
    "\n",
    "plateau_stopper = TrialPlateauStopper(\n",
    "    metric=\"val_loss\",\n",
    "    num_results = 4,\n",
    "    grace_period=10,\n",
    "    mode=\"min\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(config):  \n",
    "    device = torch.device('cuda' if torch.cuda.\n",
    "                      is_available() else 'cpu')\n",
    "    \n",
    "\n",
    "    score = train_model_hydra(config) # Have training loop in here that outputs loss of model\n",
    "    return {\"val_loss\": score}\n",
    "\n",
    "\n",
    "# Can use fractions of GPU\n",
    "tuner = tune.Tuner(tune.with_resources(tune.with_parameters(objective), resources={\"cpu\": 9/10, \"gpu\": 0}), param_space=config_space) \n",
    "\n",
    "results = tuner.fit()\n",
    "best_config = results.get_best_result(metric=\"val_loss\", mode=\"min\").config\n",
    "print(best_config)\n",
    "file_path = f\"/data/Hydra_Work/Tuning/Config_Text/Hydral_Model_best_config.txt\"\n",
    "\n",
    "# Open the file in write mode and save the configuration\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(str(best_config))\n",
    "\n",
    "print(\"Best configuration saved to:\", file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hydra_Code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
