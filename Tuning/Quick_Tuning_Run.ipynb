{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('tableau-colorblind10')\n",
    "\n",
    "import sys\n",
    "sys.path.append('/data/Hydra_Work/Competition_Functions') \n",
    "from Processing_Functions import process_forecast_date, process_seasonal_forecasts\n",
    "\n",
    "import ML_Functions\n",
    "from ML_Functions import Hydra_LSTM_Block, initialize_models_optimizers, PinballLoss, SumPinballLoss, EarlyStopper, Model_Run, No_Body_Model_Run\n",
    "from Data_Transforming import read_nested_csvs, generate_daily_flow, use_USGS_flow_data, USGS_to_daily_df_yearly\n",
    "\n",
    "\n",
    "sys.path.append('/data/Hydra_Work/Pipeline_Functions')\n",
    "from Folder_Work import filter_rows_by_year, csv_dictionary, add_day_of_year_column\n",
    "\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MinMaxScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MinMaxScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MinMaxScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MinMaxScaler from version 1.3.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# All the prep\n",
    "monthly_basins = ['animas_r_at_durango', 'boise_r_nr_boise', 'boysen_reservoir_inflow', 'colville_r_at_kettle_falls', 'detroit_lake_inflow', 'dillon_reservoir_inflow',\n",
    "    'fontenelle_reservoir_inflow', 'green_r_bl_howard_a_hanson_dam', 'hungry_horse_reservoir_inflow', 'libby_reservoir_inflow',\n",
    "    'missouri_r_at_toston','owyhee_r_bl_owyhee_dam', 'pecos_r_nr_pecos', 'pueblo_reservoir_inflow',\n",
    "    'ruedi_reservoir_inflow', 'skagit_ross_reservoir', 'snake_r_nr_heise', 'stehekin_r_at_stehekin', 'sweetwater_r_nr_alcova',\n",
    "    'taylor_park_reservoir_inflow', 'virgin_r_at_virtin', 'weber_r_nr_oakley', 'yampa_r_nr_maybell',\n",
    "]\n",
    "\n",
    "\n",
    "USGS_basins = ['animas_r_at_durango', 'boise_r_nr_boise', 'boysen_reservoir_inflow', 'colville_r_at_kettle_falls', 'detroit_lake_inflow', 'dillon_reservoir_inflow',   \n",
    "    'green_r_bl_howard_a_hanson_dam', 'hungry_horse_reservoir_inflow', 'libby_reservoir_inflow', 'merced_river_yosemite_at_pohono_bridge', 'missouri_r_at_toston',\n",
    "    'owyhee_r_bl_owyhee_dam', 'pecos_r_nr_pecos', 'pueblo_reservoir_inflow',    'san_joaquin_river_millerton_reservoir', 'snake_r_nr_heise', 'stehekin_r_at_stehekin',\n",
    "    'sweetwater_r_nr_alcova', 'taylor_park_reservoir_inflow', 'virgin_r_at_virtin', 'weber_r_nr_oakley', 'yampa_r_nr_maybell',\n",
    "]\n",
    "\n",
    "basins = list(set(monthly_basins + USGS_basins))\n",
    "\n",
    "\n",
    "selected_years = range(2000,2024,2)\n",
    "\n",
    "era5_folder = '/data/Hydra_Work/Rodeo_Data/era5'\n",
    "era5 = csv_dictionary(era5_folder, basins, years=selected_years)\n",
    "era5 = add_day_of_year_column(era5)\n",
    "\n",
    "flow_folder = '/data/Hydra_Work/Rodeo_Data/train_monthly_naturalized_flow'\n",
    "flow = csv_dictionary(flow_folder, monthly_basins)\n",
    "flow = filter_rows_by_year(flow, 1998)\n",
    "\n",
    "climatology_file_path = '/data/Hydra_Work/Rodeo_Data/climate_indices.csv'\n",
    "climate_indices = pd.read_csv(climatology_file_path)\n",
    "climate_indices['date'] = pd.to_datetime(climate_indices['date'])\n",
    "climate_indices.set_index('date', inplace = True)\n",
    "climate_indices.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "climate_indices = climate_indices[~climate_indices.index.duplicated(keep='first')]\n",
    "\n",
    "root_folder = '/data/Hydra_Work/Rodeo_Data/seasonal_forecasts'\n",
    "seasonal_forecasts = read_nested_csvs(root_folder)\n",
    "\n",
    "USGS_flow_folder = '/data/Hydra_Work/Rodeo_Data/USGS_streamflows'\n",
    "USGS_flow = csv_dictionary(USGS_flow_folder, USGS_basins)\n",
    "\n",
    "Static_variables = pd.read_csv('/data/Hydra_Work/Rodeo_Data/static_indices.csv', index_col= 'site_id')\n",
    "\n",
    "# Convert monthly flow values to daily flow estimates\n",
    "daily_flow = {}\n",
    "\n",
    "# Iterate through the dictionary and apply generate_daily_flow to each DataFrame\n",
    "for key, df in flow.items():\n",
    "    daily_flow[key] = generate_daily_flow(df, persistence_factor=0.7)\n",
    "\n",
    "# Replacing monhtly data for normalised USGS when available\n",
    "daily_flow = use_USGS_flow_data(daily_flow, USGS_flow)\n",
    "\n",
    "# Introducing the data from San_jaoqin and Merced, normalised by the yearly flow given\n",
    "path = '/data/Hydra_Work/Rodeo_Data/USGS_streamflows/san_joaquin_river_millerton_reservoir.csv'\n",
    "name = 'san_joaquin_river_millerton_reservoir'\n",
    "normalising_path = '/data/Hydra_Work/Rodeo_Data/train_yearly/san_joaquin_river_millerton_reservoir.csv'\n",
    "\n",
    "USGS_to_daily_df_yearly(daily_flow, path, name, normalising_path)\n",
    "\n",
    "path = '/data/Hydra_Work/Rodeo_Data/USGS_streamflows/merced_river_yosemite_at_pohono_bridge.csv'\n",
    "name = 'merced_river_yosemite_at_pohono_bridge'\n",
    "normalising_path = '/data/Hydra_Work/Rodeo_Data/train_yearly/merced_river_yosemite_at_pohono_bridge.csv'\n",
    "\n",
    "USGS_to_daily_df_yearly(daily_flow, path, name, normalising_path)\n",
    "\n",
    "path = '/data/Hydra_Work/Rodeo_Data/USGS_streamflows/detroit_lake_inflow.csv'\n",
    "name = 'detroit_lake_inflow'\n",
    "normalising_path = '/data/Hydra_Work/Rodeo_Data/train_yearly/detroit_lake_inflow.csv'\n",
    "\n",
    "USGS_to_daily_df_yearly(daily_flow, path, name, normalising_path)\n",
    "\n",
    "climate_scaler_filename = '/data/Hydra_Work/Rodeo_Data/scalers/climate_normalization_scaler.save'\n",
    "climate_scaler = joblib.load(climate_scaler_filename) \n",
    "climate_indices = pd.DataFrame(climate_scaler.transform(climate_indices), columns=climate_indices.columns, index=climate_indices.index)\n",
    "\n",
    "era5_scaler_filename = '/data/Hydra_Work/Rodeo_Data/scalers/era5_scaler.save'\n",
    "era5_scaler = joblib.load(era5_scaler_filename) \n",
    "era5 = {key: pd.DataFrame(era5_scaler.transform(df), columns=df.columns, index=df.index) for key, df in era5.items()}\n",
    "\n",
    "for basin, df in daily_flow.items(): \n",
    "    flow_scaler_filename = f'/data/Hydra_Work/Rodeo_Data/scalers/flows/{basin}_flow_scaler.save'\n",
    "    flow_scaler = joblib.load(flow_scaler_filename) \n",
    "    daily_flow[basin] = pd.DataFrame(flow_scaler.transform(df), columns=df.columns, index=df.index)\n",
    "\n",
    "seasonal_scaler_filename = \"/data/Hydra_Work/Rodeo_Data/scalers/seasonal_scaler.save\"\n",
    "seasonal_scaler = joblib.load(seasonal_scaler_filename)\n",
    "seasonal_forecasts = {key: pd.DataFrame(seasonal_scaler.transform(df), columns=df.columns, index=df.index ) for key, df in seasonal_forecasts.items()}\n",
    "\n",
    "static_scaler_filename = '/data/Hydra_Work/Rodeo_Data/scalers/static_scaler.save'\n",
    "static_scaler = joblib.load(static_scaler_filename) \n",
    "Static_variables = pd.DataFrame(static_scaler.transform(Static_variables), columns=Static_variables.columns, index=Static_variables.index)\n",
    "\n",
    "climatological_flows = {}\n",
    "\n",
    "for basin, df in daily_flow.items():\n",
    "    # Extract day of year and flow values\n",
    "    df['day_of_year'] = df.index.dayofyear\n",
    "\n",
    "    grouped = df.groupby('day_of_year')['daily_flow'].quantile([0.1, 0.5, 0.9]).unstack(level=1)\n",
    "\n",
    "    climatological_flows[basin] = pd.DataFrame({\n",
    "        'day_of_year': grouped.index,\n",
    "        '10th_percentile_flow': grouped[0.1],\n",
    "        '50th_percentile_flow': grouped[0.5],\n",
    "        '90th_percentile_flow': grouped[0.9]\n",
    "    })\n",
    "    \n",
    "    climatological_flows[basin].set_index('day_of_year', inplace=True)\n",
    "\n",
    "    # Drop the temporary 'day_of_year' column from the original dataframe\n",
    "    df.drop(columns='day_of_year', inplace=True)\n",
    "\n",
    "criterion = SumPinballLoss(quantiles = [0.1, 0.5, 0.9])\n",
    "\n",
    "basin = 'animas_r_at_durango' \n",
    "All_Dates = daily_flow[basin].index[\n",
    "    ((daily_flow[basin].index.month < 6) | ((daily_flow[basin].index.month == 6) & (daily_flow[basin].index.day < 25))) &\n",
    "    ((daily_flow[basin].index.year % 2 == 0) | ((daily_flow[basin].index.month > 10) | ((daily_flow[basin].index.month == 10) & (daily_flow[basin].index.day >= 1))))\n",
    "]\n",
    "All_Dates = All_Dates[All_Dates.year > 1998]\n",
    "\n",
    "\n",
    "# Validation Year\n",
    "Val_Dates = All_Dates[All_Dates.year == 2022]\n",
    "All_Dates = All_Dates[All_Dates.year < 2022]\n",
    "\n",
    "\n",
    "basin_to_remove = 'sweetwater_r_nr_alcova'\n",
    "\n",
    "if basin_to_remove in basins:\n",
    "    basins.remove(basin_to_remove)\n",
    "\n",
    "\n",
    "seed = 42 ; torch.manual_seed(seed) ; random.seed(seed) ; np.random.seed(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "days  = 90\n",
    "hidden_variables_size = 17\n",
    "\n",
    "LR = 1e-3\n",
    "static_size = np.shape(Static_variables)[1]\n",
    "forecast_size = np.shape(seasonal_forecasts['american_river_folsom_lake_2000_apr'])[1]\n",
    "History_Fourier_in_forcings = 0 #2*3*(6 - 1)\n",
    "Climate_guess = 3\n",
    "History_Statistics_in_forcings = 5*2\n",
    "\n",
    "head_input_size = forecast_size + static_size + History_Fourier_in_forcings + History_Statistics_in_forcings  + Climate_guess + 3\n",
    "head_output_size = 3\n",
    "\n",
    "# Be careful of this: Trying to unpickle estimator MinMaxScaler from version 1.3.0 when using version 1.4.1.post1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning individual basins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "static_size = np.shape(Static_variables)[1]\n",
    "forecast_size = np.shape(seasonal_forecasts['american_river_folsom_lake_2000_apr'])[1]\n",
    "History_Fourier_in_forcings = 0 #2*3*(6 - 1)\n",
    "Climate_guess = 3\n",
    "History_Statistics_in_forcings = 5*2\n",
    "\n",
    "input_size = forecast_size + static_size + History_Fourier_in_forcings + History_Statistics_in_forcings  + Climate_guess + 3\n",
    "output_size, head_hidden_size, head_num_layers =  3, 64, 3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_models(hidden_size, num_layers, dropout, bidirectional, learning_rate, copies = 3, output_size = 3, input_size = input_size, days = 90, hidden_variables_size = hidden_variables_size, device = device):\n",
    "    models = {}\n",
    "    params_to_optimize = {}\n",
    "    optimizers = {}\n",
    "    schedulers = {}\n",
    "    for copy in range(copies):\n",
    "        models[copy] = Hydra_LSTM_Block(input_size, hidden_size, num_layers, output_size, H0_sequences_size=days * hidden_variables_size, dropout= dropout, bidirectional= bidirectional)\n",
    "        models[copy].to(device)\n",
    "        params_to_optimize[copy] = list(models[copy].parameters())\n",
    "\n",
    "        optimizers[copy] = torch.optim.Adam(params_to_optimize[copy], lr= learning_rate, weight_decay = 1e-3)\n",
    "        schedulers[copy] = lr_scheduler.CosineAnnealingLR(optimizers[copy], T_max=1e4)\n",
    "\n",
    "    return models, params_to_optimize, optimizers, schedulers\n",
    "\n",
    "def update_final_parameters(Final_Parameters, basin, min_val_loss_parameters, min_val_loss):\n",
    "    Final_Parameters['basin'].append(basin)\n",
    "    Final_Parameters['hidden_size'].append(min_val_loss_parameters[0])\n",
    "    Final_Parameters['num_layers'].append(min_val_loss_parameters[1])\n",
    "    Final_Parameters['dropout'].append(min_val_loss_parameters[2])\n",
    "    Final_Parameters['bidirectional'].append(min_val_loss_parameters[3])\n",
    "    Final_Parameters['learning_rate'].append(min_val_loss_parameters[4])\n",
    "    Final_Parameters['val_loss'].append(min_val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.stopper import TrialPlateauStopper\n",
    "# Fixed parameters\n",
    "total_epochs = 30\n",
    "n_epochs = 1  # Epochs between tests\n",
    "group_lengths = np.arange(180)\n",
    "batch_size = 1\n",
    "copies = 3\n",
    "\n",
    "# parameters to tune\n",
    "hidden_sizes = [16, 64, 128]\n",
    "num_layers =  [1,3]\n",
    "dropout = [0.1, 0.4]\n",
    "bidirectional = [False, True]\n",
    "learning_rate = [1e-2, 1e-3, 1e-5]\n",
    "\n",
    "# Set up configuration space\n",
    "config_space = {\n",
    "    \"hidden_size\": tune.grid_search(hidden_sizes),\n",
    "    \"num_layers\": tune.grid_search(num_layers),\n",
    "    \"dropout\": tune.grid_search(dropout),\n",
    "    \"bidirectional\": tune.grid_search(bidirectional),\n",
    "    \"learning_rate\": tune.grid_search(learning_rate)\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(config, basin):\n",
    "\n",
    "    models, params_to_optimize, optimizers, schedulers = define_models(\n",
    "        config[\"hidden_size\"], config[\"num_layers\"], config[\"dropout\"],\n",
    "        config[\"bidirectional\"], config[\"learning_rate\"], copies=copies)\n",
    "    \n",
    "    print('Defining Models works')\n",
    "    \n",
    "    total_epochs = 30\n",
    "    losses, val_losses = [], []\n",
    "    early_stopper = EarlyStopper(patience=4, min_delta=0.01)\n",
    "    for epoch in range(total_epochs):\n",
    "\n",
    "        train_losses = {}\n",
    "        epoch_val_losses = {}\n",
    "        print('True Epoch is', epoch)\n",
    "\n",
    "        for copy in range(copies):\n",
    "            # loop through copies\n",
    "            # Need to fix the outputs of No_Body_Model_Run\n",
    "            # Need to set specialized = False as only give one model, and basins need to be replaced with a list of just the basin in it\n",
    "            train_losses[copy] = No_Body_Model_Run(All_Dates, [basin], models[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts,\n",
    "                Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper=early_stopper, n_epochs=n_epochs,\n",
    "                batch_size=batch_size, group_lengths=group_lengths, Train_Mode=True, device=device, specialized=False)\n",
    "            epoch_val_losses[copy] = No_Body_Model_Run(Val_Dates, [basin], models[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts,\n",
    "                Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper=early_stopper, n_epochs=n_epochs,\n",
    "                batch_size=batch_size, group_lengths=group_lengths, Train_Mode=False, device=device, specialized=False)\n",
    "\n",
    "        loss = np.mean(list(train_losses.values()))\n",
    "        val_loss = np.mean(list(epoch_val_losses.values())).mean()\n",
    "\n",
    "        print('Training loss is', loss)\n",
    "        losses.append(loss)\n",
    "        # Validation\n",
    "        print('Validation loss is', val_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if early_stopper.early_stop(val_loss):\n",
    "            break\n",
    "\n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "\n",
    "    All_Dates = ray.get(All_Dates_id)  \n",
    "    Val_Dates = ray.get(Val_Dates_id)  \n",
    "    era5 = ray.get(era5_id)  \n",
    "    daily_flow = ray.get(daily_flow_id)  \n",
    "    climatological_flows = ray.get(climatological_flows_id)\n",
    "    climate_indices = ray.get(climate_indices_id)\n",
    "    seasonal_forecasts = ray.get(seasonal_forecasts_id)\n",
    "    Static_variables = ray.get(Static_variables_id)\n",
    "\n",
    "    basin = 'stehekin_r_at_stehekin'\n",
    "\n",
    "    copies = 3\n",
    "    import sys\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.\n",
    "                    is_available() else 'cpu')\n",
    "\n",
    "    print(device)\n",
    "    \n",
    "    # def define_models(hidden_size, num_layers, dropout, bidirectional, learning_rate, copies = 3, output_size = 3, input_size = input_size, days = 90, hidden_variables_size = hidden_variables_size, device = device):\n",
    "    #     models = {}\n",
    "    #     params_to_optimize = {}\n",
    "    #     optimizers = {}\n",
    "    #     schedulers = {}\n",
    "    #     for copy in range(copies):\n",
    "    #         models[copy] = Hydra_LSTM_Block(input_size, hidden_size, num_layers, output_size, H0_sequences_size=days * hidden_variables_size, dropout= dropout, bidirectional= bidirectional)\n",
    "    #         models[copy].to(device)\n",
    "    #         params_to_optimize[copy] = list(models[copy].parameters())\n",
    "\n",
    "    #         optimizers[copy] = torch.optim.Adam(params_to_optimize[copy], lr= learning_rate, weight_decay = 1e-3)\n",
    "    #         schedulers[copy] = lr_scheduler.CosineAnnealingLR(optimizers[copy], T_max=1e4)\n",
    "                    \n",
    "    #     return models, params_to_optimize, optimizers, schedulers\n",
    "    \n",
    "    models, params_to_optimize, optimizers, schedulers = define_models(\n",
    "    config[\"hidden_size\"], config[\"num_layers\"], config[\"dropout\"],\n",
    "    config[\"bidirectional\"], config[\"learning_rate\"], copies=copies, device = device)\n",
    "  \n",
    "    print('Defining Models works')\n",
    "\n",
    "    losses, val_losses = [], []\n",
    "    early_stopper = EarlyStopper(patience=4, min_delta=0.01)\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(total_epochs):\n",
    "\n",
    "        train_losses = {}\n",
    "        epoch_val_losses = {}\n",
    "\n",
    "        for copy in range(copies):\n",
    "\n",
    "             # Need to fix the outputs of No_Body_Model_Run\n",
    "            train_losses[copy] = No_Body_Model_Run(All_Dates, [basin], models[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts,\n",
    "                Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper=early_stopper, n_epochs=n_epochs,\n",
    "                batch_size=batch_size, group_lengths=group_lengths, Train_Mode=True, device=device, specialised=False)\n",
    "            epoch_val_losses[copy] = No_Body_Model_Run(Val_Dates, [basin], models[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts,\n",
    "                Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper=early_stopper, n_epochs=n_epochs,\n",
    "                batch_size=batch_size, group_lengths=group_lengths, Train_Mode=False, device=device, specialised=False)\n",
    "\n",
    "        loss = np.mean(list(train_losses.values()))\n",
    "        val_loss = np.mean(list(epoch_val_losses.values())).mean()\n",
    "\n",
    "        tune.report({'val_loss' : val_loss})\n",
    "        print('Training loss is', loss)\n",
    "        losses.append(loss)\n",
    "        # Validation\n",
    "        print('Validation loss is', val_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if early_stopper.early_stop(val_loss):\n",
    "            break\n",
    "    val_loss = 5\n",
    "    return val_loss\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 14:17:53,300\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "from ray import train, tune\n",
    "\n",
    "ray.shutdown()\n",
    "# /data/Hydra_Work/Competition_Functions/\n",
    "ray.init(runtime_env = { \"env_vars\":   {\"PYTHONPATH\": '/data/Hydra_Work/Competition_Functions/' } } )\n",
    "#ray.init(runtime_env = my_runtime_env)\n",
    "         \n",
    "All_Dates_id = ray.put(All_Dates)  \n",
    "Val_Dates_id = ray.put(Val_Dates)  \n",
    "era5_id = ray.put(era5)  \n",
    "daily_flow_id = ray.put(daily_flow)  \n",
    "climatological_flows_id = ray.put(climatological_flows)\n",
    "climate_indices_id = ray.put(climate_indices)\n",
    "seasonal_forecasts_id = ray.put(seasonal_forecasts)\n",
    "Static_variables_id = ray.put(Static_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-03-18 14:20:00</td></tr>\n",
       "<tr><td>Running for: </td><td>00:02:06.03        </td></tr>\n",
       "<tr><td>Memory:      </td><td>9.9/125.9 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 1.0/16 CPUs, 0.0625/1 GPUs (0.0/1.0 accelerator_type:A100D)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 6<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                     </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_4f98e_00000</td><td style=\"text-align: right;\">           1</td><td>/home/gbmc/ray_results/objective_2024-03-18_14-17-54/objective_4f98e_00000_0_bidirectional=False,dropout=0.1000,hidden_size=16,learning_rate=0.0010,num_layers=1_2024-03-18_14-17-54/error.txt </td></tr>\n",
       "<tr><td>objective_4f98e_00001</td><td style=\"text-align: right;\">           1</td><td>/home/gbmc/ray_results/objective_2024-03-18_14-17-54/objective_4f98e_00001_1_bidirectional=False,dropout=0.1000,hidden_size=64,learning_rate=0.0010,num_layers=1_2024-03-18_14-17-54/error.txt </td></tr>\n",
       "<tr><td>objective_4f98e_00002</td><td style=\"text-align: right;\">           1</td><td>/home/gbmc/ray_results/objective_2024-03-18_14-17-54/objective_4f98e_00002_2_bidirectional=False,dropout=0.1000,hidden_size=128,learning_rate=0.0010,num_layers=1_2024-03-18_14-17-54/error.txt</td></tr>\n",
       "<tr><td>objective_4f98e_00003</td><td style=\"text-align: right;\">           1</td><td>/home/gbmc/ray_results/objective_2024-03-18_14-17-54/objective_4f98e_00003_3_bidirectional=False,dropout=0.1000,hidden_size=16,learning_rate=0.0010,num_layers=3_2024-03-18_14-17-54/error.txt </td></tr>\n",
       "<tr><td>objective_4f98e_00004</td><td style=\"text-align: right;\">           1</td><td>/home/gbmc/ray_results/objective_2024-03-18_14-17-54/objective_4f98e_00004_4_bidirectional=False,dropout=0.1000,hidden_size=64,learning_rate=0.0010,num_layers=3_2024-03-18_14-17-54/error.txt </td></tr>\n",
       "<tr><td>objective_4f98e_00005</td><td style=\"text-align: right;\">           1</td><td>/home/gbmc/ray_results/objective_2024-03-18_14-17-54/objective_4f98e_00005_5_bidirectional=False,dropout=0.1000,hidden_size=128,learning_rate=0.0010,num_layers=3_2024-03-18_14-17-54/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc                  </th><th>bidirectional  </th><th style=\"text-align: right;\">  dropout</th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  num_layers</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_4f98e_00000</td><td>ERROR   </td><td>136.156.133.98:279411</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">           16</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           1</td></tr>\n",
       "<tr><td>objective_4f98e_00001</td><td>ERROR   </td><td>136.156.133.98:279417</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           1</td></tr>\n",
       "<tr><td>objective_4f98e_00002</td><td>ERROR   </td><td>136.156.133.98:279426</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           1</td></tr>\n",
       "<tr><td>objective_4f98e_00003</td><td>ERROR   </td><td>136.156.133.98:279427</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">           16</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           3</td></tr>\n",
       "<tr><td>objective_4f98e_00004</td><td>ERROR   </td><td>136.156.133.98:279432</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           3</td></tr>\n",
       "<tr><td>objective_4f98e_00005</td><td>ERROR   </td><td>136.156.133.98:279437</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">           3</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=279411)\u001b[0m Device available is cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=279426)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=279426)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=279411)\u001b[0m cuda\n",
      "\u001b[36m(objective pid=279432)\u001b[0m Defining Models works\n",
      "\u001b[36m(objective pid=279432)\u001b[0m Start of run\n",
      "\u001b[36m(objective pid=279432)\u001b[0m defaultdict(<class 'int'>, {})\n",
      "\u001b[36m(objective pid=279432)\u001b[0m Epoch 1: Training Mode\n",
      "\u001b[36m(objective pid=279432)\u001b[0m loss difference : -1.5062580216060582\n",
      "\u001b[36m(objective pid=279432)\u001b[0m Climatology loss: 19.50871040964917\n",
      "\u001b[36m(objective pid=279437)\u001b[0m Device available is cuda\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=279432)\u001b[0m cuda\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=279417)\u001b[0m Defining Models works\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=279432)\u001b[0m Start of run\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=279432)\u001b[0m defaultdict(<class 'int'>, {})\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=279432)\u001b[0m Epoch 1: Validation Mode\n",
      "\u001b[36m(objective pid=279417)\u001b[0m Epoch 1: Training Mode\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=279411)\u001b[0m loss difference : 3.336619269507272\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=279411)\u001b[0m Climatology loss: 17.216078575679234\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=279411)\u001b[0m Start of run\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=279411)\u001b[0m defaultdict(<class 'int'>, {})\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=279417)\u001b[0m Epoch 1: Validation Mode\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=279432)\u001b[0m Epoch 1: Training Mode\n",
      "\u001b[36m(objective pid=279432)\u001b[0m loss difference : -1.2776896281371395\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=279432)\u001b[0m Climatology loss: 19.2879104589101\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=279432)\u001b[0m Start of run\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=279432)\u001b[0m defaultdict(<class 'int'>, {})\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=279426)\u001b[0m Epoch 1: Training Mode\n",
      "\u001b[36m(objective pid=279417)\u001b[0m Epoch 1: Validation Mode\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=279417)\u001b[0m loss difference : 1.0816285923549107\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=279417)\u001b[0m Climatology loss: 17.22007006440844\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=279417)\u001b[0m Start of run\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=279417)\u001b[0m defaultdict(<class 'int'>, {})\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=279411)\u001b[0m Epoch 1: Training Mode\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=279411)\u001b[0m Epoch 1: Validation Mode\n",
      "\u001b[36m(objective pid=279411)\u001b[0m loss difference : 0.2971030310222081\n",
      "\u001b[36m(objective pid=279411)\u001b[0m Climatology loss: 18.36303089754922\n",
      "\u001b[36m(objective pid=279432)\u001b[0m Start of run\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=279432)\u001b[0m defaultdict(<class 'int'>, {})\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=279427)\u001b[0m Epoch 1: Training Mode\n",
      "\u001b[36m(objective pid=279427)\u001b[0m loss difference : -1.2817347220408983\n",
      "\u001b[36m(objective pid=279427)\u001b[0m Climatology loss: 19.684557062859735\n",
      "\u001b[36m(objective pid=279427)\u001b[0m Epoch 1: Validation Mode\n",
      "\u001b[36m(objective pid=279411)\u001b[0m Start of run\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=279411)\u001b[0m defaultdict(<class 'int'>, {})\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 14:19:57,593\tERROR tune_controller.py:1374 -- Trial task failed for trial objective_4f98e_00004\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(DeprecationWarning): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=279432, ip=136.156.133.98, actor_id=47f5ed0719717b6f66e9392401000000, repr=objective)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/air/_internal/util.py\", line 88, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 115, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 332, in _trainable_func\n",
      "    output = fn()\n",
      "             ^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/util.py\", line 138, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_260971/1859541542.py\", line 8, in objective\n",
      "  File \"/tmp/ipykernel_260971/395923310.py\", line 66, in train_model\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/session.py\", line 121, in report\n",
      "    raise DeprecationWarning(_TUNE_REPORT_DEPRECATION_MSG)\n",
      "DeprecationWarning: `tune.report` is deprecated.\n",
      "Use `ray.train.report` instead -- see the example below:\n",
      "\n",
      "from ray import tune     ->     from ray import train\n",
      "tune.report(metric=1)    ->     train.report({'metric': 1})\n",
      "2024-03-18 14:19:57,664\tERROR tune_controller.py:1374 -- Trial task failed for trial objective_4f98e_00002\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(DeprecationWarning): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=279426, ip=136.156.133.98, actor_id=257996477449f5b6c20dd0f701000000, repr=objective)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/air/_internal/util.py\", line 88, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 115, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 332, in _trainable_func\n",
      "    output = fn()\n",
      "             ^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/util.py\", line 138, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_260971/1859541542.py\", line 8, in objective\n",
      "  File \"/tmp/ipykernel_260971/395923310.py\", line 66, in train_model\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/session.py\", line 121, in report\n",
      "    raise DeprecationWarning(_TUNE_REPORT_DEPRECATION_MSG)\n",
      "DeprecationWarning: `tune.report` is deprecated.\n",
      "Use `ray.train.report` instead -- see the example below:\n",
      "\n",
      "from ray import tune     ->     from ray import train\n",
      "tune.report(metric=1)    ->     train.report({'metric': 1})\n",
      "2024-03-18 14:19:58,439\tERROR tune_controller.py:1374 -- Trial task failed for trial objective_4f98e_00005\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(DeprecationWarning): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=279437, ip=136.156.133.98, actor_id=9f18ecdd5ea1f97e4f0ebdf601000000, repr=objective)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/air/_internal/util.py\", line 88, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 115, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 332, in _trainable_func\n",
      "    output = fn()\n",
      "             ^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/util.py\", line 138, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_260971/1859541542.py\", line 8, in objective\n",
      "  File \"/tmp/ipykernel_260971/395923310.py\", line 66, in train_model\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/session.py\", line 121, in report\n",
      "    raise DeprecationWarning(_TUNE_REPORT_DEPRECATION_MSG)\n",
      "DeprecationWarning: `tune.report` is deprecated.\n",
      "Use `ray.train.report` instead -- see the example below:\n",
      "\n",
      "from ray import tune     ->     from ray import train\n",
      "tune.report(metric=1)    ->     train.report({'metric': 1})\n",
      "2024-03-18 14:19:58,540\tERROR tune_controller.py:1374 -- Trial task failed for trial objective_4f98e_00003\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(DeprecationWarning): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=279427, ip=136.156.133.98, actor_id=a4db9c0c9ef0e0192d212a4101000000, repr=objective)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/air/_internal/util.py\", line 88, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 115, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 332, in _trainable_func\n",
      "    output = fn()\n",
      "             ^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/util.py\", line 138, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_260971/1859541542.py\", line 8, in objective\n",
      "  File \"/tmp/ipykernel_260971/395923310.py\", line 66, in train_model\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/session.py\", line 121, in report\n",
      "    raise DeprecationWarning(_TUNE_REPORT_DEPRECATION_MSG)\n",
      "DeprecationWarning: `tune.report` is deprecated.\n",
      "Use `ray.train.report` instead -- see the example below:\n",
      "\n",
      "from ray import tune     ->     from ray import train\n",
      "tune.report(metric=1)    ->     train.report({'metric': 1})\n",
      "2024-03-18 14:19:59,630\tERROR tune_controller.py:1374 -- Trial task failed for trial objective_4f98e_00001\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(DeprecationWarning): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=279417, ip=136.156.133.98, actor_id=8c23e3ce0e031d656bd2374d01000000, repr=objective)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/air/_internal/util.py\", line 88, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 115, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 332, in _trainable_func\n",
      "    output = fn()\n",
      "             ^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/util.py\", line 138, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_260971/1859541542.py\", line 8, in objective\n",
      "  File \"/tmp/ipykernel_260971/395923310.py\", line 66, in train_model\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/session.py\", line 121, in report\n",
      "    raise DeprecationWarning(_TUNE_REPORT_DEPRECATION_MSG)\n",
      "DeprecationWarning: `tune.report` is deprecated.\n",
      "Use `ray.train.report` instead -- see the example below:\n",
      "\n",
      "from ray import tune     ->     from ray import train\n",
      "tune.report(metric=1)    ->     train.report({'metric': 1})\n",
      "2024-03-18 14:20:00,188\tERROR tune_controller.py:1374 -- Trial task failed for trial objective_4f98e_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/_private/worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(DeprecationWarning): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=279411, ip=136.156.133.98, actor_id=81ae239abe27b2f05269cb9a01000000, repr=objective)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/air/_internal/util.py\", line 88, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 115, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 332, in _trainable_func\n",
      "    output = fn()\n",
      "             ^^^^\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/util.py\", line 138, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_260971/1859541542.py\", line 8, in objective\n",
      "  File \"/tmp/ipykernel_260971/395923310.py\", line 66, in train_model\n",
      "  File \"/home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/trainable/session.py\", line 121, in report\n",
      "    raise DeprecationWarning(_TUNE_REPORT_DEPRECATION_MSG)\n",
      "DeprecationWarning: `tune.report` is deprecated.\n",
      "Use `ray.train.report` instead -- see the example below:\n",
      "\n",
      "from ray import tune     ->     from ray import train\n",
      "tune.report(metric=1)    ->     train.report({'metric': 1})\n",
      "2024-03-18 14:20:00,198\tERROR tune.py:1038 -- Trials did not complete: [objective_4f98e_00000, objective_4f98e_00001, objective_4f98e_00002, objective_4f98e_00003, objective_4f98e_00004, objective_4f98e_00005]\n",
      "2024-03-18 14:20:00,198\tINFO tune.py:1042 -- Total run time: 126.05 seconds (126.02 seconds for the tuning loop).\n",
      "2024-03-18 14:20:00,204\tWARNING experiment_analysis.py:584 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No best trial found for the given metric: score. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m tuner \u001b[38;5;241m=\u001b[39m tune\u001b[38;5;241m.\u001b[39mTuner(tune\u001b[38;5;241m.\u001b[39mwith_resources(tune\u001b[38;5;241m.\u001b[39mwith_parameters(objective), resources\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m16\u001b[39m}), param_space\u001b[38;5;241m=\u001b[39mconfig_space) \n\u001b[1;32m     24\u001b[0m results \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mfit()\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconfig)\n",
      "File \u001b[0;32m~/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/ray/tune/result_grid.py:162\u001b[0m, in \u001b[0;36mResultGrid.get_best_result\u001b[0;34m(self, metric, mode, scope, filter_nan_and_inf)\u001b[0m\n\u001b[1;32m    151\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo best trial found for the given metric: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment_analysis\u001b[38;5;241m.\u001b[39mdefault_metric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis means that no trial has reported this metric\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m     )\n\u001b[1;32m    156\u001b[0m     error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, or all values reported for this metric are NaN. To not ignore NaN \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues, you can set the `filter_nan_and_inf` arg to False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filter_nan_and_inf\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error_msg)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trial_to_result(best_trial)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No best trial found for the given metric: score. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False."
     ]
    }
   ],
   "source": [
    "\n",
    "def objective(config):  \n",
    "    device = torch.device('cuda' if torch.cuda.\n",
    "                      is_available() else 'cpu')\n",
    "    \n",
    "    print('Device available is', device)\n",
    "    \n",
    "\n",
    "    score = train_model(config) # Have training loop in here that outputs loss of model\n",
    "    return {\"score\": score}\n",
    "\n",
    "\n",
    "def dummy_objective(config):  \n",
    "    print('In dummy objective')\n",
    "    import os\n",
    "    # current_directory = os.getcwd()\n",
    "    # print(\"Current working directory:\", current_directory)\n",
    "    print(os.environ['dummy_val'])\n",
    "    score = 0\n",
    "    return {\"score\": score}\n",
    "\n",
    "# Can use fractions of GPU\n",
    "tuner = tune.Tuner(tune.with_resources(tune.with_parameters(objective), resources={\"cpu\": 1, \"gpu\": 1/16}), param_space=config_space) \n",
    "\n",
    "results = tuner.fit()\n",
    "print(results.get_best_result(metric=\"score\", mode=\"min\").config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed parameters\n",
    "total_epochs = 20\n",
    "n_epochs = 1 # Epochs between tests\n",
    "group_lengths = np.arange(180)\n",
    "batch_size = 1\n",
    "copies = 2\n",
    "\n",
    "# parameters to tune\n",
    "hidden_sizes = [64] #[16, 64, 128]\n",
    "num_layers =  [2] #[1,3]\n",
    "dropout = [0.3] # [0.1, 0.4]\n",
    "bidirectional = [True] #[False, True]\n",
    "learning_rate = [1e-3] #[1e-2, 1e-3, 1e-5]\n",
    "\n",
    "model_combinations = list(itertools.product(hidden_sizes, num_layers, dropout, bidirectional, learning_rate))\n",
    "\n",
    "# Places to save info\n",
    "model_dir = '/data/Hydra_Work/Post_Rodeo_Work/Tuned_Single_Models/'\n",
    "Final_Parameters = {'basin': [], 'hidden_size': [], 'num_layers': [], 'dropout': [], 'bidirectional': [], 'learning_rate': [], 'val_loss': []}\n",
    "\n",
    "for basin in basins: \n",
    "    min_val_loss = float('inf')\n",
    "\n",
    "    print(basin)\n",
    "\n",
    "    for model_combination in model_combinations:\n",
    "        hidden_size, num_layers, dropout, bidirectional, learning_rate = model_combination\n",
    "        \n",
    "        models, params_to_optimize, optimizers, schedulers = define_models(hidden_size, num_layers, dropout, bidirectional, learning_rate, copies = copies)        \n",
    "        early_stopper = EarlyStopper(patience = 3, min_delta= 2)\n",
    "        losses, val_losses = [], []\n",
    "\n",
    "        for epoch in range(total_epochs): \n",
    "            # Training\n",
    "            train_losses = {}\n",
    "            epoch_val_losses = {}\n",
    "            print('True Epoch is', epoch)\n",
    "\n",
    "            for copy in range(copies):\n",
    "                # loop through copies\n",
    "                # Need tof fix the outputs of No_Body_Model_Run\n",
    "                # Need to set specialised = False as only give one model, and basins need to be replaced with a list of just the basin in it\n",
    "                dummy = No_Body_Model_Run(All_Dates, [basin], models[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts, Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper = early_stopper, n_epochs=n_epochs, batch_size=batch_size, group_lengths=group_lengths, Train_Mode=True, device=device, specialised = False)\n",
    "                train_losses[copy] = dummy[0][0] - dummy[1][0]\n",
    "                dummy_val = No_Body_Model_Run(Val_Dates, [basin], models[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts, Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper = early_stopper, n_epochs=n_epochs, batch_size=batch_size, group_lengths=group_lengths, Train_Mode= False, device=device, specialised = False)\n",
    "                epoch_val_losses[copy] = dummy_val[0][0] - dummy_val[1][0]\n",
    "\n",
    "            loss = np.mean(list(train_losses.values())).mean()\n",
    "            val_loss = np.mean(list(epoch_val_losses.values())).mean()\n",
    "\n",
    "\n",
    "            print('Training loss is', loss)\n",
    "            losses.append(loss)\n",
    "            # Validation\n",
    "            print('Validation loss is', val_loss)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            if val_loss < min_val_loss:\n",
    "                min_val_loss = val_loss\n",
    "                min_val_loss_parameters = model_combination\n",
    "                min_val_losses = val_losses\n",
    "\n",
    "                save_path = os.path.join(model_dir, f'{basin}_model.pth')\n",
    "                torch.save(models[1], save_path)\n",
    "\n",
    "\n",
    "            if early_stopper.early_stop(val_loss):\n",
    "                break\n",
    "    \n",
    "    # Save best parameters and corresponding validation loss for that basin to a dictionry\n",
    "    update_final_parameters(Final_Parameters, basin, min_val_loss_parameters, min_val_loss)\n",
    "\n",
    "# Convert dictionary to a csv file\n",
    "df = pd.DataFrame(Final_Parameters)\n",
    "df.to_csv('/data/Hydra_Work/Post_Rodeo_Work/Tuned_Single_Models/Specialised_final_parameters.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading models\n",
    "Tuned_Models = {}\n",
    "for basin in basins:\n",
    "    Tuned_Models[basin] = torch.load(f'/data/Hydra_Work/Post_Rodeo_Work/Tuned_Single_Models/basin.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New attempt at raytuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import train\n",
    "\n",
    "ray.shutdown()\n",
    "#ray.init()\n",
    "ray.init(runtime_env = {\"py_modules\": ['/data/Hydra_Work/Competition_Functions']})\n",
    "\n",
    "All_Dates_id = ray.put(All_Dates)  \n",
    "Val_Dates_id = ray.put(Val_Dates)  \n",
    "era5_id = ray.put(era5)  \n",
    "daily_flow_id = ray.put(daily_flow)  \n",
    "climatological_flows_id = ray.put(climatological_flows)\n",
    "climate_indices_id = ray.put(climate_indices)\n",
    "seasonal_forecasts_id = ray.put(seasonal_forecasts)\n",
    "Static_variables_id = ray.put(Static_variables)\n",
    "\n",
    "def trainable(config):  # Pass a \"config\" dictionary into your trainable.\n",
    "\n",
    "\n",
    "    All_Dates = ray.get(All_Dates_id)  \n",
    "    Val_Dates = ray.get(Val_Dates_id)  \n",
    "    era5 = ray.get(era5_id)  \n",
    "    daily_flow = ray.get(daily_flow_id)  \n",
    "    climatological_flows = ray.get(climatological_flows_id)\n",
    "    climate_indices = ray.get(climate_indices_id)\n",
    "    seasonal_forecasts = ray.get(seasonal_forecasts_id)\n",
    "    Static_variables = ray.get(Static_variables_id)\n",
    "\n",
    "    basin = 'stehekin_r_at_stehekin'\n",
    "\n",
    "    copies = 3\n",
    "    device = torch.device('cuda' if torch.cuda.\n",
    "                    is_available() else 'cpu')\n",
    "\n",
    "\n",
    "    sys.path.append('/data/Hydra_Work/Competition_Functions') \n",
    "    import ML_Functions\n",
    "    print('Imported')\n",
    "    from ML_Functions import Hydra_LSTM_Block, EarlyStopper, SumPinballLoss, No_Body_Model_Run, Prepare_Batch, Prepare_Basin, Get_Relevant_Dates, Process_History\n",
    "    from Processing_Functions import process_forecast_date, process_seasonal_forecasts, fit_fourier_to_h0, Get_History_Statistics\n",
    "\n",
    "    def define_models(hidden_size, num_layers, dropout, bidirectional, learning_rate, copies = 3, output_size = 3, input_size = input_size, days = 90, hidden_variables_size = hidden_variables_size, device = device):\n",
    "        models = {}\n",
    "        params_to_optimize = {}\n",
    "        optimizers = {}\n",
    "        schedulers = {}\n",
    "        for copy in range(copies):\n",
    "            models[copy] = Hydra_LSTM_Block(input_size, hidden_size, num_layers, output_size, H0_sequences_size=days * hidden_variables_size, dropout= dropout, bidirectional= bidirectional)\n",
    "            models[copy].to(device)\n",
    "            params_to_optimize[copy] = list(models[copy].parameters())\n",
    "\n",
    "            optimizers[copy] = torch.optim.Adam(params_to_optimize[copy], lr= learning_rate, weight_decay = 1e-3)\n",
    "            schedulers[copy] = lr_scheduler.CosineAnnealingLR(optimizers[copy], T_max=1e4)\n",
    "                    \n",
    "        return models, params_to_optimize, optimizers, schedulers\n",
    "    \n",
    "\n",
    "\n",
    "    models, params_to_optimize, optimizers, schedulers = define_models(\n",
    "    config[\"hidden_size\"], config[\"num_layers\"], config[\"dropout\"],\n",
    "    config[\"bidirectional\"], config[\"learning_rate\"], copies=copies)\n",
    "\n",
    "    losses, val_losses = [], []\n",
    "    early_stopper = EarlyStopper(patience=4, min_delta=0.01)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(total_epochs):\n",
    "\n",
    "        train_losses = {}\n",
    "        epoch_val_losses = {}\n",
    "        print('True Epoch is', epoch)\n",
    "\n",
    "        for copy in range(copies):\n",
    "            train_losses[copy] = 0\n",
    "            # loop through copies\n",
    "            # Need to fix the outputs of No_Body_Model_Run\n",
    "    #         # Need to set specialized = False as only give one model, and basins need to be replaced with a list of just the basin in it\n",
    "            train_losses[copy] = No_Body_Model_Run(All_Dates, [basin], models[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts,\n",
    "                Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper=early_stopper, n_epochs=n_epochs,\n",
    "                batch_size=batch_size, group_lengths=group_lengths, Train_Mode=True, device=device, specialized=False)\n",
    "            # epoch_val_losses[copy] = No_Body_Model_Run(Val_Dates, [basin], models[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts,\n",
    "            #     Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper=early_stopper, n_epochs=n_epochs,\n",
    "            #     batch_size=batch_size, group_lengths=group_lengths, Train_Mode=False, device=device, specialized=False)\n",
    "\n",
    "        loss = np.mean(list(train_losses.values()))\n",
    "        val_loss = np.mean(list(epoch_val_losses.values())).mean()\n",
    "\n",
    "        print('Training loss is', loss)\n",
    "        losses.append(loss)\n",
    "        # Validation\n",
    "        print('Validation loss is', val_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if early_stopper.early_stop(val_loss):\n",
    "            break\n",
    "        \n",
    "        train.report({\"score\": val_loss})  # Send the score to Tune.\n",
    "\n",
    "space = {\n",
    "    \"hidden_size\": tune.choice(hidden_sizes),\n",
    "    \"num_layers\": tune.choice(num_layers),\n",
    "    \"dropout\": tune.grid_search(dropout),\n",
    "    \"bidirectional\": tune.choice(bidirectional),\n",
    "    \"learning_rate\": tune.grid_search(learning_rate)\n",
    "}\n",
    "tuner = tune.Tuner(\n",
    "    trainable, param_space=space, tune_config=tune.TuneConfig(num_samples=10)\n",
    ")\n",
    "tuner.fit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning General Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "static_size = np.shape(Static_variables)[1]\n",
    "forecast_size = np.shape(seasonal_forecasts['american_river_folsom_lake_2000_apr'])[1]\n",
    "History_Fourier_in_forcings = 0 #2*3*(6 - 1)\n",
    "Climate_guess = 3\n",
    "History_Statistics_in_forcings = 5*2\n",
    "\n",
    "input_size = forecast_size + static_size + History_Fourier_in_forcings + History_Statistics_in_forcings  + Climate_guess + 3\n",
    "output_size, head_hidden_size, head_num_layers =  3, 64, 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def update_final_parameters_general(Final_Parameters, min_val_loss_parameters, min_val_loss):\n",
    "    Final_Parameters['hidden_size'].append(min_val_loss_parameters[0])\n",
    "    Final_Parameters['num_layers'].append(min_val_loss_parameters[1])\n",
    "    Final_Parameters['dropout'].append(min_val_loss_parameters[2])\n",
    "    Final_Parameters['bidirectional'].append(min_val_loss_parameters[3])\n",
    "    Final_Parameters['learning_rate'].append(min_val_loss_parameters[4])\n",
    "    Final_Parameters['val_loss'].append(min_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed parameters\n",
    "total_epochs = 20\n",
    "n_epochs = 1 # Epochs between tests\n",
    "group_lengths = np.arange(180)\n",
    "batch_size = 1\n",
    "copies = 2\n",
    "\n",
    "# parameters to tune\n",
    "# I tuned to 128,2,0.1,False,1e-3 \n",
    "hidden_sizes = [64, 128, 256]\n",
    "num_layers = [1,3]\n",
    "dropout = [0.1, 0.4]\n",
    "bidirectional =  [False, True]\n",
    "learning_rate = [1e-2, 1e-3, 1e-5]\n",
    "\n",
    "model_combinations = list(itertools.product(hidden_sizes, num_layers, dropout, bidirectional, learning_rate))\n",
    "\n",
    "# Places to save info\n",
    "model_dir = '/data/Hydra_Work/Post_Rodeo_Work/Tuned_General_Model/'\n",
    "Final_Parameters = {'hidden_size': [], 'num_layers': [], 'dropout': [], 'bidirectional': [], 'learning_rate': [], 'val_loss': []}\n",
    "\n",
    "\n",
    "min_val_loss = float('inf')\n",
    "\n",
    "for model_combination in model_combinations:\n",
    "    hidden_size, num_layers, dropout, bidirectional, learning_rate = model_combination\n",
    "    \n",
    "    models, params_to_optimize, optimizers, schedulers = define_models(hidden_size, num_layers, dropout, bidirectional, learning_rate, copies = copies)        \n",
    "    early_stopper = EarlyStopper(patience=10, min_delta=0.01)\n",
    "    losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(total_epochs): \n",
    "        # Training\n",
    "        train_losses = {}\n",
    "        epoch_val_losses = {}\n",
    "        print('True Epoch is', epoch)\n",
    "\n",
    "        for copy in range(copies):\n",
    "            # loop through copies\n",
    "            # Need tof fix the outputs of No_Body_Model_Run\n",
    "            train_losses[copy] = No_Body_Model_Run(All_Dates, basins, models[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts, Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper = early_stopper, n_epochs=n_epochs, batch_size=batch_size, group_lengths=group_lengths, Train_Mode=True, device=device, specialised = False)\n",
    "            epoch_val_losses[copy] = No_Body_Model_Run(Val_Dates, basins, models[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts, Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper = early_stopper, n_epochs=n_epochs, batch_size=batch_size, group_lengths=group_lengths, Train_Mode= False, device=device, specialised = False)\n",
    "\n",
    "\n",
    "        loss = np.mean(list(train_losses.values()))\n",
    "        val_loss = np.mean(list(epoch_val_losses.values())).mean()\n",
    "\n",
    "        print('Training loss is', loss)\n",
    "        losses.append(loss)\n",
    "        # Validation\n",
    "        print('Validation loss is', val_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            min_val_loss_parameters = model_combination\n",
    "            min_val_losses = val_losses\n",
    "\n",
    "            save_path = os.path.join(model_dir, f'General_model.pth')\n",
    "            torch.save(models[1], save_path)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "        if early_stopper.early_stop(val_loss):\n",
    "            break\n",
    "\n",
    "# Save best parameters and corresponding validation loss for that basin to a dictionry\n",
    "update_final_parameters(Final_Parameters, min_val_loss_parameters, min_val_loss)\n",
    "\n",
    "# Convert dictionary to a csv file\n",
    "df = pd.DataFrame(Final_Parameters)\n",
    "df.to_csv('/data/Hydra_Work/Post_Rodeo_Work/Tuned_General_Model/General_final_parameters.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "General_Model = torch.load('/data/Hydra_Work/Post_Rodeo_Work/Tuned_General_Model/General_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Hydra Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_models_hydra(body_input_size, body_hidden_size, body_num_layers, body_output_size, body_dropout, body_bidirectional,\n",
    "                                 head_input_size, head_hidden_size, head_num_layers, head_output_size, head_dropout, head_bidirectional,\n",
    "                        learning_rate_body, learning_rate_head, learning_rate_general_head, LR, basins = basins,  hidden_variables_size = hidden_variables_size, days = 90, device = device, copies = 3):\n",
    "    Hydra_Bodys = {}\n",
    "    model_heads = {}\n",
    "    General_Hydra_Heads = {}\n",
    "\n",
    "    params_to_optimize = {}\n",
    "    optimizers = {}\n",
    "    schedulers = {}\n",
    "    for copy in range(copies):\n",
    "        Hydra_Bodys[copy], model_heads[copy], General_Hydra_Heads[copy], optimizers[copy], schedulers[copy] = initialize_models_optimizers(basins, body_input_size, body_hidden_size, body_num_layers, body_output_size, body_dropout, body_bidirectional,\n",
    "                            head_input_size, head_hidden_size, head_num_layers, head_output_size, head_dropout, head_bidirectional,\n",
    "                            days, hidden_variables_size, learning_rate_body, learning_rate_head, learning_rate_general_head, LR, device)\n",
    "\n",
    "    return Hydra_Bodys, General_Hydra_Heads, model_heads, optimizers, schedulers \n",
    "\n",
    "def update_final_parameters_hydra(Final_Parameters, min_val_loss_parameters, min_val_loss):\n",
    "    # Append body parameters\n",
    "    Final_Parameters['body_hidden_size'].append(min_val_loss_parameters[0])\n",
    "    Final_Parameters['body_num_layers'].append(min_val_loss_parameters[1])\n",
    "    Final_Parameters['body_dropout'].append(min_val_loss_parameters[2])\n",
    "    Final_Parameters['body_learning_rate'].append(min_val_loss_parameters[3])\n",
    "    Final_Parameters['body_output'].append(min_val_loss_parameters[4])\n",
    "    # Append head parameters\n",
    "    Final_Parameters['head_hidden_size'].append(min_val_loss_parameters[5])\n",
    "    Final_Parameters['head_num_layers'].append(min_val_loss_parameters[6])\n",
    "    Final_Parameters['head_dropout'].append(min_val_loss_parameters[7])\n",
    "    Final_Parameters['head_learning_rate'].append(min_val_loss_parameters[8])\n",
    "    # Append validation loss\n",
    "    Final_Parameters['val_loss'].append(min_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed parameters\n",
    "total_epochs = 20\n",
    "n_epochs = 1 # Epochs between tests\n",
    "group_lengths = np.arange(180)\n",
    "batch_size = 1\n",
    "copies = 1\n",
    "body_input_size = input_size\n",
    "head_output_size = 3\n",
    "\n",
    "# parameters to tune\n",
    "# chose 128, 2, 0.1, 1e-3, 6, 32, 1, 0.4, 1e-3\n",
    "body_hidden_sizes = [256] #[64, 128, 256]\n",
    "body_num_layers =  [2] #[1, 3]\n",
    "body_dropouts = [0.1] # [0.1, 0.4]\n",
    "body_learning_rates = [1e-3] # [1e-2, 1e-3, 1e-5]\n",
    "body_outputs = [10] #[3, 6, 10]\n",
    "\n",
    "\n",
    "head_hidden_sizes = [64] # [16, 32, 64]\n",
    "head_num_layers = [1] #[1, 3]\n",
    "head_dropouts = [0.4] # [0.1, 0.4, 0.7]\n",
    "head_learning_rates = [1e-3] #[1e-2, 1e-3, 1e-5]\n",
    "\n",
    "bidirectionals = [True] #[False, True]\n",
    "\n",
    "\n",
    "# Generate combinations for body and head\n",
    "model_combinations = list(itertools.product(body_hidden_sizes, body_num_layers, body_dropouts, bidirectionals, body_learning_rates, body_outputs, head_hidden_sizes, head_num_layers, head_dropouts, bidirectionals, head_learning_rates))\n",
    "\n",
    "# Places to save info\n",
    "model_dir = '/data/Hydra_Work/Post_Rodeo_Work/Tuned_Hydra_Model/'\n",
    "Final_Parameters = {'body_hidden_size': [], 'body_num_layers': [], 'body_dropout': [], 'body_learning_rate': [],\n",
    "                    'head_hidden_size': [], 'head_num_layers': [], 'head_dropout': [], 'bidirectional': [], 'head_learning_rate': [],\n",
    "                    'val_loss': []}\n",
    "\n",
    "\n",
    "min_val_loss = float('inf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for model_combination in model_combinations:\n",
    "    body_hidden_size, body_num_layer, body_dropout, bidirectional, body_learning_rate, body_output_size, head_hidden_size, head_num_layer, head_dropout, bidirectional, head_learning_rate = model_combination\n",
    "    general_head_learning_rate = head_learning_rate\n",
    "    head_input_size = body_output_size\n",
    "    Hydra_Bodys, General_Hydra_Heads, model_heads, optimizers, schedulers  = define_models_hydra(body_input_size, body_hidden_size, body_num_layer, body_output_size, body_dropout, bidirectional,\n",
    "                                 head_input_size, head_hidden_size, head_num_layer, head_output_size, head_dropout, bidirectional,\n",
    "                                body_learning_rate, head_learning_rate, general_head_learning_rate, LR)        \n",
    "    \n",
    "    early_stopper = EarlyStopper(patience=10, min_delta=0.01)\n",
    "    general_losses, specific_losses, general_val_losses, specific_val_losses = [], [], [], []\n",
    "\n",
    "    for epoch in range(total_epochs): \n",
    "        # Training\n",
    "        train_general_losses = {}\n",
    "        train_specific_losses = {}\n",
    "        epoch_val_general_losses = {}\n",
    "        epoch_val_specific_losses = {}\n",
    "        climate_losses = {}\n",
    "        print('True Epoch is', epoch)\n",
    "\n",
    "        for copy in range(copies):\n",
    "            # loop through copies\n",
    "            # Need tof fix the outputs of No_Body_Model_Run\n",
    "            train_general_losses[copy], train_specific_losses[copy], climate_losses[copy] = Model_Run(All_Dates, basins, Hydra_Bodys[copy], General_Hydra_Heads[copy], model_heads[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts, Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper = early_stopper, n_epochs= n_epochs, batch_size=batch_size, group_lengths=group_lengths, Train_Mode=True, device=device, feed_forcing = False)\n",
    "\n",
    "            epoch_val_general_losses[copy], epoch_val_specific_losses[copy], climate_losses[copy] = Model_Run(All_Dates, basins, Hydra_Bodys[copy], General_Hydra_Heads[copy], model_heads[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts, Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper = early_stopper, n_epochs= n_epochs, batch_size=batch_size, group_lengths=group_lengths, Train_Mode=False, device=device, feed_forcing = False)\n",
    "\n",
    "\n",
    "        general_loss = np.mean(list(train_general_losses.values()))\n",
    "        specific_loss = np.mean(list(train_specific_losses.values()))\n",
    "        \n",
    "        epoch_val_general_loss = np.mean(list(epoch_val_general_losses.values())).mean()\n",
    "        epoch_val_specific_loss = np.mean(list(epoch_val_specific_losses.values())).mean()\n",
    "\n",
    "        print('General Training loss is', general_loss)\n",
    "        print('Specific Training loss is', specific_loss)\n",
    "        general_losses.append(general_loss)\n",
    "        specific_losses.append(specific_loss)\n",
    "\n",
    "        # Validation\n",
    "        print('General Validation loss is', epoch_val_general_loss)\n",
    "        general_val_losses.append(epoch_val_general_loss)\n",
    "        print('Specific Validation loss is', epoch_val_specific_loss)\n",
    "        specific_val_losses.append(epoch_val_specific_loss)\n",
    "\n",
    "        val_loss = epoch_val_general_loss\n",
    "        val_losses = general_val_losses\n",
    "        # Need to make a decision on how to determine a model performs better, specific vs general head performance\n",
    "        # Could also just do both\n",
    "\n",
    "        if general_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            min_val_loss_parameters = model_combination\n",
    "            min_val_losses = val_losses\n",
    "\n",
    "            for basin in basins:\n",
    "                # Construct the full path for saving the model\n",
    "                save_path = os.path.join(model_dir, f'{basin}_Head.pth')\n",
    "                # Save the model \n",
    "                torch.save(model_heads[0][f'{basin}'], save_path)\n",
    "\n",
    "\n",
    "            save_path = os.path.join(model_dir, f'General_Head.pth')\n",
    "            # Save the model\n",
    "            torch.save(General_Hydra_Heads[0], save_path)\n",
    "\n",
    "            save_path = os.path.join(model_dir, f'General_Body.pth')\n",
    "            # Save the model\n",
    "            torch.save(Hydra_Bodys[0], save_path)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "        if early_stopper.early_stop(val_loss):\n",
    "            break\n",
    "\n",
    "# Save best parameters and corresponding validation loss for that basin to a dictionry\n",
    "update_final_parameters(Final_Parameters, min_val_loss_parameters, min_val_loss)\n",
    "\n",
    "# Convert dictionary to a csv file\n",
    "df = pd.DataFrame(Final_Parameters)\n",
    "df.to_csv('/data/Hydra_Work/Post_Rodeo_Work/Tuned_General_Model/General_final_parameters.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses.values()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rodeo_V2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
