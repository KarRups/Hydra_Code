{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('tableau-colorblind10')\n",
    "\n",
    "import sys\n",
    "sys.path.append('/data/Hydra_Work/Competition_Functions') \n",
    "from Processing_Functions import process_forecast_date, process_seasonal_forecasts\n",
    "from Data_Transforming import read_nested_csvs, generate_daily_flow, use_USGS_flow_data, USGS_to_daily_df_yearly\n",
    "\n",
    "sys.path.append('/data/Hydra_Work/Pipeline_Functions')\n",
    "from Folder_Work import filter_rows_by_year, csv_dictionary, add_day_of_year_column\n",
    "\n",
    "sys.path.append('/data/Hydra_Work/Post_Rodeo_Work/ML_Functions.py')\n",
    "from Full_LSTM_ML_Functions import Specific_Heads, Google_Model_Block, SumPinballLoss, EarlyStopper, Model_Run, No_Body_Model_Run\n",
    "\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the cross validation set\n",
    "\n",
    "Cross Validation decisions:\n",
    "- It looks like I only have 10 years right now, and if the results are good I can keep it that way (justify by independent years)\n",
    "- Training set of 80% and Validation of 20% is fine, makes sense to make the Validation years adjacent instead of random, probably doesn't matter much but adjacent minimises theyre connection with the years in the training dataset\n",
    "- This means theres only 5 folds which shouldn't take forever to do \n",
    "- There's an issue right now where my validation set is also my test set, how much can I get around this?\n",
    "- I could test a 70-20-10 set up, from the looks of it there won't be that much loss in performance by reducing the training set by 12%? \n",
    "- If I assume the years are independent then it doesn't matter which dates I choose for validation years when I've got a specific testing year\n",
    "- K -fold cross validation means splitting the data in k chunks and choosing a different chunk for each, p-fold involves choosing all possible combinations of size p for the splits\n",
    "\n",
    "Structure of the folders:\n",
    "- Can do Validation_Models/Val_Years/Model/.pth, bs Model/Val_Years/.pth\n",
    "- I think the first makes more sense, I would realy want to ompare models trained over the same years\n",
    "\n",
    "\n",
    "Restructuring Current code:\n",
    "- I want to fit this whole thing into a for loop so I can run it\n",
    "- Alternatively I can have the validation years as a parameter in the config_space and just let the code run as is\n",
    "- It would be nice to make the prep section smaller visually, or hidden somewhere else\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hydra_Code\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "def get_env():\n",
    "    sp = sys.path[1].split(\"/\")\n",
    "    if \"envs\" in sp:\n",
    "        return sp[sp.index(\"envs\") + 1]\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "print(get_env())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "basins = ['libby_reservoir_inflow',  'owyhee_r_bl_owyhee_dam',  'san_joaquin_river_millerton_reservoir',  'taylor_park_reservoir_inflow',\n",
    " 'boise_r_nr_boise', 'green_r_bl_howard_a_hanson_dam', 'weber_r_nr_oakley', 'detroit_lake_inflow', 'virgin_r_at_virtin', 'dillon_reservoir_inflow',\n",
    " 'pueblo_reservoir_inflow', 'hungry_horse_reservoir_inflow', 'stehekin_r_at_stehekin', 'pecos_r_nr_pecos', 'snake_r_nr_heise', 'yampa_r_nr_maybell',\n",
    " 'colville_r_at_kettle_falls', 'missouri_r_at_toston', 'merced_river_yosemite_at_pohono_bridge', 'animas_r_at_durango','fontenelle_reservoir_inflow', 'boysen_reservoir_inflow']\n",
    "\n",
    "selected_years = range(2000,2024,2)\n",
    "\n",
    "\n",
    "base_dir = \"/data/Hydra_Work/Scaled_Data\"\n",
    "\n",
    "# Define dictionaries and DataFrames\n",
    "dictionaries = ['era5', 'seasonal_forecasts', 'daily_flow', 'climatological_flows']\n",
    "\n",
    "dataframes = ['climate_indices', 'static_variables']\n",
    "\n",
    "# Function to load dictionaries\n",
    "def load_dictionaries(base_dir, names):\n",
    "    loaded_dicts = {}\n",
    "    for name in names:\n",
    "        file_path = os.path.join(base_dir, f\"{name}.pkl\")\n",
    "        with open(file_path, 'rb') as file:\n",
    "            locals()[name] = pickle.load(file)\n",
    "    return locals()\n",
    "\n",
    "# Function to load DataFrames\n",
    "def load_dataframes(base_dir, names):\n",
    "    loaded_dfs = {}\n",
    "    for name in names:\n",
    "        file_path = os.path.join(base_dir, f\"{name}.pkl\")\n",
    "        locals()[name] = pd.read_pickle(file_path)\n",
    "    return locals()\n",
    "\n",
    "saved_dicts = load_dictionaries(base_dir, dictionaries)\n",
    "saved_dfs = load_dataframes(base_dir, dataframes)\n",
    "\n",
    "for name in dictionaries:\n",
    "    locals()[name] = saved_dicts[name]\n",
    "\n",
    "for name in dataframes:\n",
    "    locals()[name] = saved_dfs[name]\n",
    "\n",
    "criterion = SumPinballLoss(quantiles = [0.1, 0.5, 0.9])\n",
    "\n",
    "basin = 'animas_r_at_durango' \n",
    "All_Dates = daily_flow[basin].index[\n",
    "    ((daily_flow[basin].index.month < 6) | ((daily_flow[basin].index.month == 6) & (daily_flow[basin].index.day < 24))) &\n",
    "    ((daily_flow[basin].index.year % 2 == 0) | ((daily_flow[basin].index.month > 10) | ((daily_flow[basin].index.month == 10) & (daily_flow[basin].index.day >= 1))))\n",
    "]\n",
    "All_Dates = All_Dates[All_Dates.year > 1998]\n",
    "\n",
    "\n",
    "# Validation Year\n",
    "Val_Dates = All_Dates[All_Dates.year >= 2020]\n",
    "All_Dates = All_Dates[All_Dates.year < 2020]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.\n",
    "                is_available() else 'cpu')\n",
    "\n",
    "criterion = SumPinballLoss(quantiles = [0.1, 0.5, 0.9])\n",
    "\n",
    "basin = 'animas_r_at_durango' \n",
    "All_Dates = daily_flow[basin].index[\n",
    "    ((daily_flow[basin].index.month < 6) | ((daily_flow[basin].index.month == 6) & (daily_flow[basin].index.day < 24))) &\n",
    "    ((daily_flow[basin].index.year % 2 == 0) | ((daily_flow[basin].index.month > 10) | ((daily_flow[basin].index.month == 10) & (daily_flow[basin].index.day >= 1))))\n",
    "]\n",
    "All_Dates = All_Dates[All_Dates.year > 1998]\n",
    "\n",
    "\n",
    "# Validation Year\n",
    "Val_Dates = All_Dates[All_Dates.year >= 2018]\n",
    "Val_Dates = Val_Dates[Val_Dates.year <= 2022]\n",
    "Train_Dates = All_Dates[All_Dates.year == 2022]\n",
    "\n",
    "seed = 42 ; torch.manual_seed(seed) ; random.seed(seed) ; np.random.seed(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "days  = 90\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning individual basins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "static_size = np.shape(static_variables)[1]\n",
    "forecast_size = np.shape(seasonal_forecasts['american_river_folsom_lake_2000_apr'])[1]\n",
    "History_Fourier_in_forcings = 0 #2*3*(6 - 1)\n",
    "Climate_guess = 0 #3 # THis is about climatology, not climate indices\n",
    "History_Statistics_in_forcings = 0  #5*2\n",
    "\n",
    "forecast_input_size = forecast_size + static_size + History_Fourier_in_forcings + History_Statistics_in_forcings  + Climate_guess + 3\n",
    "output_size, head_hidden_size, head_num_layers =  3, 64, 3\n",
    "hindcast_input_size = 9 # 17 if we include climate indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Retrain_Basins = basins\n",
    "for basin in basins:\n",
    "    loss_path = f'/data/Hydra_Work/Tuning/Week_Ahead_Models_V2/Specific_Week_Ahead_Models/{basin}_specific_loss.txt'\n",
    "    \n",
    "    with open(loss_path, 'r') as file:\n",
    "    # Read the entire contents of the file\n",
    "        Overall_Best_Val_Loss = float(file.read())\n",
    "    \n",
    "    if Overall_Best_Val_Loss < -0.05:\n",
    "        Retrain_Basins = list(set(Retrain_Basins) - set([basin]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Retrain_Basins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we want hindcast and forecast num-layers to be different?\n",
    "def define_models(hindcast_input_size, forecast_input_size, hidden_size, num_layers, dropout, bidirectional, learning_rate, copies = 3, forecast_output_size = 3, device = device):\n",
    "    models = {}\n",
    "    params_to_optimize = {}\n",
    "    optimizers = {}\n",
    "    schedulers = {}\n",
    "    \n",
    "    hindcast_output_size = forecast_output_size\n",
    "    for copy in range(copies):\n",
    "        models[copy] = Google_Model_Block(hindcast_input_size, forecast_input_size, hindcast_output_size, forecast_output_size, hidden_size, num_layers, device, dropout, bidirectional)\n",
    "        \n",
    "        models[copy].to(device)\n",
    "        params_to_optimize[copy] = list(models[copy].parameters())\n",
    "        # Probably should be doing 1e-2 and 10\n",
    "        optimizers[copy] = torch.optim.Adam(params_to_optimize[copy], lr= learning_rate, weight_decay = 1e-3)\n",
    "        schedulers[copy] = lr_scheduler.CosineAnnealingLR(optimizers[copy], T_max = 50)\n",
    "        #.StepLR(optimizers[copy], 5, gamma=0.5)\n",
    "        \n",
    "\n",
    "    return models, params_to_optimize, optimizers, schedulers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.stopper import TrialPlateauStopper\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "import optuna\n",
    "\n",
    "# Fixed parameters\n",
    "total_epochs = 40\n",
    "n_epochs = 1  # Epochs between tests\n",
    "group_lengths = [7] #np.arange(180) 7 Day ahead for streamlined version\n",
    "batch_size = 1\n",
    "copies = 1\n",
    "\n",
    "# parameters to tune\n",
    "hidden_sizes = [128] # 64 converged upon\n",
    "num_layers =  [1]\n",
    "dropout = [0.1]\n",
    "bidirectional = [False] #[True, False]\n",
    "learning_rate = [1e-3, 1e-4] #[1e-3, 1e-5]\n",
    "\n",
    "\n",
    "# Set up configuration space\n",
    "config_space = {\n",
    "\n",
    "    \"hidden_size\": tune.grid_search(hidden_sizes),\n",
    "    \"num_layers\": tune.grid_search(num_layers),\n",
    "    \"dropout\": tune.grid_search(dropout),\n",
    "    \"bidirectional\": tune.grid_search(bidirectional),\n",
    "    \"learning_rate\": tune.grid_search(learning_rate),\n",
    "    \"basin\":  tune.grid_search(basins),\n",
    "    'test_year': tune.grid_search(list(np.arange(2000,2024,2)) )\n",
    "\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "\n",
    "    All_Dates = ray.get(All_Dates_id)  \n",
    "\n",
    "    years = list(np.arange(2000,2024,2))\n",
    "    test_year = config['test_year']\n",
    "    val_years = [years[years.index(test_year)-1], years[years.index(test_year)-2]  ]\n",
    "    train_years = [year for year in years if year not in [test_year] + val_years]\n",
    "    \n",
    "    Test_Dates = All_Dates[All_Dates.year == test_year]\n",
    "    Val_Dates = All_Dates[All_Dates.year.isin(val_years)]\n",
    "    Train_Dates = All_Dates[All_Dates.year.isin(train_years)]\n",
    "\n",
    "    era5 = ray.get(era5_id)  \n",
    "    daily_flow = ray.get(daily_flow_id)  \n",
    "    climatological_flows = ray.get(climatological_flows_id)\n",
    "    climate_indices = ray.get(climate_indices_id)\n",
    "    seasonal_forecasts = ray.get(seasonal_forecasts_id)\n",
    "    Static_variables = ray.get(Static_variables_id)\n",
    "\n",
    "    val_loss = 1000\n",
    "\n",
    "    basin = config[\"basin\"]\n",
    "\n",
    "    save_path = f'/data/Hydra_Work/Validation_Models/{test_year}/Specific_LSTM_Model/{basin}_specific.pth'\n",
    "    loss_path = f'/data/Hydra_Work/Validation_Models/{test_year}/Specific_LSTM_Model/{basin}_specific_loss.txt'\n",
    "\n",
    "    \n",
    "    if not os.path.exists(loss_path):\n",
    "        # If the file does not exist, create it and write val_loss to it\n",
    "        with open(loss_path, 'w') as file:\n",
    "            file.write('%f' % val_loss)\n",
    "    \n",
    "    copies = 1\n",
    "    device = torch.device('cuda' if torch.cuda.\n",
    "                    is_available() else 'cpu')\n",
    "    \n",
    "    models, params_to_optimize, optimizers, schedulers = define_models(hindcast_input_size, forecast_input_size,\n",
    "    config[\"hidden_size\"], config[\"num_layers\"], config[\"dropout\"],\n",
    "    config[\"bidirectional\"], config[\"learning_rate\"], copies=copies, device = device)\n",
    "\n",
    "    losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(total_epochs):\n",
    "\n",
    "        train_losses = {}\n",
    "        epoch_val_losses = {}\n",
    "\n",
    "        for copy in range(copies):\n",
    "\n",
    "             # Need to fix the outputs of No_Body_Model_Run\n",
    "            train_losses[copy], Climate_Loss = No_Body_Model_Run(All_Dates, [basin], models[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts,\n",
    "                Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper= None, n_epochs=n_epochs,\n",
    "                batch_size=batch_size, group_lengths=group_lengths, Train_Mode=True, device=device, specialised=False)\n",
    "            epoch_val_losses[copy], Climate_Loss = No_Body_Model_Run(Val_Dates, [basin], models[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts,\n",
    "                Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper= None, n_epochs=n_epochs,\n",
    "                batch_size=batch_size, group_lengths=group_lengths, Train_Mode=False, device=device, specialised=False)\n",
    "\n",
    "        loss = np.mean(list(train_losses.values())) - Climate_Loss\n",
    "        \n",
    "\n",
    "        candidate_val_loss = ((np.mean( list(epoch_val_losses.values()) ).mean() - Climate_Loss)[0])/np.mean(Climate_Loss)\n",
    "        val_loss = np.min([val_loss, candidate_val_loss ])\n",
    "        if candidate_val_loss == val_loss:\n",
    "             torch.save(models[0], save_path)\n",
    "             \n",
    "        \n",
    "        # Check best loss so far for this model\n",
    "        with open(loss_path, 'r') as file:\n",
    "            # Read the entire contents of the file\n",
    "            Overall_Best_Val_Loss = float(file.read())\n",
    "\n",
    "        if val_loss < Overall_Best_Val_Loss:\n",
    "            torch.save(models[0], save_path)\n",
    "\n",
    "            with open(loss_path, 'w') as f:\n",
    "                f.write('%f' % val_loss)\n",
    "\n",
    "\n",
    "        ray.train.report({'val_loss' : val_loss})\n",
    "        print(candidate_val_loss)\n",
    "        losses.append(loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import train, tune\n",
    "\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(runtime_env = { \"env_vars\":   {\"PYTHONPATH\": '/data/Hydra_Work/Competition_Functions/' } } )\n",
    "         \n",
    "All_Dates_id = ray.put(All_Dates)  \n",
    "Val_Dates_id = ray.put(Val_Dates)  \n",
    "era5_id = ray.put(era5)  \n",
    "daily_flow_id = ray.put(daily_flow)  \n",
    "climatological_flows_id = ray.put(climatological_flows)\n",
    "climate_indices_id = ray.put(climate_indices)\n",
    "seasonal_forecasts_id = ray.put(seasonal_forecasts)\n",
    "Static_variables_id = ray.put(static_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asha_scheduler = ASHAScheduler(\n",
    "    time_attr='training_iteration',\n",
    "    metric='val_loss',\n",
    "    mode='min',\n",
    "    max_t=100,\n",
    "    grace_period=20,\n",
    "    reduction_factor=2,\n",
    "    brackets=1,\n",
    ")\n",
    "\n",
    "\n",
    "plateau_stopper = TrialPlateauStopper(\n",
    "    metric=\"val_loss\",\n",
    "    num_results = 5,\n",
    "    grace_period=20,\n",
    "    mode=\"min\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stehekin gives :True\t0.4\t64\t0.001\t3 Even looking at overall min, and for animas r at durango\n",
    "# T-tests suggests: Bidirectional good, dropout unimportant, 16 bad, 64 vs 128 unimportant. All models that imrpvoed loss wre bidirectional\n",
    "# Libby seemed to want an single layer\n",
    "# San Joaqin is just hard, score of 9.4: {'hidden_size': 64, 'num_layers': 1, 'dropout': 0.4, 'bidirectional': False, 'learning_rate': 1e-05}\n",
    "\n",
    "\n",
    "\n",
    "# At weekly:\n",
    "# Animas has {'hidden_size': 128, 'num_layers': 3, 'dropout': 0.1, 'bidirectional': False, 'learning_rate': 1e-05}, 64,3,0.1. Results for 64, 1, 0.1, True identical\n",
    "def objective(config):   \n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.\n",
    "                      is_available() else 'cpu')\n",
    "    \n",
    "    #print('Device available is', device)\n",
    "    \n",
    "\n",
    "    score = train_model(config) # Have training loop in here that outputs loss of model\n",
    "    return {\"val_loss\": score}\n",
    "\n",
    "basin = 'stehekin_r_at_stehekin'\n",
    "\n",
    "#, search_alg = optuna_search\n",
    "optuna_tune_config = tune.TuneConfig(scheduler=asha_scheduler)\n",
    "tune_config = tune.TuneConfig(scheduler=asha_scheduler)\n",
    "running_tune_config = tune.TuneConfig()\n",
    "\n",
    "run_config=train.RunConfig(stop= plateau_stopper)\n",
    "\n",
    "# Note using < 1gb per run stops pylance from crashing I think\n",
    "# Without Optun\n",
    "tuner = tune.Tuner(tune.with_resources(tune.with_parameters(objective), resources={\"cpu\": 15/11, \"gpu\": 1/11}), param_space=config_space, tune_config = tune_config, run_config = run_config) \n",
    "# With Optuna\n",
    "#tuner = tune.Tuner(tune.with_resources(tune.with_parameters(objective), resources={\"cpu\": 1, \"gpu\": 1/16}), param_space = optuna_config_space, tune_config = optuna_tune_config, run_config = run_config) \n",
    "\n",
    "results = tuner.fit()\n",
    "best_config = results.get_best_result(metric=\"val_loss\", mode=\"min\").config\n",
    "print(best_config)\n",
    "\n",
    "\n",
    "\n",
    "# Define the file path where you want to save the best configuration\n",
    "file_path = f\"/data/Hydra_Work/Tuning/Config_Text/{basin}_best_config.txt\"\n",
    "# Open the file in write mode and save the configuration\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(str(best_config))\n",
    "\n",
    "print(\"Best configuration saved to:\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results.get_dataframe()\n",
    "results_df[results_df['val_loss'] < -0.7][['val_loss', 'config/basin', 'config/test_year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Safe_Basins = list(results_df[results_df['val_loss'] < -0.05]['config/basin'].values)\n",
    "Retrain_Basins = list(set(basins) - set(Safe_Basins))\n",
    "Retrain_Basins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "results_df = results.get_dataframe()\n",
    "columns_to_drop = ['timestamp', 'checkpoint_dir_name', 'done', 'training_iteration', \n",
    "                   'trial_id', 'date', 'time_this_iter_s', 'time_total_s', 'pid', \n",
    "                   'hostname', 'node_ip', 'time_since_restore', 'iterations_since_restore']\n",
    "\n",
    "# Drop the columns\n",
    "results_df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "val_loss_bidirectional_true = results_df[results_df['config/num_layers'] == 3]['val_loss']\n",
    "val_loss_bidirectional_false = results_df[results_df['config/num_layers'] == 1]['val_loss']\n",
    "\n",
    "# Perform a t-test\n",
    "t_statistic, p_value = stats.ttest_ind(val_loss_bidirectional_true, val_loss_bidirectional_false)\n",
    "\n",
    "# Print the results\n",
    "print(\"T-Statistic:\", t_statistic)\n",
    "print(\"P-Value:\", p_value)\n",
    "\n",
    "# Check if the difference in means is statistically significant\n",
    "alpha = 0.05  # Significance level\n",
    "if p_value < alpha:\n",
    "    print(\"The difference in mean val_loss is statistically significant.\")\n",
    "else:\n",
    "    print(\"The difference in mean val_loss is not statistically significant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading models\n",
    "Tuned_Models = {}\n",
    "for basin in basins:\n",
    "    Tuned_Models[basin] = torch.load(f'/data/Hydra_Work/Post_Rodeo_Work/Tuned_Single_Models/basin.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning General Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "static_size = np.shape(static_variables)[1]\n",
    "forecast_size = np.shape(seasonal_forecasts['american_river_folsom_lake_2000_apr'])[1]\n",
    "History_Fourier_in_forcings = 0 #2*3*(6 - 1)\n",
    "Climate_guess = 0 #3\n",
    "History_Statistics_in_forcings = 0 #5*2\n",
    "\n",
    "forecast_input_size = forecast_size + static_size + History_Fourier_in_forcings + History_Statistics_in_forcings  + Climate_guess + 3\n",
    "output_size, head_hidden_size, head_num_layers =  3, 64, 3\n",
    "hindcast_input_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.stopper import TrialPlateauStopper\n",
    "\n",
    "# Fixed parameters\n",
    "total_epochs = 30\n",
    "n_epochs = 1 # Epochs between tests\n",
    "group_lengths = [7] # \n",
    "batch_size = 1\n",
    "copies = 2\n",
    "\n",
    "# parameters to tune\n",
    "# I tuned to 128,2,0.4,False,1e-3 \n",
    "hidden_sizes = [128]\n",
    "num_layers = [1]\n",
    "dropout = [0.1]\n",
    "bidirectional =  [False]\n",
    "learning_rate = [5e-4]\n",
    "\n",
    "config_space = {\n",
    "    \"hidden_size\": tune.grid_search(hidden_sizes),\n",
    "    \"num_layers\": tune.grid_search(num_layers),\n",
    "    \"dropout\": tune.grid_search(dropout),\n",
    "    \"bidirectional\": tune.grid_search(bidirectional),\n",
    "    \"learning_rate\": tune.grid_search(learning_rate),\n",
    "    'test_year': tune.grid_search(list(np.arange(2000,2024,2)) )\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "# Places to save info\n",
    "model_dir = '/data/Hydra_Work/Post_Rodeo_Work/Tuned_General_Model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "    years = list(np.arange(2000,2024,2))\n",
    "    test_year = 2000\n",
    "    val_years = [years[years.index(test_year)-1], years[years.index(test_year)-2]  ]\n",
    "    train_years = [year for year in years if year not in [test_year] + val_years]\n",
    "    \n",
    "    Test_Dates = All_Dates[All_Dates.year == test_year]\n",
    "    Val_Dates = [date for date in All_Dates if date.year in val_years]\n",
    "    Train_Dates = [date for date in All_Dates if date.year in train_years]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2022, 2020]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_general(config):\n",
    "    \n",
    "    All_Dates = ray.get(All_Dates_id)  \n",
    "    \n",
    "    years = list(np.arange(2000,2024,2))\n",
    "    test_year = config['test_year']\n",
    "    val_years = [years[years.index(test_year)-1], years[years.index(test_year)-2]  ]\n",
    "    train_years = [year for year in years if year not in [test_year] + val_years]\n",
    "    \n",
    "    Test_Dates = All_Dates[All_Dates.year == test_year]\n",
    "    Val_Dates = All_Dates[All_Dates.year.isin(val_years)]\n",
    "    Train_Dates = All_Dates[All_Dates.year.isin(train_years)]\n",
    "    \n",
    "    era5 = ray.get(era5_id)  \n",
    "    daily_flow = ray.get(daily_flow_id)  \n",
    "    climatological_flows = ray.get(climatological_flows_id)\n",
    "    climate_indices = ray.get(climate_indices_id)\n",
    "    seasonal_forecasts = ray.get(seasonal_forecasts_id)\n",
    "    Static_variables = ray.get(Static_variables_id)\n",
    "\n",
    "    copies = 1\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.\n",
    "                    is_available() else 'cpu')\n",
    "    \n",
    "    save_path = f'/data/Hydra_Work/No_Forecast_Validation_Models/{test_year}/General_LSTM_No_Flow_Model/General_LSTM.pth'\n",
    "    loss_path = f'/data/Hydra_Work/No_Forecast_Validation_Models/{test_year}/General_LSTM_No_Flow_Model/General_LSTM_loss.txt'\n",
    "\n",
    "    val_loss = 1000\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(loss_path):\n",
    "        # If the file does not exist, create it and write val_loss to it\n",
    "        with open(loss_path, 'w') as file:\n",
    "            file.write('%f' % val_loss)\n",
    "    \n",
    "  \n",
    "    models, params_to_optimize, optimizers, schedulers = define_models(hindcast_input_size, forecast_input_size,\n",
    "    config[\"hidden_size\"], config[\"num_layers\"], config[\"dropout\"],\n",
    "    config[\"bidirectional\"], config[\"learning_rate\"], copies=copies, device = device)\n",
    "\n",
    "    losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(total_epochs):\n",
    "\n",
    "        train_losses = {}\n",
    "        epoch_val_losses = {}\n",
    "\n",
    "        for copy in range(copies):\n",
    "\n",
    "             # Need to fix the outputs of No_Body_Model_Run\n",
    "            train_losses[copy], Climate_Loss = No_Body_Model_Run(Train_Dates, basins, models[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts,\n",
    "                Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper= None, n_epochs=n_epochs,\n",
    "                batch_size=batch_size, group_lengths=group_lengths, Train_Mode=True, device=device, specialised=False)\n",
    "            epoch_val_losses[copy], Climate_Loss = No_Body_Model_Run(Val_Dates, basins, models[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts,\n",
    "                Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper= None, n_epochs=n_epochs,\n",
    "                batch_size=batch_size, group_lengths=group_lengths, Train_Mode=False, device=device, specialised=False)\n",
    "\n",
    "        loss = np.mean(list(train_losses.values())) - Climate_Loss\n",
    "\n",
    "\n",
    "        candidate_val_loss = ((np.mean(list(epoch_val_losses.values())).mean() - Climate_Loss)[0])/np.mean(Climate_Loss)\n",
    "        val_loss = np.min([val_loss, candidate_val_loss ])\n",
    "        \n",
    "        # Check best loss so far for this model\n",
    "        with open(loss_path, 'r') as file:\n",
    "            # Read the entire contents of the file\n",
    "            Overall_Best_Val_Loss = float(file.read())\n",
    "\n",
    "        if val_loss < Overall_Best_Val_Loss:\n",
    "            torch.save(models[0], save_path)\n",
    "\n",
    "            with open(loss_path, 'w') as f:\n",
    "                f.write('%f' % val_loss)\n",
    "\n",
    "            \n",
    "               \n",
    "        ray.train.report({'val_loss' : val_loss})\n",
    "\n",
    "        losses.append(loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "\n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 07:10:07,200\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "from ray import train, tune\n",
    "\n",
    "\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(runtime_env = { \"env_vars\":   {\"PYTHONPATH\": '/data/Hydra_Work/Competition_Functions/' } } )\n",
    "         \n",
    "All_Dates_id = ray.put(All_Dates)  \n",
    "Val_Dates_id = ray.put(Val_Dates)  \n",
    "era5_id = ray.put(era5)  \n",
    "daily_flow_id = ray.put(daily_flow)  \n",
    "climatological_flows_id = ray.put(climatological_flows)\n",
    "climate_indices_id = ray.put(climate_indices)\n",
    "seasonal_forecasts_id = ray.put(seasonal_forecasts)\n",
    "Static_variables_id = ray.put(static_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asha_scheduler = ASHAScheduler(\n",
    "#     time_attr='training_iteration',\n",
    "#     metric='val_loss',\n",
    "#     mode='min',\n",
    "#     max_t=100,\n",
    "#     grace_period=20,\n",
    "#     reduction_factor=2,\n",
    "#     brackets=1,\n",
    "# )\n",
    "\n",
    "\n",
    "plateau_stopper = TrialPlateauStopper(\n",
    "    metric=\"val_loss\",\n",
    "    num_results = 5,\n",
    "    grace_period=30,\n",
    "    mode=\"min\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-05-30 07:14:41</td></tr>\n",
       "<tr><td>Running for: </td><td>00:04:32.82        </td></tr>\n",
       "<tr><td>Memory:      </td><td>76.7/125.9 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 15.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A100D)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc                  </th><th>bidirectional  </th><th style=\"text-align: right;\">  dropout</th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  num_layers</th><th style=\"text-align: right;\">  test_year</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  val_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_a5dbc_00000</td><td>RUNNING </td><td>136.156.133.98:593713</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         189.931</td><td style=\"text-align: right;\">  1.14647 </td></tr>\n",
       "<tr><td>objective_a5dbc_00001</td><td>RUNNING </td><td>136.156.133.98:593715</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         267.164</td><td style=\"text-align: right;\">  0.807647</td></tr>\n",
       "<tr><td>objective_a5dbc_00002</td><td>RUNNING </td><td>136.156.133.98:593717</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         193.322</td><td style=\"text-align: right;\">  0.571646</td></tr>\n",
       "<tr><td>objective_a5dbc_00003</td><td>RUNNING </td><td>136.156.133.98:593716</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         194.116</td><td style=\"text-align: right;\">  0.461234</td></tr>\n",
       "<tr><td>objective_a5dbc_00004</td><td>RUNNING </td><td>136.156.133.98:593726</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         193.657</td><td style=\"text-align: right;\">  0.560798</td></tr>\n",
       "<tr><td>objective_a5dbc_00005</td><td>RUNNING </td><td>136.156.133.98:593727</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         194.488</td><td style=\"text-align: right;\">  0.956412</td></tr>\n",
       "<tr><td>objective_a5dbc_00006</td><td>RUNNING </td><td>136.156.133.98:593728</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         265.944</td><td style=\"text-align: right;\">  0.801351</td></tr>\n",
       "<tr><td>objective_a5dbc_00007</td><td>RUNNING </td><td>136.156.133.98:593731</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         188.171</td><td style=\"text-align: right;\">  0.856294</td></tr>\n",
       "<tr><td>objective_a5dbc_00008</td><td>RUNNING </td><td>136.156.133.98:593738</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         265.266</td><td style=\"text-align: right;\">  0.864256</td></tr>\n",
       "<tr><td>objective_a5dbc_00009</td><td>RUNNING </td><td>136.156.133.98:593739</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         193.969</td><td style=\"text-align: right;\">  1.40002 </td></tr>\n",
       "<tr><td>objective_a5dbc_00010</td><td>RUNNING </td><td>136.156.133.98:593740</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         193.813</td><td style=\"text-align: right;\">  1.07763 </td></tr>\n",
       "<tr><td>objective_a5dbc_00011</td><td>RUNNING </td><td>136.156.133.98:593745</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         189.189</td><td style=\"text-align: right;\">  1.04778 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=593726)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=593726)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 07:14:41,376\tWARNING tune.py:186 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2024-05-30 07:14:51,462\tINFO tune.py:1042 -- Total run time: 282.92 seconds (272.81 seconds for the tuning loop).\n",
      "2024-05-30 07:14:51,463\tWARNING tune.py:1057 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: Tuner.restore(path=\"/home/gbmc/ray_results/objective_2024-05-30_07-10-08\", trainable=...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_size': 128, 'num_layers': 1, 'dropout': 0.1, 'bidirectional': False, 'learning_rate': 0.0005, 'test_year': 2006}\n",
      "Best configuration saved to: /data/Hydra_Work/Tuning/Config_Text/General_Model_best_config.txt\n"
     ]
    }
   ],
   "source": [
    "# {'hidden_size': 256, 'num_layers': 3, 'dropout': 0.1, 'bidirectional': True, 'learning_rate': 0.001}\n",
    "# 7 Days:  128\t2\t0.4\tFalse\t0.001\n",
    "def objective(config):  \n",
    "    device = torch.device('cuda' if torch.cuda.\n",
    "                      is_available() else 'cpu')\n",
    "    \n",
    "    #print('Device available is', device)\n",
    "    \n",
    "\n",
    "    score = train_model_general(config) # Have training loop in here that outputs loss of model\n",
    "    return {\"val_loss\": score}\n",
    "\n",
    "\n",
    "#, search_alg = optuna_search\n",
    "# optuna_tune_config = tune.TuneConfig(scheduler=asha_scheduler)\n",
    "# tune_config = tune.TuneConfig(scheduler=asha_scheduler)\n",
    "run_config=train.RunConfig(stop= plateau_stopper)\n",
    "\n",
    "# Without Optuna\n",
    "tuner = tune.Tuner(tune.with_resources(tune.with_parameters(objective), resources={\"cpu\": 15/12, \"gpu\": 1/12}), param_space=config_space, run_config = run_config) \n",
    "# With Optuna\n",
    "#tuner = tune.Tuner(tune.with_resources(tune.with_parameters(objective), resources={\"cpu\": 1, \"gpu\": 1/16}), param_space = optuna_config_space, tune_config = optuna_tune_config, run_config = run_config) \n",
    "\n",
    "results = tuner.fit()\n",
    "# try get_best_checkpoint, or change val to be maximum of current val_loss and previous ones\n",
    "best_config = results.get_best_result(metric=\"val_loss\", mode=\"min\").config\n",
    "print(best_config)\n",
    "file_path = f\"/data/Hydra_Work/Tuning/Config_Text/General_Model_best_config.txt\"\n",
    "\n",
    "# Open the file in write mode and save the configuration\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(str(best_config))\n",
    "\n",
    "print(\"Best configuration saved to:\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results.get_dataframe()\n",
    "results_df[results_df['val_loss'] < -0.15] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "General_Model = torch.load('/data/Hydra_Work/Post_Rodeo_Work/Tuned_General_Model/General_model.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Hydra Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_models_hydra(body_hindcast_input_size, body_forecast_input_size, body_output_size, body_hidden_size, body_num_layers, body_dropout,\n",
    "                        head_hidden_size, head_num_layers, head_forecast_output_size, head_dropout, bidirectional, basins,\n",
    "                        learning_rate_general_head, learning_rate_head, learning_rate_body, LR = 1e-3, \n",
    "                        additional_specific_head_hindcast_input_size = 1, additional_specific_head_forecast_input_size = 0,\n",
    "                        copies=1, device=None):\n",
    "    Hydra_Bodys = {}\n",
    "    Basin_Heads = {}\n",
    "    General_Heads = {}   \n",
    "    general_optimizers = {}\n",
    "    optimizers = {}\n",
    "    schedulers = {}\n",
    "    \n",
    "    body_forecast_output_size = body_output_size\n",
    "    body_hindcast_output_size = body_output_size\n",
    "    \n",
    "    # Define head hindcast size as head-forecast for simplicty\n",
    "    head_hindcast_output_size = head_forecast_output_size\n",
    "    specific_head_hindcast_output_size = head_forecast_output_size\n",
    "    specific_head_forecast_output_size = head_forecast_output_size\n",
    "    specific_head_hidden_size = head_hidden_size\n",
    "    specific_head_num_layers = head_num_layers\n",
    "    \n",
    "    # Head takes Body as inputs\n",
    "    #head_hindcast_input_size = body_hindcast_input_size \n",
    "    head_hindcast_input_size = body_hindcast_output_size\n",
    "    head_forecast_input_size = body_forecast_output_size\n",
    "    \n",
    "    # Specific input size\n",
    "    specific_head_hindcast_input_size = head_hindcast_input_size + additional_specific_head_hindcast_input_size\n",
    "    specific_head_forecast_input_size = head_forecast_input_size + additional_specific_head_forecast_input_size\n",
    "    \n",
    "    for copy in range(copies):\n",
    "        Hydra_Bodys[copy] = Google_Model_Block(body_hindcast_input_size, body_forecast_input_size, body_hindcast_output_size, body_forecast_output_size, body_hidden_size, body_num_layers, device, body_dropout, bidirectional)\n",
    "        General_Heads[copy] = Google_Model_Block(head_hindcast_input_size, head_forecast_input_size, head_hindcast_output_size, head_forecast_output_size, head_hidden_size, head_num_layers, device, head_dropout, bidirectional)\n",
    "        Basin_Heads[copy] = Specific_Heads(basins, specific_head_hindcast_input_size, specific_head_forecast_input_size, specific_head_hindcast_output_size, specific_head_forecast_output_size, specific_head_hidden_size, specific_head_num_layers, device, head_dropout, bidirectional)\n",
    "\n",
    "\n",
    "        specific_head_parameters = list()\n",
    "        for basin, model in Basin_Heads[copy].items():\n",
    "            specific_head_parameters += list(model.parameters())\n",
    "\n",
    "        optimizers[copy] = torch.optim.Adam(\n",
    "        # Extra LR is the global learning rate, not really important\n",
    "        [\n",
    "            {\"params\": General_Heads[copy].parameters(), \"lr\": learning_rate_general_head},\n",
    "            {\"params\": specific_head_parameters, \"lr\": learning_rate_head},\n",
    "            {\"params\": Hydra_Bodys[copy].parameters(), \"lr\": learning_rate_body},\n",
    "        ],\n",
    "        lr=LR, weight_decay = 1e-4 ) #1e-4 good so far, 3 not so food\n",
    "\n",
    "        general_optimizers[copy] = torch.optim.SGD(\n",
    "        # Extra LR is the global learning rate, not really important\n",
    "        [\n",
    "            {\"params\": General_Heads[copy].parameters(), \"lr\": learning_rate_general_head},\n",
    "            {\"params\": Hydra_Bodys[copy].parameters(), \"lr\": learning_rate_body},\n",
    "        ],\n",
    "        lr=LR, )\n",
    "        schedulers[copy] = lr_scheduler.StepLR(optimizers[copy], 1, gamma=0.99) #.CosineAnnealingLR(optimizers[copy], T_max= 10, eta_min= 1e-4,)\n",
    "        #.StepLR(optimizers[copy], 1, gamma=0.99) #\n",
    "    return Hydra_Bodys, General_Heads, Basin_Heads, optimizers, schedulers, general_optimizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "static_size = np.shape(static_variables)[1]\n",
    "forecast_size = np.shape(seasonal_forecasts['american_river_folsom_lake_2000_apr'])[1]\n",
    "History_Fourier_in_forcings = 0 #2*3*(6 - 1)\n",
    "Climate_guess = 0 #3\n",
    "History_Statistics_in_forcings = 0 # 5*2\n",
    "\n",
    "forecast_input_size = forecast_size + static_size + History_Fourier_in_forcings + History_Statistics_in_forcings  + Climate_guess + 3\n",
    "output_size, head_hidden_size, head_num_layers =  3, 64, 3\n",
    "body_hindcast_input_size = 8\n",
    "body_forecast_input_size = forecast_input_size\n",
    "\n",
    "\n",
    "Overall_Best_Val_Loss = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.stopper import TrialPlateauStopper\n",
    "\n",
    "# Fixed parameters\n",
    "total_epochs = 300\n",
    "n_epochs = 1 # Epochs between tests\n",
    "group_lengths = [7] #np.arange(180)\n",
    "batch_size = 1\n",
    "copies = 1\n",
    "head_output_size = 3\n",
    "\n",
    "# parameters to tune\n",
    "# chose 128, 2, 0.1, 1e-3, 6, 32, 1, 0.4, 1e-3\n",
    "body_hidden_sizes =  [128]\n",
    "body_num_layers = [1]\n",
    "body_dropouts = [0.0] #[0.1, 0.4]\n",
    "body_learning_rates = [1e-3]\n",
    "body_outputs = [4] # Say hindcast and forecasts have same outputrs body_hindcast_output_size\n",
    "\n",
    "\n",
    "head_hidden_sizes = [32]\n",
    "head_num_layers = [1]\n",
    "head_dropouts = [0.0] #[0.1, 0.4, 0.7]\n",
    "head_learning_rates = [1e-3, 5e-3]\n",
    "LR = 1e-3\n",
    "bidirectionals = [False]\n",
    "spec_multiplier = [1, 10]\n",
    "\n",
    "config_space = {\n",
    "    \"body_hidden_size\": tune.grid_search(body_hidden_sizes),\n",
    "    \"body_num_layer\": tune.grid_search(body_num_layers),\n",
    "    \"body_dropout\": tune.grid_search(body_dropouts),\n",
    "    \"bidirectional\": tune.grid_search(bidirectionals),\n",
    "    \"body_output\": tune.grid_search(body_outputs),\n",
    "    \"body_learning_rate\": tune.grid_search(body_learning_rates),\n",
    "    \"head_hidden_size\": tune.grid_search(head_hidden_sizes),\n",
    "    \"head_num_layer\": tune.grid_search(head_num_layers),\n",
    "    \"head_dropout\": tune.grid_search(head_dropouts),\n",
    "    \"head_learning_rate\": tune.grid_search(head_learning_rates),\n",
    "    \"spec_multiplier\": tune.grid_search(spec_multiplier)\n",
    "    #'test_year': tune.grid_search(list(np.arange(2000,2024,2)) )\n",
    "\n",
    "    #\"general_head_learning_rate\": tune.grid_search(head_learning_rates),\n",
    "}\n",
    "\n",
    "# Places to save info\n",
    "model_dir = '/data/Hydra_Work/Post_Rodeo_Work/Tuned_Hydra_Model/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_hydra(config):\n",
    "\n",
    "    All_Dates = ray.get(All_Dates_id)  \n",
    "    \n",
    "    years = list(np.arange(2000,2024,2))\n",
    "    test_year = 2022 #config['test_year']\n",
    "    val_years = [years[years.index(test_year)-1], years[years.index(test_year)-2]  ]\n",
    "    train_years = [year for year in years if year not in [test_year] + val_years]\n",
    "    \n",
    "    Test_Dates = All_Dates[All_Dates.year == test_year]\n",
    "    Val_Dates = All_Dates[All_Dates.year.isin(val_years)]\n",
    "    Train_Dates = All_Dates[All_Dates.year.isin(train_years)]\n",
    "\n",
    "    \n",
    "    era5 = ray.get(era5_id)  \n",
    "    daily_flow = ray.get(daily_flow_id)  \n",
    "    climatological_flows = ray.get(climatological_flows_id)\n",
    "    climate_indices = ray.get(climate_indices_id)\n",
    "    seasonal_forecasts = ray.get(seasonal_forecasts_id)\n",
    "    Static_variables = ray.get(Static_variables_id)  \n",
    "  \n",
    "\n",
    "    body_save_path = f'/data/Hydra_Work/No_Forecast_Validation_Models/{test_year}/General_Head_Model/Hydra_Body_LSTM.pth'\n",
    "    head_save_path = f'/data/Hydra_Work/No_Forecast_Validation_Models/{test_year}/General_Head_Model/Hydra_Head_LSTM.pth'\n",
    "    basin_heads_save_path = f'/data/Hydra_Work/No_Forecast_Validation_Models/{test_year}/Basin_Head_Model'\n",
    "    \n",
    "    loss_path = f'/data/Hydra_Work/No_Forecast_Validation_Models/{test_year}/General_Head_Model/Hydra_LSTM_loss.txt'\n",
    "\n",
    "    val_loss = 1000\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(loss_path):\n",
    "        # If the file does not exist, create it and write val_loss to it\n",
    "        with open(loss_path, 'w') as file:\n",
    "            file.write('%f' % val_loss)\n",
    "\n",
    "\n",
    "    copies = 1\n",
    "    warmup = 6\n",
    "    best_val_loss = 999\n",
    "    device = torch.device('cuda' if torch.cuda.\n",
    "                    is_available() else 'cpu')\n",
    "   \n",
    "\n",
    "    general_head_learning_rate = config['body_learning_rate']\n",
    "    Hydra_Bodys, General_Hydra_Heads, model_heads, optimizers, schedulers, general_optimizers  = define_models_hydra(body_hindcast_input_size, body_forecast_input_size, config['body_output'],\n",
    "                                config['body_hidden_size'], config['body_num_layer'], config['body_dropout'], \n",
    "                                config['head_hidden_size'], config['head_num_layer'], 3, config['head_dropout'], config['bidirectional'], basins,\n",
    "                                general_head_learning_rate, config['head_learning_rate'], config['body_learning_rate'], LR, device = device\n",
    "                                )\n",
    "     \n",
    "    # Replace with already existing models \n",
    "    # General_Hydra_Heads = [torch.load(head_save_path)]\n",
    "    # Hydra_Bodys = [torch.load(body_save_path)]\n",
    "    # Specific_Heads = {}\n",
    "    # for basin in basins:\n",
    "    #     Specific_Heads[f'{basin}'] = torch.load(f\"{basin_heads_save_path}/{basin}.path\")\n",
    "    # models_heads = [Specific_Heads]\n",
    "    \n",
    "    # optimizers = {}    \n",
    "    # specific_head_parameters = list()\n",
    "    # for basin, model in model_heads[0].items():\n",
    "    #     specific_head_parameters += list(model.parameters())\n",
    "\n",
    "    # optimizers[0] = torch.optim.Adam(\n",
    "    # # Extra LR is the global learning rate, not really important\n",
    "    # [\n",
    "    #     {\"params\": General_Hydra_Heads[0].parameters(), \"lr\": config['body_learning_rate']},\n",
    "    #     {\"params\": specific_head_parameters, \"lr\": config['body_learning_rate']},\n",
    "    #     {\"params\": Hydra_Bodys[0].parameters(), \"lr\": config['body_learning_rate']},\n",
    "    # ],\n",
    "    # lr=LR, weight_decay = 1e-3 )\n",
    "    # schedulers[0] = lr_scheduler.CosineAnnealingLR(optimizers[0], T_max= 100)\n",
    "\n",
    "\n",
    "    \n",
    "                                                \n",
    "    general_losses, specific_losses, general_val_losses, specific_val_losses, val_losses = [], [], [], [], []\n",
    "\n",
    "    # Initialise, with dummy scheduler\n",
    "    for copy in range(copies):\n",
    "        # Initialise\n",
    "        dummy_scheduler = lr_scheduler.StepLR(optimizers[copy],step_size = warmup, gamma = 0.8)\n",
    "\n",
    "        Model_Run(All_Dates, basins, Hydra_Bodys[copy], General_Hydra_Heads[copy], model_heads[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts,\n",
    "            Static_variables, general_optimizers[copy], dummy_scheduler, criterion, early_stopper= None, n_epochs= warmup,\n",
    "            batch_size=batch_size, group_lengths=group_lengths, Train_Mode=True, device=device, feed_forcing = False)\n",
    "\n",
    "            \n",
    "    for epoch in range(total_epochs):\n",
    "        train_general_losses = {}\n",
    "        train_specific_losses = {}\n",
    "        epoch_val_general_losses = {}\n",
    "        epoch_val_specific_losses = {}\n",
    "        climate_losses = {}\n",
    "        \n",
    "        for copy in range(copies):\n",
    "                        \n",
    "\n",
    "            # Full Training\n",
    "            train_general_losses[copy], train_specific_losses[copy], climate_losses[copy] = Model_Run(Train_Dates, basins, Hydra_Bodys[copy], General_Hydra_Heads[copy], model_heads[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts,\n",
    "                Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper= None, n_epochs= n_epochs,\n",
    "                batch_size=batch_size, group_lengths=group_lengths, Train_Mode=True, device=device, feed_forcing = False, spec_multiplier = config[\"spec_multiplier\"])\n",
    "            epoch_val_general_losses[copy], epoch_val_specific_losses[copy], climate_losses[copy] = Model_Run(Val_Dates, basins, Hydra_Bodys[copy], General_Hydra_Heads[copy], model_heads[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts,\n",
    "                Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper= None, n_epochs= n_epochs,\n",
    "                batch_size=batch_size, group_lengths=group_lengths, Train_Mode=False, device=device, feed_forcing = False)\n",
    "\n",
    "        general_loss = np.mean(list(train_general_losses.values()))\n",
    "        specific_loss = np.mean(list(train_specific_losses.values()))\n",
    "        climate_loss = np.mean(list(climate_losses.values()))\n",
    "        \n",
    "        epoch_val_general_loss = np.mean(list(epoch_val_general_losses.values())).mean()\n",
    "        epoch_val_specific_loss = np.mean(list(epoch_val_specific_losses.values())).mean()\n",
    "        \n",
    "        \n",
    "        general_losses.append(general_loss)\n",
    "        specific_losses.append(specific_loss)\n",
    "        specific_val_losses.append(epoch_val_specific_loss)\n",
    "        general_val_losses.append(epoch_val_general_loss)\n",
    "\n",
    "        val_loss = 0.5*(epoch_val_general_loss + epoch_val_specific_loss)\n",
    "        \n",
    "        candidate_val_loss = ((val_loss.mean() - climate_loss))/np.mean(climate_loss)\n",
    "        best_val_loss = np.min([best_val_loss, candidate_val_loss ])\n",
    "         \n",
    "        with open(loss_path, 'r') as file:\n",
    "            # Read the entire contents of the file\n",
    "            Overall_Best_Val_Loss = float(file.read())\n",
    "\n",
    "        if best_val_loss < Overall_Best_Val_Loss:\n",
    "            with open(loss_path, 'w') as f:\n",
    "                f.write('%f' % best_val_loss)\n",
    "\n",
    "            torch.save(Hydra_Bodys[0], body_save_path)\n",
    "            torch.save(General_Hydra_Heads[0], head_save_path)\n",
    "            for basin in basins:\n",
    "                torch.save(model_heads[0][basin], f\"{basin_heads_save_path}/{basin}.path\")\n",
    "                \n",
    "            \n",
    "               \n",
    "        ray.train.report({'val_loss' : best_val_loss})\n",
    "        #print('Validation Loss', candidate_val_loss)\n",
    "        print('Training Loss', 1 - (specific_loss / general_loss) )\n",
    "        val_losses.append(best_val_loss)\n",
    "\n",
    "\n",
    "    return best_val_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 15:44:20,375\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "from ray import train, tune\n",
    "\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(runtime_env = { \"env_vars\":   {\"PYTHONPATH\": '/data/Hydra_Work/Competition_Functions/' } } )\n",
    "         \n",
    "All_Dates_id = ray.put(All_Dates)  \n",
    "Val_Dates_id = ray.put(Val_Dates)  \n",
    "era5_id = ray.put(era5)  \n",
    "daily_flow_id = ray.put(daily_flow)  \n",
    "climatological_flows_id = ray.put(climatological_flows)\n",
    "climate_indices_id = ray.put(climate_indices)\n",
    "seasonal_forecasts_id = ray.put(seasonal_forecasts)\n",
    "Static_variables_id = ray.put(static_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "asha_scheduler = ASHAScheduler(\n",
    "    time_attr='training_iteration',\n",
    "    metric='val_loss',\n",
    "    mode='min',\n",
    "    max_t=100,\n",
    "    grace_period=20,\n",
    "    reduction_factor=3,\n",
    "    brackets=1,\n",
    ")\n",
    "\n",
    "\n",
    "plateau_stopper = TrialPlateauStopper(\n",
    "    metric=\"val_loss\",\n",
    "    num_results = 20,\n",
    "    grace_period=20,\n",
    "    mode=\"min\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-05-30 16:24:38</td></tr>\n",
       "<tr><td>Running for: </td><td>00:40:12.68        </td></tr>\n",
       "<tr><td>Memory:      </td><td>48.4/125.9 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 15.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A100D)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc                  </th><th>bidirectional  </th><th style=\"text-align: right;\">  body_dropout</th><th style=\"text-align: right;\">  body_hidden_size</th><th style=\"text-align: right;\">  body_learning_rate</th><th style=\"text-align: right;\">  body_num_layer</th><th style=\"text-align: right;\">  body_output</th><th style=\"text-align: right;\">  head_dropout</th><th style=\"text-align: right;\">  head_hidden_size</th><th style=\"text-align: right;\">  head_learning_rate</th><th style=\"text-align: right;\">  head_num_layer</th><th style=\"text-align: right;\">  spec_multiplier</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  val_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_7e743_00000</td><td>RUNNING </td><td>136.156.133.98:624661</td><td>False          </td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">               128</td><td style=\"text-align: right;\">               0.001</td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">            4</td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">                32</td><td style=\"text-align: right;\">               0.001</td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         2390.52</td><td style=\"text-align: right;\"> 0.162531 </td></tr>\n",
       "<tr><td>objective_7e743_00001</td><td>RUNNING </td><td>136.156.133.98:624662</td><td>False          </td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">               128</td><td style=\"text-align: right;\">               0.001</td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">            4</td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">                32</td><td style=\"text-align: right;\">               0.005</td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         2402.31</td><td style=\"text-align: right;\">-0.01856  </td></tr>\n",
       "<tr><td>objective_7e743_00002</td><td>RUNNING </td><td>136.156.133.98:624663</td><td>False          </td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">               128</td><td style=\"text-align: right;\">               0.001</td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">            4</td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">                32</td><td style=\"text-align: right;\">               0.001</td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">               10</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         2379.18</td><td style=\"text-align: right;\"> 0.249431 </td></tr>\n",
       "<tr><td>objective_7e743_00003</td><td>RUNNING </td><td>136.156.133.98:624664</td><td>False          </td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">               128</td><td style=\"text-align: right;\">               0.001</td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">            4</td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">                32</td><td style=\"text-align: right;\">               0.005</td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">               10</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         2393.87</td><td style=\"text-align: right;\">-0.0375056</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=624661)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "\u001b[36m(objective pid=624661)\u001b[0m   warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 6.659820812810651\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 7.21976998369408\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 7.076786775571157\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 5.945127211624957\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 6.20652062348182\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 6.654894722823616\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 5.725508874508699\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 5.801343070092966\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 6.172613645044768\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 5.124006579852385\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 5.7735241522015395\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 6.034910353437721\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 4.348406151512117\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 5.241627866284889\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 5.895491665631134\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 4.068094986796209\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 4.878960224633229\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 4.042559135472847\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 4.9418048844201445\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 3.593041525571533\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 4.681358258156337\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 3.527266655446882\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 4.708935330773068\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 3.4035121689602152\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 4.718842023503412\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 3.43694109935847\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 5.584802380669921\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 3.1371365114062417\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 4.593565647837123\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 3.32905682492444\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 4.958621631601124\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 3.349057534853036\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 4.9994630840583865\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 3.1470606142072866\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 4.650136216384146\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 4.282157292833788\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 3.2231042306967446\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 4.325207397550401\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 4.219738624724402\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 3.227364850159969\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 4.665500653859045\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 4.253782183370588\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 2.960351435698801\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 4.054292909452427\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 3.7905939085400746\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 2.744429373519672\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 4.505962738588234\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 4.126105440888414\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 2.850951494299288\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 3.949434028235326\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 4.056484136950962\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 2.6920741693366304\n",
      "\u001b[36m(objective pid=624662)\u001b[0m Training Loss 3.0120048468504494\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 3.7648501416050575\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 4.061738582035108\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 2.8434443598853827\n",
      "\u001b[36m(objective pid=624662)\u001b[0m Training Loss 2.8515954380718616\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 3.867322975251513\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 4.16461236587827\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 2.7805261401652834\n",
      "\u001b[36m(objective pid=624662)\u001b[0m Training Loss 2.8516880196752177\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 3.6019766152137445\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 3.938020059436958\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 3.0738458541986065\n",
      "\u001b[36m(objective pid=624662)\u001b[0m Training Loss 2.841782473850542\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 3.7949933351380936\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 4.0257996549292745\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 2.677627847382335\n",
      "\u001b[36m(objective pid=624662)\u001b[0m Training Loss 2.6499898752895965\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 3.7660815804428465\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 3.7353118950113378\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 2.8061707739420068\n",
      "\u001b[36m(objective pid=624662)\u001b[0m Training Loss 2.6823707921803948\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 3.8214103082499635\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 4.032912583472735\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 2.786037930688761\n",
      "\u001b[36m(objective pid=624662)\u001b[0m Training Loss 2.5762109634822754\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 3.7373206579285885\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 4.145351531127507\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 2.8230551860308872\n",
      "\u001b[36m(objective pid=624662)\u001b[0m Training Loss 2.6932083700305145\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 3.471674966480628\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 3.8696852857940596\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 2.939704831025636\n",
      "\u001b[36m(objective pid=624662)\u001b[0m Training Loss 2.9438096364754958\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 3.5743302514220523\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 3.467685355588481\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 2.558210747770581\n",
      "\u001b[36m(objective pid=624662)\u001b[0m Training Loss 2.601426436885347\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 3.9896618369741472\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 3.4671387255433674\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 2.5818138752893836\n",
      "\u001b[36m(objective pid=624662)\u001b[0m Training Loss 2.7109006517723837\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 3.6536979427043037\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 3.7279509342898614\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 2.4705688513692095\n",
      "\u001b[36m(objective pid=624662)\u001b[0m Training Loss 2.6595495704046903\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 3.5778643477691503\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 3.482015609442429\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 2.553902199018787\n",
      "\u001b[36m(objective pid=624662)\u001b[0m Training Loss 2.6338181526422084\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 3.249602076417887\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 3.3059814059804533\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 2.4564446677885985\n",
      "\u001b[36m(objective pid=624662)\u001b[0m Training Loss 2.3950690452506223\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 3.442410560304632\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 3.6951291592759663\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 2.5256592348089915\n",
      "\u001b[36m(objective pid=624662)\u001b[0m Training Loss 2.7649556779979068\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 3.4452815336488865\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 3.3983779582251303\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 2.383917319293731\n",
      "\u001b[36m(objective pid=624662)\u001b[0m Training Loss 2.8021439474780303\n",
      "\u001b[36m(objective pid=624663)\u001b[0m Training Loss 3.460627350121509\n",
      "\u001b[36m(objective pid=624661)\u001b[0m Training Loss 3.478764211164964\n",
      "\u001b[36m(objective pid=624664)\u001b[0m Training Loss 2.4930925138690703\n",
      "\u001b[36m(objective pid=624662)\u001b[0m Training Loss 2.643075128024851\n"
     ]
    }
   ],
   "source": [
    "runs_per_iteration = 4\n",
    "def objective(config):  \n",
    "    device = torch.device('cuda' if torch.cuda.\n",
    "                      is_available() else 'cpu')\n",
    "    \n",
    "\n",
    "    score = train_model_hydra(config) # Have training loop in here that outputs loss of model\n",
    "    return {\"val_loss\": score}\n",
    "\n",
    "\n",
    "run_config=train.RunConfig(stop= plateau_stopper)\n",
    "# Can use fractions of GPU\n",
    "tuner = tune.Tuner(tune.with_resources(tune.with_parameters(objective), resources={\"cpu\": 15/runs_per_iteration, \"gpu\": 1/(runs_per_iteration)}), param_space=config_space, run_config = run_config) \n",
    "\n",
    "results = tuner.fit()\n",
    "best_config = results.get_best_result(metric=\"val_loss\", mode=\"min\").config\n",
    "print(best_config)\n",
    "file_path = f\"/data/Hydra_Work/Tuning/Config_Text/Hydral_Model_best_config.txt\"\n",
    "\n",
    "# Open the file in write mode and save the configuration\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(str(best_config))\n",
    "\n",
    "print(\"Best configuration saved to:\", file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results.get_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[results_df['val_loss'] < -0.1]#[['val_loss', 'config/body_output']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hydra_Code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
