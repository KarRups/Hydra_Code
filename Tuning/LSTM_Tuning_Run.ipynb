{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('tableau-colorblind10')\n",
    "\n",
    "import sys\n",
    "sys.path.append('/data/Hydra_Work/Competition_Functions') \n",
    "from Processing_Functions import process_forecast_date, process_seasonal_forecasts\n",
    "from Data_Transforming import read_nested_csvs, generate_daily_flow, use_USGS_flow_data, USGS_to_daily_df_yearly\n",
    "\n",
    "sys.path.append('/data/Hydra_Work/Pipeline_Functions')\n",
    "from Folder_Work import filter_rows_by_year, csv_dictionary, add_day_of_year_column\n",
    "\n",
    "sys.path.append('/data/Hydra_Work/Post_Rodeo_Work/ML_Functions.py')\n",
    "from Full_LSTM_ML_Functions import Specific_Heads, Google_Model_Block, SumPinballLoss, EarlyStopper, Model_Run, No_Body_Model_Run\n",
    "\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the cross validation set\n",
    "\n",
    "Cross Validation decisions:\n",
    "- It looks like I only have 10 years right now, and if the results are good I can keep it that way (justify by independent years)\n",
    "- Training set of 80% and Validation of 20% is fine, makes sense to make the Validation years adjacent instead of random, probably doesn't matter much but adjacent minimises theyre connection with the years in the training dataset\n",
    "- This means theres only 5 folds which shouldn't take forever to do \n",
    "- There's an issue right now where my validation set is also my test set, how much can I get around this?\n",
    "- I could test a 70-20-10 set up, from the looks of it there won't be that much loss in performance by reducing the training set by 12%? \n",
    "- If I assume the years are independent then it doesn't matter which dates I choose for validation years when I've got a specific testing year\n",
    "- K -fold cross validation means splitting the data in k chunks and choosing a different chunk for each, p-fold involves choosing all possible combinations of size p for the splits\n",
    "\n",
    "Structure of the folders:\n",
    "- Can do Validation_Models/Val_Years/Model/.pth, bs Model/Val_Years/.pth\n",
    "- I think the first makes more sense, I would realy want to ompare models trained over the same years\n",
    "\n",
    "\n",
    "Restructuring Current code:\n",
    "- I want to fit this whole thing into a for loop so I can run it\n",
    "- Alternatively I can have the validation years as a parameter in the config_space and just let the code run as is\n",
    "- It would be nice to make the prep section smaller visually, or hidden somewhere else\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hydra_Code\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "def get_env():\n",
    "    sp = sys.path[1].split(\"/\")\n",
    "    if \"envs\" in sp:\n",
    "        return sp[sp.index(\"envs\") + 1]\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "print(get_env())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "basins = ['libby_reservoir_inflow',  'owyhee_r_bl_owyhee_dam',  'san_joaquin_river_millerton_reservoir',  'taylor_park_reservoir_inflow',\n",
    " 'boise_r_nr_boise', 'green_r_bl_howard_a_hanson_dam', 'weber_r_nr_oakley', 'detroit_lake_inflow', 'virgin_r_at_virtin', 'dillon_reservoir_inflow',\n",
    " 'pueblo_reservoir_inflow', 'hungry_horse_reservoir_inflow', 'stehekin_r_at_stehekin', 'pecos_r_nr_pecos', 'snake_r_nr_heise', 'yampa_r_nr_maybell',\n",
    " 'colville_r_at_kettle_falls', 'missouri_r_at_toston', 'merced_river_yosemite_at_pohono_bridge', 'animas_r_at_durango','fontenelle_reservoir_inflow', 'boysen_reservoir_inflow']\n",
    "\n",
    "selected_years = range(2000,2024,2)\n",
    "\n",
    "\n",
    "base_dir = \"/data/Hydra_Work/Scaled_Data\"\n",
    "\n",
    "# Define dictionaries and DataFrames\n",
    "dictionaries = ['era5', 'seasonal_forecasts', 'daily_flow', 'climatological_flows']\n",
    "\n",
    "dataframes = ['climate_indices', 'static_variables']\n",
    "\n",
    "# Function to load dictionaries\n",
    "def load_dictionaries(base_dir, names):\n",
    "    loaded_dicts = {}\n",
    "    for name in names:\n",
    "        file_path = os.path.join(base_dir, f\"{name}.pkl\")\n",
    "        with open(file_path, 'rb') as file:\n",
    "            locals()[name] = pickle.load(file)\n",
    "    return locals()\n",
    "\n",
    "# Function to load DataFrames\n",
    "def load_dataframes(base_dir, names):\n",
    "    loaded_dfs = {}\n",
    "    for name in names:\n",
    "        file_path = os.path.join(base_dir, f\"{name}.pkl\")\n",
    "        locals()[name] = pd.read_pickle(file_path)\n",
    "    return locals()\n",
    "\n",
    "saved_dicts = load_dictionaries(base_dir, dictionaries)\n",
    "saved_dfs = load_dataframes(base_dir, dataframes)\n",
    "\n",
    "for name in dictionaries:\n",
    "    locals()[name] = saved_dicts[name]\n",
    "\n",
    "for name in dataframes:\n",
    "    locals()[name] = saved_dfs[name]\n",
    "\n",
    "criterion = SumPinballLoss(quantiles = [0.1, 0.5, 0.9])\n",
    "\n",
    "basin = 'animas_r_at_durango' \n",
    "All_Dates = daily_flow[basin].index[\n",
    "    ((daily_flow[basin].index.month < 6) | ((daily_flow[basin].index.month == 6) & (daily_flow[basin].index.day < 24))) &\n",
    "    ((daily_flow[basin].index.year % 2 == 0) | ((daily_flow[basin].index.month > 10) | ((daily_flow[basin].index.month == 10) & (daily_flow[basin].index.day >= 1))))\n",
    "]\n",
    "All_Dates = All_Dates[All_Dates.year > 1998]\n",
    "\n",
    "\n",
    "# Validation Year\n",
    "# Val_Dates = All_Dates[All_Dates.year >= 2020]\n",
    "# All_Dates = All_Dates[All_Dates.year < 2020]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.\n",
    "                is_available() else 'cpu')\n",
    "\n",
    "criterion = SumPinballLoss(quantiles = [0.1, 0.5, 0.9])\n",
    "\n",
    "basin = 'animas_r_at_durango' \n",
    "All_Dates = daily_flow[basin].index[\n",
    "    ((daily_flow[basin].index.month < 6) | ((daily_flow[basin].index.month == 6) & (daily_flow[basin].index.day < 24))) &\n",
    "    ((daily_flow[basin].index.year % 2 == 0) | ((daily_flow[basin].index.month > 10) | ((daily_flow[basin].index.month == 10) & (daily_flow[basin].index.day >= 1))))\n",
    "]\n",
    "All_Dates = All_Dates[All_Dates.year > 1998]\n",
    "\n",
    "\n",
    "# Validation Year\n",
    "# Val_Dates = All_Dates[All_Dates.year >= 2018]\n",
    "# Val_Dates = Val_Dates[Val_Dates.year <= 2022]\n",
    "# Train_Dates = All_Dates[All_Dates.year == 2022]\n",
    "\n",
    "seed = 42 ; torch.manual_seed(seed) ; random.seed(seed) ; np.random.seed(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "days  = 90\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning individual basins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "static_size = np.shape(static_variables)[1]\n",
    "forecast_size = np.shape(seasonal_forecasts['american_river_folsom_lake_2000_apr'])[1]\n",
    "History_Fourier_in_forcings = 0 #2*3*(6 - 1)\n",
    "Climate_guess = 0 #3 # THis is about climatology, not climate indices\n",
    "History_Statistics_in_forcings = 0  #5*2\n",
    "\n",
    "forecast_input_size = forecast_size + static_size + History_Fourier_in_forcings + History_Statistics_in_forcings  + Climate_guess + 3\n",
    "output_size, head_hidden_size, head_num_layers =  3, 64, 3\n",
    "hindcast_input_size = 9 # 17 if we include climate indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Retrain_Basins = basins\n",
    "for basin in basins:\n",
    "    loss_path = f'/data/Hydra_Work/Tuning/Week_Ahead_Models_V2/Specific_Week_Ahead_Models/{basin}_specific_loss.txt'\n",
    "    \n",
    "    with open(loss_path, 'r') as file:\n",
    "    # Read the entire contents of the file\n",
    "        Overall_Best_Val_Loss = float(file.read())\n",
    "    \n",
    "    if Overall_Best_Val_Loss < -0.05:\n",
    "        Retrain_Basins = list(set(Retrain_Basins) - set([basin]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we want hindcast and forecast num-layers to be different?\n",
    "def define_models(hindcast_input_size, forecast_input_size, hidden_size, num_layers, dropout, bidirectional, learning_rate, copies = 3, forecast_output_size = 3, device = device):\n",
    "    models = {}\n",
    "    params_to_optimize = {}\n",
    "    optimizers = {}\n",
    "    schedulers = {}\n",
    "    \n",
    "    hindcast_output_size = forecast_output_size\n",
    "    for copy in range(copies):\n",
    "        models[copy] = Google_Model_Block(hindcast_input_size, forecast_input_size, hindcast_output_size, forecast_output_size, hidden_size, num_layers, device, dropout, bidirectional)\n",
    "        \n",
    "        models[copy].to(device)\n",
    "        params_to_optimize[copy] = list(models[copy].parameters())\n",
    "        # Probably should be doing 1e-2 and 10\n",
    "        optimizers[copy] = torch.optim.Adam(params_to_optimize[copy], lr= learning_rate, weight_decay = 1e-4)\n",
    "        schedulers[copy] = lr_scheduler.CosineAnnealingLR(optimizers[copy], T_max = 100000,)\n",
    "        #.StepLR(optimizers[copy], 5, gamma=0.5)\n",
    "        \n",
    "\n",
    "    return models, params_to_optimize, optimizers, schedulers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.stopper import TrialPlateauStopper\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "import optuna\n",
    "\n",
    "# Fixed parameters\n",
    "total_epochs = 200\n",
    "n_epochs = 1  # Epochs between tests\n",
    "group_lengths = [7] #np.arange(180) 7 Day ahead for streamlined version\n",
    "batch_size = 256\n",
    "copies = 1\n",
    "\n",
    "# parameters to tune\n",
    "hidden_sizes = [128] # 64 converged upon\n",
    "num_layers =  [1]\n",
    "dropout = [0.1]\n",
    "bidirectional = [False] #[True, False]\n",
    "learning_rate = [1e-1, 1e-2] #[1e-3, 1e-5]\n",
    "\n",
    "\n",
    "# Set up configuration space\n",
    "config_space = {\n",
    "\n",
    "    \"hidden_size\": tune.grid_search(hidden_sizes),\n",
    "    \"num_layers\": tune.grid_search(num_layers),\n",
    "    \"dropout\": tune.grid_search(dropout),\n",
    "    \"bidirectional\": tune.grid_search(bidirectional),\n",
    "    \"learning_rate\": tune.grid_search(learning_rate),\n",
    "    \"basin\":  tune.grid_search(basins),\n",
    "    'test_year': tune.grid_search(list(np.arange(2000,2024,2)) )\n",
    "\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "\n",
    "    All_Dates = ray.get(All_Dates_id)  \n",
    "\n",
    "    years = list(np.arange(2000,2024,2))\n",
    "    test_year = config['test_year']\n",
    "    val_years = [years[years.index(test_year)-1], years[years.index(test_year)-2]  ]\n",
    "    train_years = [year for year in years if year not in [test_year] + val_years]\n",
    "    \n",
    "    Test_Dates = All_Dates[All_Dates.year == test_year]\n",
    "    Val_Dates = All_Dates[All_Dates.year.isin(val_years)]\n",
    "    Train_Dates = All_Dates[All_Dates.year.isin(train_years)]\n",
    "\n",
    "    era5 = ray.get(era5_id)  \n",
    "    daily_flow = ray.get(daily_flow_id)  \n",
    "    climatological_flows = ray.get(climatological_flows_id)\n",
    "    climate_indices = ray.get(climate_indices_id)\n",
    "    seasonal_forecasts = ray.get(seasonal_forecasts_id)\n",
    "    Static_variables = ray.get(Static_variables_id)\n",
    "\n",
    "    val_loss = 1000\n",
    "\n",
    "    basin = config[\"basin\"]\n",
    "\n",
    "    save_path = f'/data/Hydra_Work/No_Forecast_Validation_Models/{test_year}/Specific_LSTM_Model/{basin}_specific.pth'\n",
    "    loss_path = f'/data/Hydra_Work/No_Forecast_Validation_Models/{test_year}/Specific_LSTM_Model/{basin}_specific_loss.txt'\n",
    "\n",
    "    \n",
    "    if not os.path.exists(loss_path):\n",
    "        # If the file does not exist, create it and write val_loss to it\n",
    "        with open(loss_path, 'w') as file:\n",
    "            file.write('%f' % val_loss)\n",
    "    \n",
    "    copies = 1\n",
    "    device = torch.device('cuda' if torch.cuda.\n",
    "                    is_available() else 'cpu')\n",
    "    \n",
    "    models, params_to_optimize, optimizers, schedulers = define_models(hindcast_input_size, forecast_input_size,\n",
    "    config[\"hidden_size\"], config[\"num_layers\"], config[\"dropout\"],\n",
    "    config[\"bidirectional\"], config[\"learning_rate\"], copies=copies, device = device)\n",
    "\n",
    "    losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(total_epochs):\n",
    "\n",
    "        train_losses = {}\n",
    "        epoch_val_losses = {}\n",
    "\n",
    "        for copy in range(copies):\n",
    "\n",
    "             # Need to fix the outputs of No_Body_Model_Run\n",
    "            train_losses[copy], Climate_Loss = No_Body_Model_Run(All_Dates, [basin], models[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts,\n",
    "                Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper= None, n_epochs=n_epochs,\n",
    "                batch_size=batch_size, group_lengths=group_lengths, Train_Mode=True, device=device, specialised=True)\n",
    "            epoch_val_losses[copy], Climate_Loss = No_Body_Model_Run(Val_Dates, [basin], models[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts,\n",
    "                Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper= None, n_epochs=n_epochs,\n",
    "                batch_size=batch_size, group_lengths=group_lengths, Train_Mode=False, device=device, specialised=True)\n",
    "\n",
    "        loss = np.mean(list(train_losses.values())) - Climate_Loss\n",
    "        \n",
    "\n",
    "        candidate_val_loss = ((np.mean( list(epoch_val_losses.values()) ).mean() - Climate_Loss)[0])/np.mean(Climate_Loss)\n",
    "        val_loss = np.min([val_loss, candidate_val_loss ])\n",
    "        if candidate_val_loss == val_loss:\n",
    "             torch.save(models[0], save_path)\n",
    "             \n",
    "        \n",
    "        # Check best loss so far for this model\n",
    "        with open(loss_path, 'r') as file:\n",
    "            # Read the entire contents of the file\n",
    "            Overall_Best_Val_Loss = float(file.read())\n",
    "\n",
    "        if val_loss < Overall_Best_Val_Loss:\n",
    "            torch.save(models[0], save_path)\n",
    "\n",
    "            with open(loss_path, 'w') as f:\n",
    "                f.write('%f' % val_loss)\n",
    "\n",
    "\n",
    "        ray.train.report({'val_loss' : val_loss})\n",
    "        #print(candidate_val_loss)\n",
    "        losses.append(loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 19:05:46,123\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "from ray import train, tune\n",
    "\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(runtime_env = { \"env_vars\":   {\"PYTHONPATH\": '/data/Hydra_Work/Competition_Functions/' } } )\n",
    "         \n",
    "All_Dates_id = ray.put(All_Dates)   \n",
    "era5_id = ray.put(era5)  \n",
    "daily_flow_id = ray.put(daily_flow)  \n",
    "climatological_flows_id = ray.put(climatological_flows)\n",
    "climate_indices_id = ray.put(climate_indices)\n",
    "seasonal_forecasts_id = ray.put(seasonal_forecasts)\n",
    "Static_variables_id = ray.put(static_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "asha_scheduler = ASHAScheduler(\n",
    "    time_attr='training_iteration',\n",
    "    metric='val_loss',\n",
    "    mode='min',\n",
    "    max_t=100,\n",
    "    grace_period=20,\n",
    "    reduction_factor=2,\n",
    "    brackets=1,\n",
    ")\n",
    "\n",
    "\n",
    "plateau_stopper = TrialPlateauStopper(\n",
    "    metric=\"val_loss\",\n",
    "    num_results = 20,\n",
    "    grace_period=50,\n",
    "    mode=\"min\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-06-05 11:57:04</td></tr>\n",
       "<tr><td>Running for: </td><td>16:51:15.83        </td></tr>\n",
       "<tr><td>Memory:      </td><td>30.3/125.9 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=399<br>Bracket: Iter 80.000: 0.2950924361830671 | Iter 40.000: -0.0998394753885025 | Iter 20.000: -0.6699774102167242<br>Logical resource usage: 1.25/16 CPUs, 0.08333333333333333/1 GPUs (0.0/1.0 accelerator_type:A100D)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc                  </th><th>basin               </th><th>bidirectional  </th><th style=\"text-align: right;\">  dropout</th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  num_layers</th><th style=\"text-align: right;\">  test_year</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    val_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_73fcb_00000</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>libby_reservoir_b5a0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.683</td><td style=\"text-align: right;\"> 1.53938    </td></tr>\n",
       "<tr><td>objective_73fcb_00001</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>owyhee_r_bl_owy_d480</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1833.36 </td><td style=\"text-align: right;\"> 0.250933   </td></tr>\n",
       "<tr><td>objective_73fcb_00002</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>san_joaquin_riv_4750</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">        2518.9  </td><td style=\"text-align: right;\">-0.0138664  </td></tr>\n",
       "<tr><td>objective_73fcb_00003</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>taylor_park_res_ca80</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         792.186</td><td style=\"text-align: right;\"> 2.18262    </td></tr>\n",
       "<tr><td>objective_73fcb_00004</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>boise_r_nr_boise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1578.87 </td><td style=\"text-align: right;\"> 0.591562   </td></tr>\n",
       "<tr><td>objective_73fcb_00005</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>green_r_bl_howa_e560</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1955.47 </td><td style=\"text-align: right;\"> 0.0406169  </td></tr>\n",
       "<tr><td>objective_73fcb_00006</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>weber_r_nr_oakley   </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         741.973</td><td style=\"text-align: right;\"> 1.99107    </td></tr>\n",
       "<tr><td>objective_73fcb_00007</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>detroit_lake_inflow </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1833.96 </td><td style=\"text-align: right;\"> 0.0782084  </td></tr>\n",
       "<tr><td>objective_73fcb_00008</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>virgin_r_at_virtin  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1577.42 </td><td style=\"text-align: right;\"> 0.331732   </td></tr>\n",
       "<tr><td>objective_73fcb_00009</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>dillon_reservoi_b320</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         792.279</td><td style=\"text-align: right;\"> 1.70859    </td></tr>\n",
       "<tr><td>objective_73fcb_00010</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>pueblo_reservoi_2ce0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         787.044</td><td style=\"text-align: right;\"> 0.9802     </td></tr>\n",
       "<tr><td>objective_73fcb_00011</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>hungry_horse_re_17f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         748.457</td><td style=\"text-align: right;\"> 1.04583    </td></tr>\n",
       "<tr><td>objective_73fcb_00012</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>stehekin_r_at_s_21f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         734.201</td><td style=\"text-align: right;\"> 1.39707    </td></tr>\n",
       "<tr><td>objective_73fcb_00013</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>pecos_r_nr_pecos    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1471.12 </td><td style=\"text-align: right;\"> 0.689701   </td></tr>\n",
       "<tr><td>objective_73fcb_00014</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>snake_r_nr_heise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.237</td><td style=\"text-align: right;\"> 1.66187    </td></tr>\n",
       "<tr><td>objective_73fcb_00015</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>yampa_r_nr_maybell  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         783.205</td><td style=\"text-align: right;\"> 1.60394    </td></tr>\n",
       "<tr><td>objective_73fcb_00016</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>colville_r_at_k_6b50</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1967.93 </td><td style=\"text-align: right;\"> 0.0203858  </td></tr>\n",
       "<tr><td>objective_73fcb_00017</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>missouri_r_at_toston</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         787.695</td><td style=\"text-align: right;\"> 1.01962    </td></tr>\n",
       "<tr><td>objective_73fcb_00018</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>merced_river_yo_7930</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1832.26 </td><td style=\"text-align: right;\">-0.010562   </td></tr>\n",
       "<tr><td>objective_73fcb_00019</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>animas_r_at_durango </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.869</td><td style=\"text-align: right;\"> 1.10288    </td></tr>\n",
       "<tr><td>objective_73fcb_00020</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>fontenelle_rese_f230</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         783.576</td><td style=\"text-align: right;\"> 1.39736    </td></tr>\n",
       "<tr><td>objective_73fcb_00021</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>boysen_reservoi_3050</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.026</td><td style=\"text-align: right;\"> 1.69908    </td></tr>\n",
       "<tr><td>objective_73fcb_00022</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>libby_reservoir_b5a0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.915</td><td style=\"text-align: right;\"> 1.31107    </td></tr>\n",
       "<tr><td>objective_73fcb_00023</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>owyhee_r_bl_owy_d480</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1572.62 </td><td style=\"text-align: right;\"> 0.200389   </td></tr>\n",
       "<tr><td>objective_73fcb_00024</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>san_joaquin_riv_4750</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1829.76 </td><td style=\"text-align: right;\">-0.0560171  </td></tr>\n",
       "<tr><td>objective_73fcb_00025</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>taylor_park_res_ca80</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         731.192</td><td style=\"text-align: right;\"> 2.12329    </td></tr>\n",
       "<tr><td>objective_73fcb_00026</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>boise_r_nr_boise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1571.37 </td><td style=\"text-align: right;\"> 0.64298    </td></tr>\n",
       "<tr><td>objective_73fcb_00027</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>green_r_bl_howa_e560</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1839.84 </td><td style=\"text-align: right;\"> 0.0583985  </td></tr>\n",
       "<tr><td>objective_73fcb_00028</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>weber_r_nr_oakley   </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         783.033</td><td style=\"text-align: right;\"> 1.93887    </td></tr>\n",
       "<tr><td>objective_73fcb_00029</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>detroit_lake_inflow </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1959.11 </td><td style=\"text-align: right;\"> 0.0783653  </td></tr>\n",
       "<tr><td>objective_73fcb_00030</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>virgin_r_at_virtin  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1572.99 </td><td style=\"text-align: right;\"> 0.313212   </td></tr>\n",
       "<tr><td>objective_73fcb_00031</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>dillon_reservoi_b320</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.414</td><td style=\"text-align: right;\"> 1.59719    </td></tr>\n",
       "<tr><td>objective_73fcb_00032</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>pueblo_reservoi_2ce0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1572.12 </td><td style=\"text-align: right;\"> 0.932388   </td></tr>\n",
       "<tr><td>objective_73fcb_00033</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>hungry_horse_re_17f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         730.574</td><td style=\"text-align: right;\"> 1.01504    </td></tr>\n",
       "<tr><td>objective_73fcb_00034</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>stehekin_r_at_s_21f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         788.526</td><td style=\"text-align: right;\"> 1.40941    </td></tr>\n",
       "<tr><td>objective_73fcb_00035</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>pecos_r_nr_pecos    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">        3682.26 </td><td style=\"text-align: right;\">-0.171296   </td></tr>\n",
       "<tr><td>objective_73fcb_00036</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>snake_r_nr_heise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         787.059</td><td style=\"text-align: right;\"> 1.49502    </td></tr>\n",
       "<tr><td>objective_73fcb_00037</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>yampa_r_nr_maybell  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         787.546</td><td style=\"text-align: right;\"> 1.11789    </td></tr>\n",
       "<tr><td>objective_73fcb_00038</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>colville_r_at_k_6b50</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1826.06 </td><td style=\"text-align: right;\"> 0.0160382  </td></tr>\n",
       "<tr><td>objective_73fcb_00039</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>missouri_r_at_toston</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1467.18 </td><td style=\"text-align: right;\"> 0.873695   </td></tr>\n",
       "<tr><td>objective_73fcb_00040</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>merced_river_yo_7930</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1965.21 </td><td style=\"text-align: right;\">-0.142887   </td></tr>\n",
       "<tr><td>objective_73fcb_00041</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>animas_r_at_durango </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1576.32 </td><td style=\"text-align: right;\"> 0.283441   </td></tr>\n",
       "<tr><td>objective_73fcb_00042</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>fontenelle_rese_f230</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">        3030.15 </td><td style=\"text-align: right;\">-0.295092   </td></tr>\n",
       "<tr><td>objective_73fcb_00043</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>boysen_reservoi_3050</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2000</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.686</td><td style=\"text-align: right;\"> 1.69384    </td></tr>\n",
       "<tr><td>objective_73fcb_00044</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>libby_reservoir_b5a0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.582</td><td style=\"text-align: right;\"> 1.20967    </td></tr>\n",
       "<tr><td>objective_73fcb_00045</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>owyhee_r_bl_owy_d480</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1572.44 </td><td style=\"text-align: right;\"> 0.477874   </td></tr>\n",
       "<tr><td>objective_73fcb_00046</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>san_joaquin_riv_4750</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">        2132.32 </td><td style=\"text-align: right;\"> 0.0608934  </td></tr>\n",
       "<tr><td>objective_73fcb_00047</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>taylor_park_res_ca80</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.997</td><td style=\"text-align: right;\"> 1.97313    </td></tr>\n",
       "<tr><td>objective_73fcb_00048</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>boise_r_nr_boise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         783.991</td><td style=\"text-align: right;\"> 1.04342    </td></tr>\n",
       "<tr><td>objective_73fcb_00049</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>green_r_bl_howa_e560</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1966.77 </td><td style=\"text-align: right;\">-0.00468203 </td></tr>\n",
       "<tr><td>objective_73fcb_00050</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>weber_r_nr_oakley   </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.189</td><td style=\"text-align: right;\"> 1.8231     </td></tr>\n",
       "<tr><td>objective_73fcb_00051</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>detroit_lake_inflow </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1469.12 </td><td style=\"text-align: right;\"> 0.143097   </td></tr>\n",
       "<tr><td>objective_73fcb_00052</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>virgin_r_at_virtin  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1570.72 </td><td style=\"text-align: right;\"> 0.281785   </td></tr>\n",
       "<tr><td>objective_73fcb_00053</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>dillon_reservoi_b320</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         783.958</td><td style=\"text-align: right;\"> 1.27759    </td></tr>\n",
       "<tr><td>objective_73fcb_00054</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>pueblo_reservoi_2ce0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1462    </td><td style=\"text-align: right;\"> 0.910647   </td></tr>\n",
       "<tr><td>objective_73fcb_00055</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>hungry_horse_re_17f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         787.119</td><td style=\"text-align: right;\"> 1.10527    </td></tr>\n",
       "<tr><td>objective_73fcb_00056</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>stehekin_r_at_s_21f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.698</td><td style=\"text-align: right;\"> 1.33149    </td></tr>\n",
       "<tr><td>objective_73fcb_00057</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>pecos_r_nr_pecos    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1965.38 </td><td style=\"text-align: right;\"> 0.0802584  </td></tr>\n",
       "<tr><td>objective_73fcb_00058</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>snake_r_nr_heise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.615</td><td style=\"text-align: right;\"> 1.74549    </td></tr>\n",
       "<tr><td>objective_73fcb_00059</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>yampa_r_nr_maybell  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         783.63 </td><td style=\"text-align: right;\"> 1.60843    </td></tr>\n",
       "<tr><td>objective_73fcb_00060</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>colville_r_at_k_6b50</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1576.18 </td><td style=\"text-align: right;\"> 0.5173     </td></tr>\n",
       "<tr><td>objective_73fcb_00061</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>missouri_r_at_toston</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1468.97 </td><td style=\"text-align: right;\"> 0.398132   </td></tr>\n",
       "<tr><td>objective_73fcb_00062</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>merced_river_yo_7930</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1467.11 </td><td style=\"text-align: right;\"> 0.474807   </td></tr>\n",
       "<tr><td>objective_73fcb_00063</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>animas_r_at_durango </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.634</td><td style=\"text-align: right;\"> 1.24557    </td></tr>\n",
       "<tr><td>objective_73fcb_00064</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>fontenelle_rese_f230</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.32 </td><td style=\"text-align: right;\"> 1.18752    </td></tr>\n",
       "<tr><td>objective_73fcb_00065</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>boysen_reservoi_3050</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.949</td><td style=\"text-align: right;\"> 1.25501    </td></tr>\n",
       "<tr><td>objective_73fcb_00066</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>libby_reservoir_b5a0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         730.14 </td><td style=\"text-align: right;\"> 1.22775    </td></tr>\n",
       "<tr><td>objective_73fcb_00067</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>owyhee_r_bl_owy_d480</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        3132.4  </td><td style=\"text-align: right;\">-0.230011   </td></tr>\n",
       "<tr><td>objective_73fcb_00068</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>san_joaquin_riv_4750</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1964.75 </td><td style=\"text-align: right;\"> 0.0247555  </td></tr>\n",
       "<tr><td>objective_73fcb_00069</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>taylor_park_res_ca80</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         732.797</td><td style=\"text-align: right;\"> 1.85559    </td></tr>\n",
       "<tr><td>objective_73fcb_00070</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>boise_r_nr_boise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1566.14 </td><td style=\"text-align: right;\"> 0.227446   </td></tr>\n",
       "<tr><td>objective_73fcb_00071</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>green_r_bl_howa_e560</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1962.73 </td><td style=\"text-align: right;\">-0.012767   </td></tr>\n",
       "<tr><td>objective_73fcb_00072</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>weber_r_nr_oakley   </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         787.251</td><td style=\"text-align: right;\"> 1.57822    </td></tr>\n",
       "<tr><td>objective_73fcb_00073</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>detroit_lake_inflow </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1570.71 </td><td style=\"text-align: right;\"> 0.163161   </td></tr>\n",
       "<tr><td>objective_73fcb_00074</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>virgin_r_at_virtin  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1463.88 </td><td style=\"text-align: right;\"> 0.311364   </td></tr>\n",
       "<tr><td>objective_73fcb_00075</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>dillon_reservoi_b320</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         733.758</td><td style=\"text-align: right;\"> 1.18413    </td></tr>\n",
       "<tr><td>objective_73fcb_00076</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>pueblo_reservoi_2ce0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1571.15 </td><td style=\"text-align: right;\"> 0.542661   </td></tr>\n",
       "<tr><td>objective_73fcb_00077</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>hungry_horse_re_17f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         787.453</td><td style=\"text-align: right;\"> 1.02806    </td></tr>\n",
       "<tr><td>objective_73fcb_00078</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>stehekin_r_at_s_21f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         736.32 </td><td style=\"text-align: right;\"> 1.27799    </td></tr>\n",
       "<tr><td>objective_73fcb_00079</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>pecos_r_nr_pecos    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">        3416.06 </td><td style=\"text-align: right;\">-0.517411   </td></tr>\n",
       "<tr><td>objective_73fcb_00080</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>snake_r_nr_heise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.604</td><td style=\"text-align: right;\"> 1.65551    </td></tr>\n",
       "<tr><td>objective_73fcb_00081</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>yampa_r_nr_maybell  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         733.295</td><td style=\"text-align: right;\"> 1.01785    </td></tr>\n",
       "<tr><td>objective_73fcb_00082</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>colville_r_at_k_6b50</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1572.78 </td><td style=\"text-align: right;\"> 0.51297    </td></tr>\n",
       "<tr><td>objective_73fcb_00083</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>missouri_r_at_toston</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1564.66 </td><td style=\"text-align: right;\"> 0.206981   </td></tr>\n",
       "<tr><td>objective_73fcb_00084</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>merced_river_yo_7930</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1473.49 </td><td style=\"text-align: right;\"> 0.469517   </td></tr>\n",
       "<tr><td>objective_73fcb_00085</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>animas_r_at_durango </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1570.31 </td><td style=\"text-align: right;\"> 0.369798   </td></tr>\n",
       "<tr><td>objective_73fcb_00086</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>fontenelle_rese_f230</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.691</td><td style=\"text-align: right;\"> 0.998608   </td></tr>\n",
       "<tr><td>objective_73fcb_00087</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>boysen_reservoi_3050</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2002</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        3660.23 </td><td style=\"text-align: right;\">-0.400977   </td></tr>\n",
       "<tr><td>objective_73fcb_00088</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>libby_reservoir_b5a0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.386</td><td style=\"text-align: right;\"> 1.34138    </td></tr>\n",
       "<tr><td>objective_73fcb_00089</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>owyhee_r_bl_owy_d480</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1465.84 </td><td style=\"text-align: right;\"> 0.660553   </td></tr>\n",
       "<tr><td>objective_73fcb_00090</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>san_joaquin_riv_4750</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">        2594.27 </td><td style=\"text-align: right;\">-0.130077   </td></tr>\n",
       "<tr><td>objective_73fcb_00091</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>taylor_park_res_ca80</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1570.81 </td><td style=\"text-align: right;\"> 0.645147   </td></tr>\n",
       "<tr><td>objective_73fcb_00092</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>boise_r_nr_boise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.677</td><td style=\"text-align: right;\"> 1.4604     </td></tr>\n",
       "<tr><td>objective_73fcb_00093</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>green_r_bl_howa_e560</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1961.73 </td><td style=\"text-align: right;\"> 0.0801855  </td></tr>\n",
       "<tr><td>objective_73fcb_00094</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>weber_r_nr_oakley   </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         782.089</td><td style=\"text-align: right;\"> 1.56309    </td></tr>\n",
       "<tr><td>objective_73fcb_00095</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>detroit_lake_inflow </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1964.63 </td><td style=\"text-align: right;\"> 0.165768   </td></tr>\n",
       "<tr><td>objective_73fcb_00096</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>virgin_r_at_virtin  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1847.42 </td><td style=\"text-align: right;\"> 0.00804369 </td></tr>\n",
       "<tr><td>objective_73fcb_00097</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>dillon_reservoi_b320</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1565.49 </td><td style=\"text-align: right;\"> 0.673884   </td></tr>\n",
       "<tr><td>objective_73fcb_00098</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>pueblo_reservoi_2ce0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1964.23 </td><td style=\"text-align: right;\"> 0.0627757  </td></tr>\n",
       "<tr><td>objective_73fcb_00099</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>hungry_horse_re_17f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.69 </td><td style=\"text-align: right;\"> 1.27971    </td></tr>\n",
       "<tr><td>objective_73fcb_00100</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>stehekin_r_at_s_21f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         731.383</td><td style=\"text-align: right;\"> 1.34748    </td></tr>\n",
       "<tr><td>objective_73fcb_00101</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>pecos_r_nr_pecos    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1950.91 </td><td style=\"text-align: right;\">-0.379714   </td></tr>\n",
       "<tr><td>objective_73fcb_00102</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>snake_r_nr_heise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.536</td><td style=\"text-align: right;\"> 1.74702    </td></tr>\n",
       "<tr><td>objective_73fcb_00103</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>yampa_r_nr_maybell  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1572.72 </td><td style=\"text-align: right;\"> 0.498787   </td></tr>\n",
       "<tr><td>objective_73fcb_00104</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>colville_r_at_k_6b50</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1465.51 </td><td style=\"text-align: right;\"> 0.805077   </td></tr>\n",
       "<tr><td>objective_73fcb_00105</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>missouri_r_at_toston</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1839.1  </td><td style=\"text-align: right;\"> 0.142059   </td></tr>\n",
       "<tr><td>objective_73fcb_00106</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>merced_river_yo_7930</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1571.95 </td><td style=\"text-align: right;\"> 0.756908   </td></tr>\n",
       "<tr><td>objective_73fcb_00107</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>animas_r_at_durango </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1953.79 </td><td style=\"text-align: right;\"> 0.119126   </td></tr>\n",
       "<tr><td>objective_73fcb_00108</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>fontenelle_rese_f230</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1570.54 </td><td style=\"text-align: right;\"> 0.576933   </td></tr>\n",
       "<tr><td>objective_73fcb_00109</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>boysen_reservoi_3050</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1963.36 </td><td style=\"text-align: right;\"> 0.0025259  </td></tr>\n",
       "<tr><td>objective_73fcb_00110</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>libby_reservoir_b5a0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         738.017</td><td style=\"text-align: right;\"> 1.10571    </td></tr>\n",
       "<tr><td>objective_73fcb_00111</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>owyhee_r_bl_owy_d480</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1573.28 </td><td style=\"text-align: right;\"> 0.608066   </td></tr>\n",
       "<tr><td>objective_73fcb_00112</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>san_joaquin_riv_4750</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1963.99 </td><td style=\"text-align: right;\">-0.259848   </td></tr>\n",
       "<tr><td>objective_73fcb_00113</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>taylor_park_res_ca80</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        3667.13 </td><td style=\"text-align: right;\">-0.401257   </td></tr>\n",
       "<tr><td>objective_73fcb_00114</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>boise_r_nr_boise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         737.757</td><td style=\"text-align: right;\"> 1.39891    </td></tr>\n",
       "<tr><td>objective_73fcb_00115</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>green_r_bl_howa_e560</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1951.9  </td><td style=\"text-align: right;\"> 0.0825651  </td></tr>\n",
       "<tr><td>objective_73fcb_00116</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>weber_r_nr_oakley   </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         734.161</td><td style=\"text-align: right;\"> 1.46452    </td></tr>\n",
       "<tr><td>objective_73fcb_00117</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>detroit_lake_inflow </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1571.92 </td><td style=\"text-align: right;\"> 0.172286   </td></tr>\n",
       "<tr><td>objective_73fcb_00118</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>virgin_r_at_virtin  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">        3610.98 </td><td style=\"text-align: right;\">-0.388741   </td></tr>\n",
       "<tr><td>objective_73fcb_00119</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>dillon_reservoi_b320</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1571.64 </td><td style=\"text-align: right;\"> 0.368129   </td></tr>\n",
       "<tr><td>objective_73fcb_00120</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>pueblo_reservoi_2ce0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        3127.8  </td><td style=\"text-align: right;\">-0.276738   </td></tr>\n",
       "<tr><td>objective_73fcb_00121</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>hungry_horse_re_17f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1476.08 </td><td style=\"text-align: right;\"> 0.323497   </td></tr>\n",
       "<tr><td>objective_73fcb_00122</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>stehekin_r_at_s_21f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         734.183</td><td style=\"text-align: right;\"> 1.14973    </td></tr>\n",
       "<tr><td>objective_73fcb_00123</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>pecos_r_nr_pecos    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1964.44 </td><td style=\"text-align: right;\">-0.388324   </td></tr>\n",
       "<tr><td>objective_73fcb_00124</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>snake_r_nr_heise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         782.123</td><td style=\"text-align: right;\"> 1.59442    </td></tr>\n",
       "<tr><td>objective_73fcb_00125</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>yampa_r_nr_maybell  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1570.42 </td><td style=\"text-align: right;\"> 0.457002   </td></tr>\n",
       "<tr><td>objective_73fcb_00126</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>colville_r_at_k_6b50</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         735.669</td><td style=\"text-align: right;\"> 0.824985   </td></tr>\n",
       "<tr><td>objective_73fcb_00127</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>missouri_r_at_toston</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        3927.92 </td><td style=\"text-align: right;\">-0.373928   </td></tr>\n",
       "<tr><td>objective_73fcb_00128</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>merced_river_yo_7930</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.27 </td><td style=\"text-align: right;\"> 0.782604   </td></tr>\n",
       "<tr><td>objective_73fcb_00129</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>animas_r_at_durango </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        3904.31 </td><td style=\"text-align: right;\">-0.511129   </td></tr>\n",
       "<tr><td>objective_73fcb_00130</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>fontenelle_rese_f230</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1559.94 </td><td style=\"text-align: right;\"> 0.257439   </td></tr>\n",
       "<tr><td>objective_73fcb_00131</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>boysen_reservoi_3050</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2004</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">        3341.47 </td><td style=\"text-align: right;\">-0.615589   </td></tr>\n",
       "<tr><td>objective_73fcb_00132</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>libby_reservoir_b5a0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         740.176</td><td style=\"text-align: right;\"> 1.26644    </td></tr>\n",
       "<tr><td>objective_73fcb_00133</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>owyhee_r_bl_owy_d480</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1464.4  </td><td style=\"text-align: right;\"> 0.602986   </td></tr>\n",
       "<tr><td>objective_73fcb_00134</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>san_joaquin_riv_4750</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1966.11 </td><td style=\"text-align: right;\">-0.392811   </td></tr>\n",
       "<tr><td>objective_73fcb_00135</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>taylor_park_res_ca80</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1572.31 </td><td style=\"text-align: right;\"> 0.521307   </td></tr>\n",
       "<tr><td>objective_73fcb_00136</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>boise_r_nr_boise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.712</td><td style=\"text-align: right;\"> 1.13485    </td></tr>\n",
       "<tr><td>objective_73fcb_00137</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>green_r_bl_howa_e560</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1851.48 </td><td style=\"text-align: right;\"> 0.135508   </td></tr>\n",
       "<tr><td>objective_73fcb_00138</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>weber_r_nr_oakley   </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         779.894</td><td style=\"text-align: right;\"> 1.17671    </td></tr>\n",
       "<tr><td>objective_73fcb_00139</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>detroit_lake_inflow </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1840.86 </td><td style=\"text-align: right;\"> 0.0907764  </td></tr>\n",
       "<tr><td>objective_73fcb_00140</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>virgin_r_at_virtin  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1961.77 </td><td style=\"text-align: right;\">-0.151505   </td></tr>\n",
       "<tr><td>objective_73fcb_00141</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>dillon_reservoi_b320</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1824.86 </td><td style=\"text-align: right;\"> 0.0904465  </td></tr>\n",
       "<tr><td>objective_73fcb_00142</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>pueblo_reservoi_2ce0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1955.36 </td><td style=\"text-align: right;\">-0.0696591  </td></tr>\n",
       "<tr><td>objective_73fcb_00143</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>hungry_horse_re_17f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         738.286</td><td style=\"text-align: right;\"> 1.37272    </td></tr>\n",
       "<tr><td>objective_73fcb_00144</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>stehekin_r_at_s_21f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.872</td><td style=\"text-align: right;\"> 1.59786    </td></tr>\n",
       "<tr><td>objective_73fcb_00145</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>pecos_r_nr_pecos    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1560.01 </td><td style=\"text-align: right;\"> 0.163502   </td></tr>\n",
       "<tr><td>objective_73fcb_00146</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>snake_r_nr_heise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.344</td><td style=\"text-align: right;\"> 1.27131    </td></tr>\n",
       "<tr><td>objective_73fcb_00147</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>yampa_r_nr_maybell  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1850.45 </td><td style=\"text-align: right;\"> 0.0130431  </td></tr>\n",
       "<tr><td>objective_73fcb_00148</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>colville_r_at_k_6b50</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1843.74 </td><td style=\"text-align: right;\"> 0.00869296 </td></tr>\n",
       "<tr><td>objective_73fcb_00149</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>missouri_r_at_toston</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1966.76 </td><td style=\"text-align: right;\">-0.102265   </td></tr>\n",
       "<tr><td>objective_73fcb_00150</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>merced_river_yo_7930</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.745</td><td style=\"text-align: right;\"> 0.718753   </td></tr>\n",
       "<tr><td>objective_73fcb_00151</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>animas_r_at_durango </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1572.37 </td><td style=\"text-align: right;\"> 0.195664   </td></tr>\n",
       "<tr><td>objective_73fcb_00152</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>fontenelle_rese_f230</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1472.07 </td><td style=\"text-align: right;\"> 0.185397   </td></tr>\n",
       "<tr><td>objective_73fcb_00153</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>boysen_reservoi_3050</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1819.74 </td><td style=\"text-align: right;\">-0.0237226  </td></tr>\n",
       "<tr><td>objective_73fcb_00154</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>libby_reservoir_b5a0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.552</td><td style=\"text-align: right;\"> 1.08512    </td></tr>\n",
       "<tr><td>objective_73fcb_00155</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>owyhee_r_bl_owy_d480</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1569.17 </td><td style=\"text-align: right;\"> 0.62347    </td></tr>\n",
       "<tr><td>objective_73fcb_00156</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>san_joaquin_riv_4750</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1952.25 </td><td style=\"text-align: right;\">-0.437354   </td></tr>\n",
       "<tr><td>objective_73fcb_00157</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>taylor_park_res_ca80</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1564.86 </td><td style=\"text-align: right;\"> 0.174422   </td></tr>\n",
       "<tr><td>objective_73fcb_00158</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>boise_r_nr_boise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         780.401</td><td style=\"text-align: right;\"> 0.888639   </td></tr>\n",
       "<tr><td>objective_73fcb_00159</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>green_r_bl_howa_e560</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1961.96 </td><td style=\"text-align: right;\"> 0.126581   </td></tr>\n",
       "<tr><td>objective_73fcb_00160</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>weber_r_nr_oakley   </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.987</td><td style=\"text-align: right;\"> 0.739674   </td></tr>\n",
       "<tr><td>objective_73fcb_00161</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>detroit_lake_inflow </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1854.48 </td><td style=\"text-align: right;\"> 0.0869087  </td></tr>\n",
       "<tr><td>objective_73fcb_00162</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>virgin_r_at_virtin  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1847.8  </td><td style=\"text-align: right;\">-0.0833267  </td></tr>\n",
       "<tr><td>objective_73fcb_00163</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>dillon_reservoi_b320</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1949.8  </td><td style=\"text-align: right;\">-0.188873   </td></tr>\n",
       "<tr><td>objective_73fcb_00164</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>pueblo_reservoi_2ce0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">        3133.96 </td><td style=\"text-align: right;\">-0.441598   </td></tr>\n",
       "<tr><td>objective_73fcb_00165</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>hungry_horse_re_17f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.413</td><td style=\"text-align: right;\"> 1.33014    </td></tr>\n",
       "<tr><td>objective_73fcb_00166</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>stehekin_r_at_s_21f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.701</td><td style=\"text-align: right;\"> 1.50577    </td></tr>\n",
       "<tr><td>objective_73fcb_00167</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>pecos_r_nr_pecos    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        3142.29 </td><td style=\"text-align: right;\">-0.16998    </td></tr>\n",
       "<tr><td>objective_73fcb_00168</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>snake_r_nr_heise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.264</td><td style=\"text-align: right;\"> 1.07425    </td></tr>\n",
       "<tr><td>objective_73fcb_00169</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>yampa_r_nr_maybell  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1957.1  </td><td style=\"text-align: right;\"> 0.0433608  </td></tr>\n",
       "<tr><td>objective_73fcb_00170</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>colville_r_at_k_6b50</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1823.85 </td><td style=\"text-align: right;\"> 0.0693752  </td></tr>\n",
       "<tr><td>objective_73fcb_00171</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>missouri_r_at_toston</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">        2969.74 </td><td style=\"text-align: right;\">-0.508102   </td></tr>\n",
       "<tr><td>objective_73fcb_00172</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>merced_river_yo_7930</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.575</td><td style=\"text-align: right;\"> 0.662896   </td></tr>\n",
       "<tr><td>objective_73fcb_00173</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>animas_r_at_durango </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        3140.24 </td><td style=\"text-align: right;\">-0.246962   </td></tr>\n",
       "<tr><td>objective_73fcb_00174</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>fontenelle_rese_f230</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">        2191.6  </td><td style=\"text-align: right;\">-0.506883   </td></tr>\n",
       "<tr><td>objective_73fcb_00175</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>boysen_reservoi_3050</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2006</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">        2354.51 </td><td style=\"text-align: right;\">-0.666357   </td></tr>\n",
       "<tr><td>objective_73fcb_00176</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>libby_reservoir_b5a0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         742.141</td><td style=\"text-align: right;\"> 1.18114    </td></tr>\n",
       "<tr><td>objective_73fcb_00177</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>owyhee_r_bl_owy_d480</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1848.99 </td><td style=\"text-align: right;\"> 0.0901178  </td></tr>\n",
       "<tr><td>objective_73fcb_00178</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>san_joaquin_riv_4750</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1959.62 </td><td style=\"text-align: right;\">-0.0348238  </td></tr>\n",
       "<tr><td>objective_73fcb_00179</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>taylor_park_res_ca80</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         780.383</td><td style=\"text-align: right;\"> 2.19172    </td></tr>\n",
       "<tr><td>objective_73fcb_00180</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>boise_r_nr_boise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1460.53 </td><td style=\"text-align: right;\"> 0.486339   </td></tr>\n",
       "<tr><td>objective_73fcb_00181</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>green_r_bl_howa_e560</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1485.03 </td><td style=\"text-align: right;\"> 0.181401   </td></tr>\n",
       "<tr><td>objective_73fcb_00182</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>weber_r_nr_oakley   </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         783.496</td><td style=\"text-align: right;\"> 1.21465    </td></tr>\n",
       "<tr><td>objective_73fcb_00183</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>detroit_lake_inflow </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1561.1  </td><td style=\"text-align: right;\"> 0.131194   </td></tr>\n",
       "<tr><td>objective_73fcb_00184</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>virgin_r_at_virtin  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1566.37 </td><td style=\"text-align: right;\"> 0.53504    </td></tr>\n",
       "<tr><td>objective_73fcb_00185</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>dillon_reservoi_b320</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         737.123</td><td style=\"text-align: right;\"> 0.710624   </td></tr>\n",
       "<tr><td>objective_73fcb_00186</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>pueblo_reservoi_2ce0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         782.736</td><td style=\"text-align: right;\"> 0.937225   </td></tr>\n",
       "<tr><td>objective_73fcb_00187</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>hungry_horse_re_17f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.179</td><td style=\"text-align: right;\"> 1.42021    </td></tr>\n",
       "<tr><td>objective_73fcb_00188</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>stehekin_r_at_s_21f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.376</td><td style=\"text-align: right;\"> 1.56726    </td></tr>\n",
       "<tr><td>objective_73fcb_00189</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>pecos_r_nr_pecos    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1473.57 </td><td style=\"text-align: right;\"> 0.290811   </td></tr>\n",
       "<tr><td>objective_73fcb_00190</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>snake_r_nr_heise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         782.131</td><td style=\"text-align: right;\"> 1.25284    </td></tr>\n",
       "<tr><td>objective_73fcb_00191</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>yampa_r_nr_maybell  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         782.493</td><td style=\"text-align: right;\"> 1.05828    </td></tr>\n",
       "<tr><td>objective_73fcb_00192</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>colville_r_at_k_6b50</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1826.09 </td><td style=\"text-align: right;\"> 0.0879499  </td></tr>\n",
       "<tr><td>objective_73fcb_00193</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>missouri_r_at_toston</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1485.63 </td><td style=\"text-align: right;\"> 0.288579   </td></tr>\n",
       "<tr><td>objective_73fcb_00194</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>merced_river_yo_7930</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1845.83 </td><td style=\"text-align: right;\"> 0.056204   </td></tr>\n",
       "<tr><td>objective_73fcb_00195</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>animas_r_at_durango </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.354</td><td style=\"text-align: right;\"> 1.54809    </td></tr>\n",
       "<tr><td>objective_73fcb_00196</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>fontenelle_rese_f230</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         783.56 </td><td style=\"text-align: right;\"> 1.07429    </td></tr>\n",
       "<tr><td>objective_73fcb_00197</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>boysen_reservoi_3050</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1952.9  </td><td style=\"text-align: right;\">-0.190364   </td></tr>\n",
       "<tr><td>objective_73fcb_00198</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>libby_reservoir_b5a0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.028</td><td style=\"text-align: right;\"> 1.09453    </td></tr>\n",
       "<tr><td>objective_73fcb_00199</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>owyhee_r_bl_owy_d480</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1963.58 </td><td style=\"text-align: right;\"> 0.0919697  </td></tr>\n",
       "<tr><td>objective_73fcb_00200</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>san_joaquin_riv_4750</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1959.22 </td><td style=\"text-align: right;\">-0.00309192 </td></tr>\n",
       "<tr><td>objective_73fcb_00201</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>taylor_park_res_ca80</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         781.858</td><td style=\"text-align: right;\"> 2.09127    </td></tr>\n",
       "<tr><td>objective_73fcb_00202</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>boise_r_nr_boise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1566.05 </td><td style=\"text-align: right;\"> 0.231722   </td></tr>\n",
       "<tr><td>objective_73fcb_00203</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>green_r_bl_howa_e560</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1570.11 </td><td style=\"text-align: right;\"> 0.206136   </td></tr>\n",
       "<tr><td>objective_73fcb_00204</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>weber_r_nr_oakley   </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         782.768</td><td style=\"text-align: right;\"> 0.949933   </td></tr>\n",
       "<tr><td>objective_73fcb_00205</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>detroit_lake_inflow </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1469.43 </td><td style=\"text-align: right;\"> 0.152529   </td></tr>\n",
       "<tr><td>objective_73fcb_00206</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>virgin_r_at_virtin  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1569.83 </td><td style=\"text-align: right;\"> 0.31474    </td></tr>\n",
       "<tr><td>objective_73fcb_00207</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>dillon_reservoi_b320</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         783.368</td><td style=\"text-align: right;\"> 0.66454    </td></tr>\n",
       "<tr><td>objective_73fcb_00208</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>pueblo_reservoi_2ce0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         742.839</td><td style=\"text-align: right;\"> 0.959156   </td></tr>\n",
       "<tr><td>objective_73fcb_00209</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>hungry_horse_re_17f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         732.237</td><td style=\"text-align: right;\"> 1.01185    </td></tr>\n",
       "<tr><td>objective_73fcb_00210</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>stehekin_r_at_s_21f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         783.256</td><td style=\"text-align: right;\"> 1.58126    </td></tr>\n",
       "<tr><td>objective_73fcb_00211</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>pecos_r_nr_pecos    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1477.6  </td><td style=\"text-align: right;\"> 0.27233    </td></tr>\n",
       "<tr><td>objective_73fcb_00212</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>snake_r_nr_heise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         781.543</td><td style=\"text-align: right;\"> 1.18384    </td></tr>\n",
       "<tr><td>objective_73fcb_00213</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>yampa_r_nr_maybell  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         745.514</td><td style=\"text-align: right;\"> 1.07763    </td></tr>\n",
       "<tr><td>objective_73fcb_00214</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>colville_r_at_k_6b50</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1956.44 </td><td style=\"text-align: right;\"> 0.0727129  </td></tr>\n",
       "<tr><td>objective_73fcb_00215</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>missouri_r_at_toston</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1570.78 </td><td style=\"text-align: right;\"> 0.301539   </td></tr>\n",
       "<tr><td>objective_73fcb_00216</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>merced_river_yo_7930</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1959.49 </td><td style=\"text-align: right;\"> 0.0992813  </td></tr>\n",
       "<tr><td>objective_73fcb_00217</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>animas_r_at_durango </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         730.693</td><td style=\"text-align: right;\"> 1.51017    </td></tr>\n",
       "<tr><td>objective_73fcb_00218</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>fontenelle_rese_f230</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         783.85 </td><td style=\"text-align: right;\"> 1.02076    </td></tr>\n",
       "<tr><td>objective_73fcb_00219</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>boysen_reservoi_3050</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2008</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">        3525.15 </td><td style=\"text-align: right;\">-0.613284   </td></tr>\n",
       "<tr><td>objective_73fcb_00220</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>libby_reservoir_b5a0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.269</td><td style=\"text-align: right;\"> 1.32154    </td></tr>\n",
       "<tr><td>objective_73fcb_00221</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>owyhee_r_bl_owy_d480</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1833.59 </td><td style=\"text-align: right;\"> 0.0678627  </td></tr>\n",
       "<tr><td>objective_73fcb_00222</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>san_joaquin_riv_4750</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1962.7  </td><td style=\"text-align: right;\">-0.051754   </td></tr>\n",
       "<tr><td>objective_73fcb_00223</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>taylor_park_res_ca80</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         743.577</td><td style=\"text-align: right;\"> 0.914545   </td></tr>\n",
       "<tr><td>objective_73fcb_00224</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>boise_r_nr_boise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1563.83 </td><td style=\"text-align: right;\"> 0.517826   </td></tr>\n",
       "<tr><td>objective_73fcb_00225</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>green_r_bl_howa_e560</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1828.43 </td><td style=\"text-align: right;\">-0.0183979  </td></tr>\n",
       "<tr><td>objective_73fcb_00226</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>weber_r_nr_oakley   </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.165</td><td style=\"text-align: right;\"> 0.913908   </td></tr>\n",
       "<tr><td>objective_73fcb_00227</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>detroit_lake_inflow </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1958.01 </td><td style=\"text-align: right;\"> 0.0560504  </td></tr>\n",
       "<tr><td>objective_73fcb_00228</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>virgin_r_at_virtin  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1473.6  </td><td style=\"text-align: right;\"> 0.462089   </td></tr>\n",
       "<tr><td>objective_73fcb_00229</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>dillon_reservoi_b320</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         743.341</td><td style=\"text-align: right;\"> 1.08326    </td></tr>\n",
       "<tr><td>objective_73fcb_00230</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>pueblo_reservoi_2ce0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.52 </td><td style=\"text-align: right;\"> 1.21121    </td></tr>\n",
       "<tr><td>objective_73fcb_00231</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>hungry_horse_re_17f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         783.506</td><td style=\"text-align: right;\"> 1.18459    </td></tr>\n",
       "<tr><td>objective_73fcb_00232</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>stehekin_r_at_s_21f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         783.133</td><td style=\"text-align: right;\"> 1.15137    </td></tr>\n",
       "<tr><td>objective_73fcb_00233</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>pecos_r_nr_pecos    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1569.19 </td><td style=\"text-align: right;\"> 0.303706   </td></tr>\n",
       "<tr><td>objective_73fcb_00234</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>snake_r_nr_heise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         733.437</td><td style=\"text-align: right;\"> 1.14929    </td></tr>\n",
       "<tr><td>objective_73fcb_00235</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>yampa_r_nr_maybell  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         743.006</td><td style=\"text-align: right;\"> 0.771187   </td></tr>\n",
       "<tr><td>objective_73fcb_00236</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>colville_r_at_k_6b50</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1565.67 </td><td style=\"text-align: right;\"> 0.383719   </td></tr>\n",
       "<tr><td>objective_73fcb_00237</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>missouri_r_at_toston</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.111</td><td style=\"text-align: right;\"> 0.979923   </td></tr>\n",
       "<tr><td>objective_73fcb_00238</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>merced_river_yo_7930</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1966.53 </td><td style=\"text-align: right;\"> 0.0785599  </td></tr>\n",
       "<tr><td>objective_73fcb_00239</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>animas_r_at_durango </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.797</td><td style=\"text-align: right;\"> 1.19018    </td></tr>\n",
       "<tr><td>objective_73fcb_00240</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>fontenelle_rese_f230</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         734.02 </td><td style=\"text-align: right;\"> 1.45456    </td></tr>\n",
       "<tr><td>objective_73fcb_00241</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>boysen_reservoi_3050</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1567.48 </td><td style=\"text-align: right;\"> 0.616408   </td></tr>\n",
       "<tr><td>objective_73fcb_00242</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>libby_reservoir_b5a0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         731.165</td><td style=\"text-align: right;\"> 1.04911    </td></tr>\n",
       "<tr><td>objective_73fcb_00243</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>owyhee_r_bl_owy_d480</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">        2751.09 </td><td style=\"text-align: right;\">-0.0438651  </td></tr>\n",
       "<tr><td>objective_73fcb_00244</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>san_joaquin_riv_4750</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1858.59 </td><td style=\"text-align: right;\">-6.98898e-05</td></tr>\n",
       "<tr><td>objective_73fcb_00245</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>taylor_park_res_ca80</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1565.74 </td><td style=\"text-align: right;\"> 0.206198   </td></tr>\n",
       "<tr><td>objective_73fcb_00246</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>boise_r_nr_boise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1572.23 </td><td style=\"text-align: right;\"> 0.310287   </td></tr>\n",
       "<tr><td>objective_73fcb_00247</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>green_r_bl_howa_e560</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1831.99 </td><td style=\"text-align: right;\">-0.0177952  </td></tr>\n",
       "<tr><td>objective_73fcb_00248</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>weber_r_nr_oakley   </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.612</td><td style=\"text-align: right;\"> 0.91992    </td></tr>\n",
       "<tr><td>objective_73fcb_00249</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>detroit_lake_inflow </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1830.84 </td><td style=\"text-align: right;\"> 0.0533339  </td></tr>\n",
       "<tr><td>objective_73fcb_00250</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>virgin_r_at_virtin  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1569.16 </td><td style=\"text-align: right;\"> 0.517923   </td></tr>\n",
       "<tr><td>objective_73fcb_00251</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>dillon_reservoi_b320</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.353</td><td style=\"text-align: right;\"> 0.938906   </td></tr>\n",
       "<tr><td>objective_73fcb_00252</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>pueblo_reservoi_2ce0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         782.727</td><td style=\"text-align: right;\"> 0.94259    </td></tr>\n",
       "<tr><td>objective_73fcb_00253</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>hungry_horse_re_17f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.734</td><td style=\"text-align: right;\"> 1.16546    </td></tr>\n",
       "<tr><td>objective_73fcb_00254</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>stehekin_r_at_s_21f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.315</td><td style=\"text-align: right;\"> 0.929164   </td></tr>\n",
       "<tr><td>objective_73fcb_00255</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>pecos_r_nr_pecos    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        3142.31 </td><td style=\"text-align: right;\">-0.164916   </td></tr>\n",
       "<tr><td>objective_73fcb_00256</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>snake_r_nr_heise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         783.426</td><td style=\"text-align: right;\"> 1.11212    </td></tr>\n",
       "<tr><td>objective_73fcb_00257</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>yampa_r_nr_maybell  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1566.93 </td><td style=\"text-align: right;\"> 0.237615   </td></tr>\n",
       "<tr><td>objective_73fcb_00258</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>colville_r_at_k_6b50</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1566.33 </td><td style=\"text-align: right;\"> 0.116217   </td></tr>\n",
       "<tr><td>objective_73fcb_00259</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>missouri_r_at_toston</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         742.792</td><td style=\"text-align: right;\"> 0.702781   </td></tr>\n",
       "<tr><td>objective_73fcb_00260</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>merced_river_yo_7930</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        3142.07 </td><td style=\"text-align: right;\">-0.00428209 </td></tr>\n",
       "<tr><td>objective_73fcb_00261</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>animas_r_at_durango </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.69 </td><td style=\"text-align: right;\"> 0.78025    </td></tr>\n",
       "<tr><td>objective_73fcb_00262</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>fontenelle_rese_f230</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.495</td><td style=\"text-align: right;\"> 1.2125     </td></tr>\n",
       "<tr><td>objective_73fcb_00263</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>boysen_reservoi_3050</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1462.59 </td><td style=\"text-align: right;\"> 0.540145   </td></tr>\n",
       "<tr><td>objective_73fcb_00264</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>libby_reservoir_b5a0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.367</td><td style=\"text-align: right;\"> 1.24099    </td></tr>\n",
       "<tr><td>objective_73fcb_00265</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>owyhee_r_bl_owy_d480</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1464.32 </td><td style=\"text-align: right;\"> 0.432379   </td></tr>\n",
       "<tr><td>objective_73fcb_00266</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>san_joaquin_riv_4750</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1959.53 </td><td style=\"text-align: right;\"> 0.0985688  </td></tr>\n",
       "<tr><td>objective_73fcb_00267</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>taylor_park_res_ca80</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         744.46 </td><td style=\"text-align: right;\"> 0.85276    </td></tr>\n",
       "<tr><td>objective_73fcb_00268</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>boise_r_nr_boise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         734.668</td><td style=\"text-align: right;\"> 0.718737   </td></tr>\n",
       "<tr><td>objective_73fcb_00269</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>green_r_bl_howa_e560</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1960.76 </td><td style=\"text-align: right;\">-0.0833422  </td></tr>\n",
       "<tr><td>objective_73fcb_00270</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>weber_r_nr_oakley   </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1566.81 </td><td style=\"text-align: right;\"> 0.622191   </td></tr>\n",
       "<tr><td>objective_73fcb_00271</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>detroit_lake_inflow </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1959.17 </td><td style=\"text-align: right;\"> 0.0172826  </td></tr>\n",
       "<tr><td>objective_73fcb_00272</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>virgin_r_at_virtin  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1566.51 </td><td style=\"text-align: right;\"> 0.247237   </td></tr>\n",
       "<tr><td>objective_73fcb_00273</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>dillon_reservoi_b320</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         783.34 </td><td style=\"text-align: right;\"> 1.285      </td></tr>\n",
       "<tr><td>objective_73fcb_00274</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>pueblo_reservoi_2ce0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         745.562</td><td style=\"text-align: right;\"> 1.52783    </td></tr>\n",
       "<tr><td>objective_73fcb_00275</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>hungry_horse_re_17f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         735.139</td><td style=\"text-align: right;\"> 1.11097    </td></tr>\n",
       "<tr><td>objective_73fcb_00276</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>stehekin_r_at_s_21f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         731.905</td><td style=\"text-align: right;\"> 1.11914    </td></tr>\n",
       "<tr><td>objective_73fcb_00277</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>pecos_r_nr_pecos    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         733.566</td><td style=\"text-align: right;\"> 0.755059   </td></tr>\n",
       "<tr><td>objective_73fcb_00278</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>snake_r_nr_heise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         783.103</td><td style=\"text-align: right;\"> 0.830339   </td></tr>\n",
       "<tr><td>objective_73fcb_00279</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>yampa_r_nr_maybell  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         744.66 </td><td style=\"text-align: right;\"> 0.680659   </td></tr>\n",
       "<tr><td>objective_73fcb_00280</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>colville_r_at_k_6b50</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1838.3  </td><td style=\"text-align: right;\">-0.0541156  </td></tr>\n",
       "<tr><td>objective_73fcb_00281</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>missouri_r_at_toston</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.252</td><td style=\"text-align: right;\"> 0.853495   </td></tr>\n",
       "<tr><td>objective_73fcb_00282</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>merced_river_yo_7930</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         733.096</td><td style=\"text-align: right;\"> 0.695804   </td></tr>\n",
       "<tr><td>objective_73fcb_00283</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>animas_r_at_durango </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.256</td><td style=\"text-align: right;\"> 1.1341     </td></tr>\n",
       "<tr><td>objective_73fcb_00284</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>fontenelle_rese_f230</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         783.816</td><td style=\"text-align: right;\"> 0.819024   </td></tr>\n",
       "<tr><td>objective_73fcb_00285</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>boysen_reservoi_3050</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         733.241</td><td style=\"text-align: right;\"> 1.01806    </td></tr>\n",
       "<tr><td>objective_73fcb_00286</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>libby_reservoir_b5a0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.343</td><td style=\"text-align: right;\"> 1.17224    </td></tr>\n",
       "<tr><td>objective_73fcb_00287</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>owyhee_r_bl_owy_d480</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        3135.89 </td><td style=\"text-align: right;\">-0.116821   </td></tr>\n",
       "<tr><td>objective_73fcb_00288</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>san_joaquin_riv_4750</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">        2235.1  </td><td style=\"text-align: right;\">-0.0294864  </td></tr>\n",
       "<tr><td>objective_73fcb_00289</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>taylor_park_res_ca80</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         782.875</td><td style=\"text-align: right;\"> 0.890114   </td></tr>\n",
       "<tr><td>objective_73fcb_00290</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>boise_r_nr_boise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.468</td><td style=\"text-align: right;\"> 0.722351   </td></tr>\n",
       "<tr><td>objective_73fcb_00291</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>green_r_bl_howa_e560</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1959.22 </td><td style=\"text-align: right;\">-0.0780219  </td></tr>\n",
       "<tr><td>objective_73fcb_00292</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>weber_r_nr_oakley   </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1463.84 </td><td style=\"text-align: right;\"> 0.610072   </td></tr>\n",
       "<tr><td>objective_73fcb_00293</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>detroit_lake_inflow </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1961.66 </td><td style=\"text-align: right;\"> 0.0408026  </td></tr>\n",
       "<tr><td>objective_73fcb_00294</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>virgin_r_at_virtin  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1568.57 </td><td style=\"text-align: right;\"> 0.126264   </td></tr>\n",
       "<tr><td>objective_73fcb_00295</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>dillon_reservoi_b320</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         732.593</td><td style=\"text-align: right;\"> 0.946788   </td></tr>\n",
       "<tr><td>objective_73fcb_00296</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>pueblo_reservoi_2ce0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.075</td><td style=\"text-align: right;\"> 1.54162    </td></tr>\n",
       "<tr><td>objective_73fcb_00297</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>hungry_horse_re_17f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.29 </td><td style=\"text-align: right;\"> 1.02333    </td></tr>\n",
       "<tr><td>objective_73fcb_00298</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>stehekin_r_at_s_21f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         783.791</td><td style=\"text-align: right;\"> 1.15435    </td></tr>\n",
       "<tr><td>objective_73fcb_00299</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>pecos_r_nr_pecos    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.008</td><td style=\"text-align: right;\"> 0.75065    </td></tr>\n",
       "<tr><td>objective_73fcb_00300</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>snake_r_nr_heise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         735.607</td><td style=\"text-align: right;\"> 0.777607   </td></tr>\n",
       "<tr><td>objective_73fcb_00301</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>yampa_r_nr_maybell  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1464.95 </td><td style=\"text-align: right;\"> 0.183014   </td></tr>\n",
       "<tr><td>objective_73fcb_00302</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>colville_r_at_k_6b50</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1962.25 </td><td style=\"text-align: right;\"> 0.00274119 </td></tr>\n",
       "<tr><td>objective_73fcb_00303</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>missouri_r_at_toston</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.235</td><td style=\"text-align: right;\"> 0.707316   </td></tr>\n",
       "<tr><td>objective_73fcb_00304</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>merced_river_yo_7930</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         782.85 </td><td style=\"text-align: right;\"> 0.698614   </td></tr>\n",
       "<tr><td>objective_73fcb_00305</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>animas_r_at_durango </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.443</td><td style=\"text-align: right;\"> 1.02674    </td></tr>\n",
       "<tr><td>objective_73fcb_00306</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>fontenelle_rese_f230</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">        3467.45 </td><td style=\"text-align: right;\">-0.390919   </td></tr>\n",
       "<tr><td>objective_73fcb_00307</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>boysen_reservoi_3050</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2012</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        2938.2  </td><td style=\"text-align: right;\">-0.153937   </td></tr>\n",
       "<tr><td>objective_73fcb_00308</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>libby_reservoir_b5a0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.622</td><td style=\"text-align: right;\"> 1.10748    </td></tr>\n",
       "<tr><td>objective_73fcb_00309</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>owyhee_r_bl_owy_d480</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1569.62 </td><td style=\"text-align: right;\"> 0.210035   </td></tr>\n",
       "<tr><td>objective_73fcb_00310</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>san_joaquin_riv_4750</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">        1933.37 </td><td style=\"text-align: right;\"> 0.0505189  </td></tr>\n",
       "<tr><td>objective_73fcb_00311</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>taylor_park_res_ca80</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.568</td><td style=\"text-align: right;\"> 0.818219   </td></tr>\n",
       "<tr><td>objective_73fcb_00312</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>boise_r_nr_boise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1571.16 </td><td style=\"text-align: right;\"> 0.631633   </td></tr>\n",
       "<tr><td>objective_73fcb_00313</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>green_r_bl_howa_e560</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1964.34 </td><td style=\"text-align: right;\">-0.0439824  </td></tr>\n",
       "<tr><td>objective_73fcb_00314</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>weber_r_nr_oakley   </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1568.1  </td><td style=\"text-align: right;\"> 0.639133   </td></tr>\n",
       "<tr><td>objective_73fcb_00315</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>detroit_lake_inflow </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">        2194.4  </td><td style=\"text-align: right;\">-0.073234   </td></tr>\n",
       "<tr><td>objective_73fcb_00316</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>virgin_r_at_virtin  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1570.07 </td><td style=\"text-align: right;\"> 0.276173   </td></tr>\n",
       "<tr><td>objective_73fcb_00317</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>dillon_reservoi_b320</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1567.53 </td><td style=\"text-align: right;\"> 0.518167   </td></tr>\n",
       "<tr><td>objective_73fcb_00318</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>pueblo_reservoi_2ce0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1569.3  </td><td style=\"text-align: right;\"> 0.633013   </td></tr>\n",
       "<tr><td>objective_73fcb_00319</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>hungry_horse_re_17f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.777</td><td style=\"text-align: right;\"> 1.16013    </td></tr>\n",
       "<tr><td>objective_73fcb_00320</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>stehekin_r_at_s_21f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.287</td><td style=\"text-align: right;\"> 1.42849    </td></tr>\n",
       "<tr><td>objective_73fcb_00321</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>pecos_r_nr_pecos    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.883</td><td style=\"text-align: right;\"> 0.702772   </td></tr>\n",
       "<tr><td>objective_73fcb_00322</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>snake_r_nr_heise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.252</td><td style=\"text-align: right;\"> 1.13732    </td></tr>\n",
       "<tr><td>objective_73fcb_00323</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>yampa_r_nr_maybell  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1568.38 </td><td style=\"text-align: right;\"> 0.454005   </td></tr>\n",
       "<tr><td>objective_73fcb_00324</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>colville_r_at_k_6b50</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1484.75 </td><td style=\"text-align: right;\"> 0.325745   </td></tr>\n",
       "<tr><td>objective_73fcb_00325</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>missouri_r_at_toston</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         788.191</td><td style=\"text-align: right;\"> 0.788537   </td></tr>\n",
       "<tr><td>objective_73fcb_00326</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>merced_river_yo_7930</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1567.31 </td><td style=\"text-align: right;\"> 0.426826   </td></tr>\n",
       "<tr><td>objective_73fcb_00327</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>animas_r_at_durango </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         783.95 </td><td style=\"text-align: right;\"> 1.15921    </td></tr>\n",
       "<tr><td>objective_73fcb_00328</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>fontenelle_rese_f230</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.374</td><td style=\"text-align: right;\"> 1.06893    </td></tr>\n",
       "<tr><td>objective_73fcb_00329</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>boysen_reservoi_3050</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.894</td><td style=\"text-align: right;\"> 0.724956   </td></tr>\n",
       "<tr><td>objective_73fcb_00330</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>libby_reservoir_b5a0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.578</td><td style=\"text-align: right;\"> 1.08719    </td></tr>\n",
       "<tr><td>objective_73fcb_00331</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>owyhee_r_bl_owy_d480</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">        2275.63 </td><td style=\"text-align: right;\">-0.0784133  </td></tr>\n",
       "<tr><td>objective_73fcb_00332</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>san_joaquin_riv_4750</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">        2043.24 </td><td style=\"text-align: right;\">-0.018173   </td></tr>\n",
       "<tr><td>objective_73fcb_00333</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>taylor_park_res_ca80</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         731.828</td><td style=\"text-align: right;\"> 0.76605    </td></tr>\n",
       "<tr><td>objective_73fcb_00334</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>boise_r_nr_boise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1570.06 </td><td style=\"text-align: right;\"> 0.275805   </td></tr>\n",
       "<tr><td>objective_73fcb_00335</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>green_r_bl_howa_e560</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1827.41 </td><td style=\"text-align: right;\">-0.0471627  </td></tr>\n",
       "<tr><td>objective_73fcb_00336</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>weber_r_nr_oakley   </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1566.99 </td><td style=\"text-align: right;\"> 0.57344    </td></tr>\n",
       "<tr><td>objective_73fcb_00337</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>detroit_lake_inflow </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        3137.67 </td><td style=\"text-align: right;\">-0.181749   </td></tr>\n",
       "<tr><td>objective_73fcb_00338</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>virgin_r_at_virtin  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1572.21 </td><td style=\"text-align: right;\"> 0.315083   </td></tr>\n",
       "<tr><td>objective_73fcb_00339</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>dillon_reservoi_b320</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1571.51 </td><td style=\"text-align: right;\"> 0.483906   </td></tr>\n",
       "<tr><td>objective_73fcb_00340</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>pueblo_reservoi_2ce0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1484.34 </td><td style=\"text-align: right;\"> 0.129032   </td></tr>\n",
       "<tr><td>objective_73fcb_00341</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>hungry_horse_re_17f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         733.688</td><td style=\"text-align: right;\"> 1.06959    </td></tr>\n",
       "<tr><td>objective_73fcb_00342</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>stehekin_r_at_s_21f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.887</td><td style=\"text-align: right;\"> 1.44941    </td></tr>\n",
       "<tr><td>objective_73fcb_00343</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>pecos_r_nr_pecos    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.126</td><td style=\"text-align: right;\"> 0.663024   </td></tr>\n",
       "<tr><td>objective_73fcb_00344</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>snake_r_nr_heise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         732.002</td><td style=\"text-align: right;\"> 1.10431    </td></tr>\n",
       "<tr><td>objective_73fcb_00345</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>yampa_r_nr_maybell  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        3140.93 </td><td style=\"text-align: right;\">-0.220631   </td></tr>\n",
       "<tr><td>objective_73fcb_00346</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>colville_r_at_k_6b50</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1573.39 </td><td style=\"text-align: right;\"> 0.178988   </td></tr>\n",
       "<tr><td>objective_73fcb_00347</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>missouri_r_at_toston</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.993</td><td style=\"text-align: right;\"> 0.727717   </td></tr>\n",
       "<tr><td>objective_73fcb_00348</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>merced_river_yo_7930</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1568.86 </td><td style=\"text-align: right;\"> 0.243104   </td></tr>\n",
       "<tr><td>objective_73fcb_00349</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>animas_r_at_durango </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.423</td><td style=\"text-align: right;\"> 1.13891    </td></tr>\n",
       "<tr><td>objective_73fcb_00350</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>fontenelle_rese_f230</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.181</td><td style=\"text-align: right;\"> 0.793263   </td></tr>\n",
       "<tr><td>objective_73fcb_00351</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>boysen_reservoi_3050</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2014</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.529</td><td style=\"text-align: right;\"> 0.671334   </td></tr>\n",
       "<tr><td>objective_73fcb_00352</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>libby_reservoir_b5a0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         732.891</td><td style=\"text-align: right;\"> 1.35526    </td></tr>\n",
       "<tr><td>objective_73fcb_00353</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>owyhee_r_bl_owy_d480</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1830.81 </td><td style=\"text-align: right;\">-0.321614   </td></tr>\n",
       "<tr><td>objective_73fcb_00354</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>san_joaquin_riv_4750</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1844.54 </td><td style=\"text-align: right;\">-0.272973   </td></tr>\n",
       "<tr><td>objective_73fcb_00355</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>taylor_park_res_ca80</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         731.996</td><td style=\"text-align: right;\"> 0.778769   </td></tr>\n",
       "<tr><td>objective_73fcb_00356</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>boise_r_nr_boise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.586</td><td style=\"text-align: right;\"> 0.93029    </td></tr>\n",
       "<tr><td>objective_73fcb_00357</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>green_r_bl_howa_e560</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1964.05 </td><td style=\"text-align: right;\">-0.0508287  </td></tr>\n",
       "<tr><td>objective_73fcb_00358</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>weber_r_nr_oakley   </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.081</td><td style=\"text-align: right;\"> 1.34199    </td></tr>\n",
       "<tr><td>objective_73fcb_00359</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>detroit_lake_inflow </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1963.36 </td><td style=\"text-align: right;\">-0.223466   </td></tr>\n",
       "<tr><td>objective_73fcb_00360</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>virgin_r_at_virtin  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1833.84 </td><td style=\"text-align: right;\">-0.00795134 </td></tr>\n",
       "<tr><td>objective_73fcb_00361</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>dillon_reservoi_b320</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1465.84 </td><td style=\"text-align: right;\"> 0.491826   </td></tr>\n",
       "<tr><td>objective_73fcb_00362</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>pueblo_reservoi_2ce0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.068</td><td style=\"text-align: right;\"> 0.720571   </td></tr>\n",
       "<tr><td>objective_73fcb_00363</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>hungry_horse_re_17f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.956</td><td style=\"text-align: right;\"> 1.16661    </td></tr>\n",
       "<tr><td>objective_73fcb_00364</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>stehekin_r_at_s_21f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.074</td><td style=\"text-align: right;\"> 1.8107     </td></tr>\n",
       "<tr><td>objective_73fcb_00365</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>pecos_r_nr_pecos    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1568.78 </td><td style=\"text-align: right;\"> 0.436907   </td></tr>\n",
       "<tr><td>objective_73fcb_00366</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>snake_r_nr_heise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.788</td><td style=\"text-align: right;\"> 1.93922    </td></tr>\n",
       "<tr><td>objective_73fcb_00367</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>yampa_r_nr_maybell  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1466.62 </td><td style=\"text-align: right;\"> 0.627444   </td></tr>\n",
       "<tr><td>objective_73fcb_00368</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>colville_r_at_k_6b50</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1476.19 </td><td style=\"text-align: right;\"> 0.529287   </td></tr>\n",
       "<tr><td>objective_73fcb_00369</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>missouri_r_at_toston</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.463</td><td style=\"text-align: right;\"> 0.985774   </td></tr>\n",
       "<tr><td>objective_73fcb_00370</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>merced_river_yo_7930</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1961.67 </td><td style=\"text-align: right;\">-0.231065   </td></tr>\n",
       "<tr><td>objective_73fcb_00371</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>animas_r_at_durango </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         783.656</td><td style=\"text-align: right;\"> 1.08117    </td></tr>\n",
       "<tr><td>objective_73fcb_00372</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>fontenelle_rese_f230</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         734.353</td><td style=\"text-align: right;\"> 1.32543    </td></tr>\n",
       "<tr><td>objective_73fcb_00373</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>boysen_reservoi_3050</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.688</td><td style=\"text-align: right;\"> 0.755127   </td></tr>\n",
       "<tr><td>objective_73fcb_00374</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>libby_reservoir_b5a0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.041</td><td style=\"text-align: right;\"> 1.37521    </td></tr>\n",
       "<tr><td>objective_73fcb_00375</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>owyhee_r_bl_owy_d480</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1965.32 </td><td style=\"text-align: right;\">-0.304014   </td></tr>\n",
       "<tr><td>objective_73fcb_00376</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>san_joaquin_riv_4750</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">        2198.64 </td><td style=\"text-align: right;\">-0.255756   </td></tr>\n",
       "<tr><td>objective_73fcb_00377</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>taylor_park_res_ca80</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1570.48 </td><td style=\"text-align: right;\"> 0.348348   </td></tr>\n",
       "<tr><td>objective_73fcb_00378</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>boise_r_nr_boise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.463</td><td style=\"text-align: right;\"> 0.677174   </td></tr>\n",
       "<tr><td>objective_73fcb_00379</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>green_r_bl_howa_e560</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1961.01 </td><td style=\"text-align: right;\">-0.0544131  </td></tr>\n",
       "<tr><td>objective_73fcb_00380</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>weber_r_nr_oakley   </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         733.429</td><td style=\"text-align: right;\"> 0.66705    </td></tr>\n",
       "<tr><td>objective_73fcb_00381</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>detroit_lake_inflow </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1961.9  </td><td style=\"text-align: right;\">-0.174877   </td></tr>\n",
       "<tr><td>objective_73fcb_00382</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>virgin_r_at_virtin  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1964.01 </td><td style=\"text-align: right;\"> 0.0302137  </td></tr>\n",
       "<tr><td>objective_73fcb_00383</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>dillon_reservoi_b320</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1571.02 </td><td style=\"text-align: right;\"> 0.489038   </td></tr>\n",
       "<tr><td>objective_73fcb_00384</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>pueblo_reservoi_2ce0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1465.52 </td><td style=\"text-align: right;\"> 0.333134   </td></tr>\n",
       "<tr><td>objective_73fcb_00385</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>hungry_horse_re_17f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         737.613</td><td style=\"text-align: right;\"> 1.14698    </td></tr>\n",
       "<tr><td>objective_73fcb_00386</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>stehekin_r_at_s_21f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         787.012</td><td style=\"text-align: right;\"> 1.60972    </td></tr>\n",
       "<tr><td>objective_73fcb_00387</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>pecos_r_nr_pecos    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1468.86 </td><td style=\"text-align: right;\"> 0.420252   </td></tr>\n",
       "<tr><td>objective_73fcb_00388</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>snake_r_nr_heise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         783.284</td><td style=\"text-align: right;\"> 1.40887    </td></tr>\n",
       "<tr><td>objective_73fcb_00389</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>yampa_r_nr_maybell  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1475.68 </td><td style=\"text-align: right;\"> 0.47339    </td></tr>\n",
       "<tr><td>objective_73fcb_00390</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>colville_r_at_k_6b50</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1570.08 </td><td style=\"text-align: right;\"> 0.519703   </td></tr>\n",
       "<tr><td>objective_73fcb_00391</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>missouri_r_at_toston</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.338</td><td style=\"text-align: right;\"> 0.921418   </td></tr>\n",
       "<tr><td>objective_73fcb_00392</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>merced_river_yo_7930</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1966.71 </td><td style=\"text-align: right;\">-0.327298   </td></tr>\n",
       "<tr><td>objective_73fcb_00393</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>animas_r_at_durango </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.634</td><td style=\"text-align: right;\"> 1.0621     </td></tr>\n",
       "<tr><td>objective_73fcb_00394</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>fontenelle_rese_f230</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         733.333</td><td style=\"text-align: right;\"> 1.03587    </td></tr>\n",
       "<tr><td>objective_73fcb_00395</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>boysen_reservoi_3050</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2016</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         732.089</td><td style=\"text-align: right;\"> 0.7017     </td></tr>\n",
       "<tr><td>objective_73fcb_00396</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>libby_reservoir_b5a0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.06 </td><td style=\"text-align: right;\"> 1.12814    </td></tr>\n",
       "<tr><td>objective_73fcb_00397</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>owyhee_r_bl_owy_d480</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1960.79 </td><td style=\"text-align: right;\">-0.00548887 </td></tr>\n",
       "<tr><td>objective_73fcb_00398</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>san_joaquin_riv_4750</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1962.65 </td><td style=\"text-align: right;\">-0.0857431  </td></tr>\n",
       "<tr><td>objective_73fcb_00399</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>taylor_park_res_ca80</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.622</td><td style=\"text-align: right;\"> 1.33899    </td></tr>\n",
       "<tr><td>objective_73fcb_00400</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>boise_r_nr_boise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.812</td><td style=\"text-align: right;\"> 1.15914    </td></tr>\n",
       "<tr><td>objective_73fcb_00401</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>green_r_bl_howa_e560</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1835.36 </td><td style=\"text-align: right;\"> 0.019563   </td></tr>\n",
       "<tr><td>objective_73fcb_00402</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>weber_r_nr_oakley   </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         783.642</td><td style=\"text-align: right;\"> 1.56952    </td></tr>\n",
       "<tr><td>objective_73fcb_00403</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>detroit_lake_inflow </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1829.34 </td><td style=\"text-align: right;\">-0.227034   </td></tr>\n",
       "<tr><td>objective_73fcb_00404</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>virgin_r_at_virtin  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1462.26 </td><td style=\"text-align: right;\"> 0.161442   </td></tr>\n",
       "<tr><td>objective_73fcb_00405</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>dillon_reservoi_b320</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         738.429</td><td style=\"text-align: right;\"> 1.00039    </td></tr>\n",
       "<tr><td>objective_73fcb_00406</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>pueblo_reservoi_2ce0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.695</td><td style=\"text-align: right;\"> 1.40754    </td></tr>\n",
       "<tr><td>objective_73fcb_00407</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>hungry_horse_re_17f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.282</td><td style=\"text-align: right;\"> 1.22141    </td></tr>\n",
       "<tr><td>objective_73fcb_00408</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>stehekin_r_at_s_21f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.459</td><td style=\"text-align: right;\"> 0.758559   </td></tr>\n",
       "<tr><td>objective_73fcb_00409</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>pecos_r_nr_pecos    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.774</td><td style=\"text-align: right;\"> 0.74205    </td></tr>\n",
       "<tr><td>objective_73fcb_00410</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>snake_r_nr_heise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.009</td><td style=\"text-align: right;\"> 1.75314    </td></tr>\n",
       "<tr><td>objective_73fcb_00411</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>yampa_r_nr_maybell  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         738.649</td><td style=\"text-align: right;\"> 1.22932    </td></tr>\n",
       "<tr><td>objective_73fcb_00412</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>colville_r_at_k_6b50</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1570.43 </td><td style=\"text-align: right;\"> 0.364058   </td></tr>\n",
       "<tr><td>objective_73fcb_00413</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>missouri_r_at_toston</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.035</td><td style=\"text-align: right;\"> 1.04669    </td></tr>\n",
       "<tr><td>objective_73fcb_00414</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>merced_river_yo_7930</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1962.46 </td><td style=\"text-align: right;\"> 0.0871026  </td></tr>\n",
       "<tr><td>objective_73fcb_00415</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>animas_r_at_durango </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.029</td><td style=\"text-align: right;\"> 1.28518    </td></tr>\n",
       "<tr><td>objective_73fcb_00416</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>fontenelle_rese_f230</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.199</td><td style=\"text-align: right;\"> 1.37384    </td></tr>\n",
       "<tr><td>objective_73fcb_00417</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>boysen_reservoi_3050</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.683</td><td style=\"text-align: right;\"> 0.984308   </td></tr>\n",
       "<tr><td>objective_73fcb_00418</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>libby_reservoir_b5a0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.671</td><td style=\"text-align: right;\"> 0.883433   </td></tr>\n",
       "<tr><td>objective_73fcb_00419</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>owyhee_r_bl_owy_d480</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        3656.13 </td><td style=\"text-align: right;\">-0.515618   </td></tr>\n",
       "<tr><td>objective_73fcb_00420</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>san_joaquin_riv_4750</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">        2159.59 </td><td style=\"text-align: right;\">-0.303218   </td></tr>\n",
       "<tr><td>objective_73fcb_00421</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>taylor_park_res_ca80</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         738.206</td><td style=\"text-align: right;\"> 1.39316    </td></tr>\n",
       "<tr><td>objective_73fcb_00422</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>boise_r_nr_boise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.473</td><td style=\"text-align: right;\"> 1.07057    </td></tr>\n",
       "<tr><td>objective_73fcb_00423</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>green_r_bl_howa_e560</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1966.2  </td><td style=\"text-align: right;\"> 0.0183337  </td></tr>\n",
       "<tr><td>objective_73fcb_00424</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>weber_r_nr_oakley   </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         732.369</td><td style=\"text-align: right;\"> 0.959013   </td></tr>\n",
       "<tr><td>objective_73fcb_00425</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>detroit_lake_inflow </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1964.68 </td><td style=\"text-align: right;\">-0.216288   </td></tr>\n",
       "<tr><td>objective_73fcb_00426</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>virgin_r_at_virtin  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1572.91 </td><td style=\"text-align: right;\"> 0.138094   </td></tr>\n",
       "<tr><td>objective_73fcb_00427</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>dillon_reservoi_b320</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.607</td><td style=\"text-align: right;\"> 0.954351   </td></tr>\n",
       "<tr><td>objective_73fcb_00428</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>pueblo_reservoi_2ce0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.764</td><td style=\"text-align: right;\"> 1.48072    </td></tr>\n",
       "<tr><td>objective_73fcb_00429</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>hungry_horse_re_17f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         737.471</td><td style=\"text-align: right;\"> 0.962458   </td></tr>\n",
       "<tr><td>objective_73fcb_00430</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>stehekin_r_at_s_21f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.411</td><td style=\"text-align: right;\"> 0.706795   </td></tr>\n",
       "<tr><td>objective_73fcb_00431</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>pecos_r_nr_pecos    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1572.59 </td><td style=\"text-align: right;\"> 0.180386   </td></tr>\n",
       "<tr><td>objective_73fcb_00432</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>snake_r_nr_heise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         732.309</td><td style=\"text-align: right;\"> 1.59656    </td></tr>\n",
       "<tr><td>objective_73fcb_00433</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>yampa_r_nr_maybell  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.138</td><td style=\"text-align: right;\"> 1.10475    </td></tr>\n",
       "<tr><td>objective_73fcb_00434</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>colville_r_at_k_6b50</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1476.18 </td><td style=\"text-align: right;\"> 0.332475   </td></tr>\n",
       "<tr><td>objective_73fcb_00435</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>missouri_r_at_toston</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        3139.83 </td><td style=\"text-align: right;\">-0.151386   </td></tr>\n",
       "<tr><td>objective_73fcb_00436</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>merced_river_yo_7930</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">        3885.77 </td><td style=\"text-align: right;\">-0.573167   </td></tr>\n",
       "<tr><td>objective_73fcb_00437</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>animas_r_at_durango </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.92 </td><td style=\"text-align: right;\"> 1.11877    </td></tr>\n",
       "<tr><td>objective_73fcb_00438</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>fontenelle_rese_f230</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         733.427</td><td style=\"text-align: right;\"> 1.3809     </td></tr>\n",
       "<tr><td>objective_73fcb_00439</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>boysen_reservoi_3050</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2018</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.404</td><td style=\"text-align: right;\"> 0.874518   </td></tr>\n",
       "<tr><td>objective_73fcb_00440</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>libby_reservoir_b5a0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         732.799</td><td style=\"text-align: right;\"> 0.812509   </td></tr>\n",
       "<tr><td>objective_73fcb_00441</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>owyhee_r_bl_owy_d480</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1961.52 </td><td style=\"text-align: right;\"> 0.0822047  </td></tr>\n",
       "<tr><td>objective_73fcb_00442</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>san_joaquin_riv_4750</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1574.51 </td><td style=\"text-align: right;\"> 0.168148   </td></tr>\n",
       "<tr><td>objective_73fcb_00443</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>taylor_park_res_ca80</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.641</td><td style=\"text-align: right;\"> 1.53914    </td></tr>\n",
       "<tr><td>objective_73fcb_00444</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>boise_r_nr_boise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         783.813</td><td style=\"text-align: right;\"> 0.950408   </td></tr>\n",
       "<tr><td>objective_73fcb_00445</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>green_r_bl_howa_e560</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1831.33 </td><td style=\"text-align: right;\"> 0.0693792  </td></tr>\n",
       "<tr><td>objective_73fcb_00446</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>weber_r_nr_oakley   </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.073</td><td style=\"text-align: right;\"> 1.50657    </td></tr>\n",
       "<tr><td>objective_73fcb_00447</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>detroit_lake_inflow </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1962.36 </td><td style=\"text-align: right;\">-0.107571   </td></tr>\n",
       "<tr><td>objective_73fcb_00448</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>virgin_r_at_virtin  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1464.88 </td><td style=\"text-align: right;\"> 0.187814   </td></tr>\n",
       "<tr><td>objective_73fcb_00449</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>dillon_reservoi_b320</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         736.343</td><td style=\"text-align: right;\"> 1.43585    </td></tr>\n",
       "<tr><td>objective_73fcb_00450</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>pueblo_reservoi_2ce0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.651</td><td style=\"text-align: right;\"> 1.04126    </td></tr>\n",
       "<tr><td>objective_73fcb_00451</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>hungry_horse_re_17f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.306</td><td style=\"text-align: right;\"> 0.759441   </td></tr>\n",
       "<tr><td>objective_73fcb_00452</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>stehekin_r_at_s_21f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1571.76 </td><td style=\"text-align: right;\"> 0.525282   </td></tr>\n",
       "<tr><td>objective_73fcb_00453</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>pecos_r_nr_pecos    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1465.12 </td><td style=\"text-align: right;\"> 0.142683   </td></tr>\n",
       "<tr><td>objective_73fcb_00454</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>snake_r_nr_heise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         737.24 </td><td style=\"text-align: right;\"> 1.21342    </td></tr>\n",
       "<tr><td>objective_73fcb_00455</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>yampa_r_nr_maybell  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.954</td><td style=\"text-align: right;\"> 1.29793    </td></tr>\n",
       "<tr><td>objective_73fcb_00456</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>colville_r_at_k_6b50</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1570.55 </td><td style=\"text-align: right;\"> 0.334426   </td></tr>\n",
       "<tr><td>objective_73fcb_00457</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>missouri_r_at_toston</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1568.91 </td><td style=\"text-align: right;\"> 0.406398   </td></tr>\n",
       "<tr><td>objective_73fcb_00458</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>merced_river_yo_7930</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1568.77 </td><td style=\"text-align: right;\"> 0.4118     </td></tr>\n",
       "<tr><td>objective_73fcb_00459</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>animas_r_at_durango </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1464.56 </td><td style=\"text-align: right;\"> 0.434721   </td></tr>\n",
       "<tr><td>objective_73fcb_00460</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>fontenelle_rese_f230</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         735.421</td><td style=\"text-align: right;\"> 1.12767    </td></tr>\n",
       "<tr><td>objective_73fcb_00461</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>boysen_reservoi_3050</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         737.048</td><td style=\"text-align: right;\"> 0.743031   </td></tr>\n",
       "<tr><td>objective_73fcb_00462</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>libby_reservoir_b5a0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.835</td><td style=\"text-align: right;\"> 0.712372   </td></tr>\n",
       "<tr><td>objective_73fcb_00463</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>owyhee_r_bl_owy_d480</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">        3003.16 </td><td style=\"text-align: right;\">-0.28545    </td></tr>\n",
       "<tr><td>objective_73fcb_00464</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>san_joaquin_riv_4750</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1966.07 </td><td style=\"text-align: right;\"> 0.0134613  </td></tr>\n",
       "<tr><td>objective_73fcb_00465</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>taylor_park_res_ca80</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.036</td><td style=\"text-align: right;\"> 1.51749    </td></tr>\n",
       "<tr><td>objective_73fcb_00466</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>boise_r_nr_boise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         732.219</td><td style=\"text-align: right;\"> 0.959721   </td></tr>\n",
       "<tr><td>objective_73fcb_00467</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>green_r_bl_howa_e560</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1839.53 </td><td style=\"text-align: right;\"> 0.0503056  </td></tr>\n",
       "<tr><td>objective_73fcb_00468</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>weber_r_nr_oakley   </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         741.91 </td><td style=\"text-align: right;\"> 1.35517    </td></tr>\n",
       "<tr><td>objective_73fcb_00469</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>detroit_lake_inflow </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1971.37 </td><td style=\"text-align: right;\">-0.0504881  </td></tr>\n",
       "<tr><td>objective_73fcb_00470</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>virgin_r_at_virtin  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1572.34 </td><td style=\"text-align: right;\"> 0.19698    </td></tr>\n",
       "<tr><td>objective_73fcb_00471</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>dillon_reservoi_b320</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.25 </td><td style=\"text-align: right;\"> 0.767228   </td></tr>\n",
       "<tr><td>objective_73fcb_00472</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>pueblo_reservoi_2ce0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         784.372</td><td style=\"text-align: right;\"> 0.849112   </td></tr>\n",
       "<tr><td>objective_73fcb_00473</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>hungry_horse_re_17f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         788.666</td><td style=\"text-align: right;\"> 0.672249   </td></tr>\n",
       "<tr><td>objective_73fcb_00474</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>stehekin_r_at_s_21f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1573.5  </td><td style=\"text-align: right;\"> 0.540195   </td></tr>\n",
       "<tr><td>objective_73fcb_00475</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>pecos_r_nr_pecos    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">        2694.81 </td><td style=\"text-align: right;\">-0.141125   </td></tr>\n",
       "<tr><td>objective_73fcb_00476</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>snake_r_nr_heise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         733.484</td><td style=\"text-align: right;\"> 1.05237    </td></tr>\n",
       "<tr><td>objective_73fcb_00477</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>yampa_r_nr_maybell  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         740.789</td><td style=\"text-align: right;\"> 1.27526    </td></tr>\n",
       "<tr><td>objective_73fcb_00478</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>colville_r_at_k_6b50</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1587.65 </td><td style=\"text-align: right;\"> 0.423445   </td></tr>\n",
       "<tr><td>objective_73fcb_00479</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>missouri_r_at_toston</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1584.3  </td><td style=\"text-align: right;\"> 0.125923   </td></tr>\n",
       "<tr><td>objective_73fcb_00480</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>merced_river_yo_7930</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">        3430.38 </td><td style=\"text-align: right;\">-0.374113   </td></tr>\n",
       "<tr><td>objective_73fcb_00481</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>animas_r_at_durango </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">        2135.61 </td><td style=\"text-align: right;\">-0.175747   </td></tr>\n",
       "<tr><td>objective_73fcb_00482</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>fontenelle_rese_f230</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         748.557</td><td style=\"text-align: right;\"> 1.13104    </td></tr>\n",
       "<tr><td>objective_73fcb_00483</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>boysen_reservoi_3050</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2020</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         795.285</td><td style=\"text-align: right;\"> 0.887936   </td></tr>\n",
       "<tr><td>objective_73fcb_00484</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>libby_reservoir_b5a0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         748.112</td><td style=\"text-align: right;\"> 1.19467    </td></tr>\n",
       "<tr><td>objective_73fcb_00485</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>owyhee_r_bl_owy_d480</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1900.91 </td><td style=\"text-align: right;\">-0.0129342  </td></tr>\n",
       "<tr><td>objective_73fcb_00486</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>san_joaquin_riv_4750</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1581.84 </td><td style=\"text-align: right;\"> 0.269131   </td></tr>\n",
       "<tr><td>objective_73fcb_00487</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>taylor_park_res_ca80</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         799.801</td><td style=\"text-align: right;\"> 1.88629    </td></tr>\n",
       "<tr><td>objective_73fcb_00488</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>boise_r_nr_boise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         743.623</td><td style=\"text-align: right;\"> 1.02032    </td></tr>\n",
       "<tr><td>objective_73fcb_00489</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>green_r_bl_howa_e560</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1580.07 </td><td style=\"text-align: right;\"> 0.138448   </td></tr>\n",
       "<tr><td>objective_73fcb_00490</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>weber_r_nr_oakley   </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         733.956</td><td style=\"text-align: right;\"> 1.57537    </td></tr>\n",
       "<tr><td>objective_73fcb_00491</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>detroit_lake_inflow </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1963.46 </td><td style=\"text-align: right;\">-0.140909   </td></tr>\n",
       "<tr><td>objective_73fcb_00492</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>virgin_r_at_virtin  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1570.77 </td><td style=\"text-align: right;\"> 0.203558   </td></tr>\n",
       "<tr><td>objective_73fcb_00493</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>dillon_reservoi_b320</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         785.965</td><td style=\"text-align: right;\"> 1.7392     </td></tr>\n",
       "<tr><td>objective_73fcb_00494</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>pueblo_reservoi_2ce0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         787.655</td><td style=\"text-align: right;\"> 0.923019   </td></tr>\n",
       "<tr><td>objective_73fcb_00495</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>hungry_horse_re_17f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         734.289</td><td style=\"text-align: right;\"> 0.75047    </td></tr>\n",
       "<tr><td>objective_73fcb_00496</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>stehekin_r_at_s_21f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         733.073</td><td style=\"text-align: right;\"> 1.27902    </td></tr>\n",
       "<tr><td>objective_73fcb_00497</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>pecos_r_nr_pecos    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1570.85 </td><td style=\"text-align: right;\"> 0.10779    </td></tr>\n",
       "<tr><td>objective_73fcb_00498</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>snake_r_nr_heise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         730.67 </td><td style=\"text-align: right;\"> 1.20743    </td></tr>\n",
       "<tr><td>objective_73fcb_00499</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>yampa_r_nr_maybell  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.382</td><td style=\"text-align: right;\"> 1.76762    </td></tr>\n",
       "<tr><td>objective_73fcb_00500</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>colville_r_at_k_6b50</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1569.85 </td><td style=\"text-align: right;\"> 0.344088   </td></tr>\n",
       "<tr><td>objective_73fcb_00501</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>missouri_r_at_toston</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1469.6  </td><td style=\"text-align: right;\"> 0.53593    </td></tr>\n",
       "<tr><td>objective_73fcb_00502</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>merced_river_yo_7930</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1465.89 </td><td style=\"text-align: right;\"> 0.279945   </td></tr>\n",
       "<tr><td>objective_73fcb_00503</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>animas_r_at_durango </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1575.14 </td><td style=\"text-align: right;\"> 0.296914   </td></tr>\n",
       "<tr><td>objective_73fcb_00504</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>fontenelle_rese_f230</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.604</td><td style=\"text-align: right;\"> 1.1722     </td></tr>\n",
       "<tr><td>objective_73fcb_00505</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>boysen_reservoi_3050</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.1 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         733.734</td><td style=\"text-align: right;\"> 0.876908   </td></tr>\n",
       "<tr><td>objective_73fcb_00506</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>libby_reservoir_b5a0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         729.677</td><td style=\"text-align: right;\"> 1.08468    </td></tr>\n",
       "<tr><td>objective_73fcb_00507</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>owyhee_r_bl_owy_d480</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1963.84 </td><td style=\"text-align: right;\">-0.211732   </td></tr>\n",
       "<tr><td>objective_73fcb_00508</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>san_joaquin_riv_4750</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1963.1  </td><td style=\"text-align: right;\"> 0.0265265  </td></tr>\n",
       "<tr><td>objective_73fcb_00509</td><td>TERMINATED</td><td>136.156.133.98:951956</td><td>taylor_park_res_ca80</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">        2420.53 </td><td style=\"text-align: right;\"> 0.00267369 </td></tr>\n",
       "<tr><td>objective_73fcb_00510</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>boise_r_nr_boise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         787.584</td><td style=\"text-align: right;\"> 0.938098   </td></tr>\n",
       "<tr><td>objective_73fcb_00511</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>green_r_bl_howa_e560</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1467.63 </td><td style=\"text-align: right;\"> 0.14529    </td></tr>\n",
       "<tr><td>objective_73fcb_00512</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>weber_r_nr_oakley   </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         786.1  </td><td style=\"text-align: right;\"> 1.32239    </td></tr>\n",
       "<tr><td>objective_73fcb_00513</td><td>TERMINATED</td><td>136.156.133.98:951986</td><td>detroit_lake_inflow </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        2573.57 </td><td style=\"text-align: right;\">-0.274982   </td></tr>\n",
       "<tr><td>objective_73fcb_00514</td><td>TERMINATED</td><td>136.156.133.98:951846</td><td>virgin_r_at_virtin  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1459.48 </td><td style=\"text-align: right;\"> 0.214965   </td></tr>\n",
       "<tr><td>objective_73fcb_00515</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>dillon_reservoi_b320</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         737.764</td><td style=\"text-align: right;\"> 1.3814     </td></tr>\n",
       "<tr><td>objective_73fcb_00516</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>pueblo_reservoi_2ce0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         732.345</td><td style=\"text-align: right;\"> 0.903878   </td></tr>\n",
       "<tr><td>objective_73fcb_00517</td><td>TERMINATED</td><td>136.156.133.98:951915</td><td>hungry_horse_re_17f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        2319.12 </td><td style=\"text-align: right;\">-0.157572   </td></tr>\n",
       "<tr><td>objective_73fcb_00518</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>stehekin_r_at_s_21f0</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         787.279</td><td style=\"text-align: right;\"> 0.988272   </td></tr>\n",
       "<tr><td>objective_73fcb_00519</td><td>TERMINATED</td><td>136.156.133.98:951779</td><td>pecos_r_nr_pecos    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">        2142.23 </td><td style=\"text-align: right;\">-0.198643   </td></tr>\n",
       "<tr><td>objective_73fcb_00520</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>snake_r_nr_heise    </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         787.079</td><td style=\"text-align: right;\"> 1.15512    </td></tr>\n",
       "<tr><td>objective_73fcb_00521</td><td>TERMINATED</td><td>136.156.133.98:952007</td><td>yampa_r_nr_maybell  </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         718.802</td><td style=\"text-align: right;\"> 1.71522    </td></tr>\n",
       "<tr><td>objective_73fcb_00522</td><td>TERMINATED</td><td>136.156.133.98:951948</td><td>colville_r_at_k_6b50</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1239.89 </td><td style=\"text-align: right;\"> 0.36644    </td></tr>\n",
       "<tr><td>objective_73fcb_00523</td><td>TERMINATED</td><td>136.156.133.98:951995</td><td>missouri_r_at_toston</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        1189.44 </td><td style=\"text-align: right;\"> 0.406508   </td></tr>\n",
       "<tr><td>objective_73fcb_00524</td><td>TERMINATED</td><td>136.156.133.98:951980</td><td>merced_river_yo_7930</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">        1696.92 </td><td style=\"text-align: right;\">-0.367187   </td></tr>\n",
       "<tr><td>objective_73fcb_00525</td><td>TERMINATED</td><td>136.156.133.98:951978</td><td>animas_r_at_durango </td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1907.41 </td><td style=\"text-align: right;\">-0.403179   </td></tr>\n",
       "<tr><td>objective_73fcb_00526</td><td>TERMINATED</td><td>136.156.133.98:952002</td><td>fontenelle_rese_f230</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         687.112</td><td style=\"text-align: right;\"> 1.071      </td></tr>\n",
       "<tr><td>objective_73fcb_00527</td><td>TERMINATED</td><td>136.156.133.98:951996</td><td>boysen_reservoi_3050</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         653.715</td><td style=\"text-align: right;\"> 0.797373   </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\u001b[32m [repeated 12x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951956)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951956)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951986)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951986)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951846)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951846)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951915)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951779)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951779)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952007)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952007)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951948)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951948)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951995)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951980)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951980)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951978)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951978)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=952002)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=952002)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(objective pid=951996)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=951996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 19:18:14,588\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 19:18:21,827\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 19:18:58,011\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 19:18:59,724\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 19:19:04,562\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 19:19:04,870\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 19:30:28,913\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 19:32:02,401\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 19:32:03,065\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 19:32:10,690\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 19:32:11,502\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 19:32:12,699\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 19:36:26,235\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 19:36:26,949\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 19:38:27,920\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 19:42:53,142\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 19:45:06,789\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 19:45:07,421\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 19:45:16,866\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 19:45:17,548\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 19:47:51,374\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 19:48:38,255\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 19:51:52,829\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 19:58:10,095\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 19:58:23,113\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 19:58:25,576\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 20:00:48,948\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 20:01:01,452\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 20:04:39,552\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 20:05:01,509\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 20:06:56,230\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 20:11:30,118\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 20:11:30,320\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 20:11:33,293\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 20:13:33,219\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 20:14:03,753\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 20:17:46,828\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 20:24:36,961\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 20:24:37,040\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 20:25:28,814\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 20:27:10,907\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 20:30:50,962\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 20:31:15,271\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 20:31:18,089\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 20:37:25,107\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 20:37:43,362\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 20:37:45,994\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 20:43:55,116\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 20:44:25,353\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 20:49:05,832\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 20:49:58,125\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 20:50:31,966\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 20:50:52,772\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 20:53:21,880\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 20:55:37,458\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 20:56:58,924\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 20:57:24,045\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 20:57:26,773\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 20:59:32,919\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n",
      "2024-06-04 21:03:38,765\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 21:03:59,251\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 21:06:27,987\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 21:07:47,713\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 21:09:39,675\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 21:10:29,055\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 21:10:41,812\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 21:13:34,992\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 21:14:25,425\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 21:17:06,646\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 21:21:53,555\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 21:23:49,424\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 21:25:39,347\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 21:25:51,427\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 21:30:09,122\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 21:30:13,402\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 21:32:11,794\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 21:32:38,954\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 21:34:06,976\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 21:36:21,801\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 21:36:40,460\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 21:43:20,249\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 21:45:44,501\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 21:49:11,870\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 21:50:02,478\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 21:50:25,102\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 21:51:44,261\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 21:56:19,688\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 21:56:27,113\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 21:58:33,021\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 22:02:14,118\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 22:02:51,521\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 22:09:33,976\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 22:10:44,519\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 22:11:21,904\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 22:15:57,204\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 22:17:50,035\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 22:18:26,536\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 22:19:36,466\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 22:21:12,751\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 22:22:47,473\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 22:29:04,228\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 22:33:12,499\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '6269ca81')}\n",
      "2024-06-04 22:33:30,875\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 22:34:45,293\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 22:35:10,229\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 22:35:47,015\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 22:42:01,222\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 22:42:09,443\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 22:44:37,348\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 22:45:48,738\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 22:47:24,498\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 22:49:01,017\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 22:50:24,147\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 22:52:20,137\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 22:59:38,801\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 23:01:48,506\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 23:01:59,196\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 23:03:26,415\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 23:07:17,471\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 23:08:21,318\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 23:10:25,017\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 23:11:54,590\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 23:15:05,611\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 23:18:30,810\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 23:21:45,756\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 23:22:45,298\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-04 23:33:17,652\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 23:34:20,120\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 23:34:50,613\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-04 23:36:19,192\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-04 23:36:45,635\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 23:42:12,624\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-04 23:44:43,382\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-04 23:46:17,672\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-04 23:47:52,014\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-04 23:53:37,007\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-04 23:54:31,024\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-04 23:57:50,410\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 00:00:57,503\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 00:04:03,286\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-05 00:05:01,222\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 00:06:44,297\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 00:07:16,963\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-05 00:07:32,720\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 00:08:31,346\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd31ea24e')}\n",
      "2024-06-05 00:09:21,278\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 00:12:17,909\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 00:14:03,411\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 00:20:23,663\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 00:24:27,712\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 00:25:14,981\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 00:25:18,453\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 00:29:33,539\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 00:30:15,907\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 00:30:37,506\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 00:33:29,802\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 00:33:42,204\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 00:35:26,366\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 00:37:04,269\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 00:41:03,903\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 00:43:22,468\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 00:43:43,352\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 00:46:45,695\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 00:46:47,611\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 00:55:22,425\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 00:56:03,005\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 00:56:28,201\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 00:57:48,533\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 01:07:28,366\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 01:07:44,680\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 01:08:03,788\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 01:10:49,098\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 01:21:07,434\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 01:21:47,910\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 01:23:17,665\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 01:25:52,604\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 01:26:02,521\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 01:26:52,239\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 01:29:08,216\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 01:30:34,110\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 01:31:49,080\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 01:32:29,901\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 01:34:05,151\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 01:36:04,155\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '77ade844')}\n",
      "2024-06-05 01:36:20,547\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 01:36:50,469\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 01:38:57,954\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 01:39:09,046\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 01:42:10,475\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 01:43:36,760\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 01:47:14,040\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 01:49:09,666\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 01:49:24,262\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 01:51:26,001\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 01:52:03,126\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 01:56:38,764\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 01:57:15,751\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 02:02:15,384\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 02:02:27,154\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 02:04:51,234\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 02:09:23,656\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 02:09:38,749\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 02:09:42,268\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 02:11:52,956\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 02:13:20,342\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 02:14:27,731\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 02:14:49,989\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 02:15:20,050\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 02:15:30,543\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 02:15:55,608\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 02:18:13,186\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 02:22:04,377\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 02:22:25,349\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 02:26:38,516\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 02:27:53,977\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 02:28:34,990\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 02:29:29,042\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 02:34:28,087\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 02:38:03,983\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 02:40:58,282\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 02:42:18,998\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 02:46:00,135\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 02:46:29,448\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 02:46:51,537\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 02:48:29,417\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 02:50:56,207\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 02:51:10,655\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 02:54:01,903\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 02:54:02,865\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 02:55:22,275\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 02:57:07,242\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 02:58:42,997\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 02:59:14,645\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 03:01:13,331\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 03:04:02,471\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 03:06:17,000\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 03:07:07,827\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 03:09:18,522\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 03:12:09,565\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 03:14:05,792\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '5a92826b')}\n",
      "2024-06-05 03:14:35,322\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 03:20:12,581\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 03:21:30,047\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 03:23:57,549\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 03:27:10,303\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 03:27:19,321\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 03:27:38,196\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 03:30:13,516\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 03:30:14,979\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 03:33:17,447\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 03:34:34,538\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 03:36:49,222\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 03:38:18,983\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 03:39:49,584\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 03:40:13,876\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 03:42:36,421\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 03:44:34,479\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 03:46:23,253\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 03:47:40,189\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 03:51:24,490\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 03:53:26,489\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 03:53:44,773\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 03:55:00,996\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 03:56:49,248\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:01:12,018\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 04:04:14,077\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:06:48,339\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:07:26,685\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:09:04,478\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:12:53,705\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:13:24,109\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:13:47,244\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:16:20,434\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 04:16:27,776\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:19:04,381\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:19:33,232\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:19:51,456\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:19:51,619\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:22:37,571\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '80f16ada')}\n",
      "2024-06-05 04:24:04,029\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:25:37,304\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:25:58,115\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:26:51,643\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:28:41,122\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:29:24,388\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:32:09,858\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:32:54,639\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:35:42,191\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:39:42,992\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:40:53,843\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:42:28,623\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:45:15,276\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:45:58,564\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:48:47,359\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:50:01,354\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:51:58,701\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:53:00,473\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:56:43,550\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:57:06,844\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:58:19,661\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:58:40,078\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 04:59:01,553\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 05:01:51,935\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 05:05:18,968\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 05:06:05,259\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 05:11:24,354\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 05:11:49,595\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 05:15:11,187\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 05:22:53,469\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 05:24:51,488\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 05:28:00,365\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 05:28:17,091\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 05:29:20,459\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 05:31:46,197\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 05:32:15,596\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 05:35:57,978\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 05:37:32,109\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 05:37:57,501\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 05:37:59,133\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 05:40:57,307\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 05:41:06,774\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 05:41:53,634\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 05:44:54,525\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 05:47:49,259\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '34b3c1e4')}\n",
      "2024-06-05 05:49:02,080\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 05:50:36,641\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 05:51:02,544\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 05:51:04,866\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 05:54:05,455\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 05:54:05,554\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 05:54:25,727\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 05:58:23,197\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 06:06:19,353\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 06:07:31,851\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 06:11:04,818\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 06:11:27,455\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 06:15:09,326\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 06:15:10,339\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 06:17:14,995\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 06:17:16,612\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 06:18:16,963\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 06:18:31,480\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 06:18:49,987\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 06:18:53,301\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 06:24:33,598\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 06:28:16,890\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 06:30:21,316\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 06:30:22,265\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 06:30:29,991\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 06:31:05,400\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 06:37:18,494\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 06:37:38,337\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 06:41:18,467\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 06:42:54,762\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 06:43:27,551\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 06:49:02,523\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 06:49:34,756\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 06:50:24,729\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 06:50:44,480\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 06:54:22,680\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 06:55:31,410\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 06:56:33,467\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 06:59:53,421\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '56d80434')}\n",
      "2024-06-05 07:01:01,219\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:01:04,076\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:03:05,896\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:03:30,332\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:07:26,483\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:07:45,864\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:09:03,756\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:09:39,282\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:12:58,609\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:13:29,380\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:14:11,207\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:16:36,939\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:19:59,390\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:23:26,491\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:26:28,927\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:29:16,627\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:29:44,087\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:33:46,845\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:36:29,921\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:37:43,032\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:37:55,091\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:39:09,865\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:40:07,800\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:41:46,022\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:42:23,597\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:42:50,571\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:44:28,563\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:49:36,701\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:49:56,465\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:50:07,290\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:51:04,799\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:52:15,089\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 07:55:26,959\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 07:55:29,355\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 07:55:55,514\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:02:40,478\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:03:23,330\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:05:21,916\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:06:33,987\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '1f98018a')}\n",
      "2024-06-05 08:08:31,382\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:08:34,950\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:09:01,436\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:12:48,872\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:14:28,987\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:14:29,880\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:15:04,146\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:15:42,075\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:15:46,747\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:19:40,152\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:20:26,089\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:21:40,164\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:22:07,787\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:25:53,706\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:27:35,801\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:28:00,405\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:28:51,466\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:31:32,594\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:32:38,566\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:38:58,448\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:40:17,992\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:40:42,692\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:41:14,143\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:41:58,011\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:44:50,990\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:48:20,931\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:51:03,969\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:52:03,732\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:52:26,651\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:54:25,173\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:55:03,167\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:57:04,545\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 08:57:45,413\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 09:01:25,589\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 09:03:16,881\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 09:04:54,356\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 09:07:30,950\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 09:08:07,112\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 09:10:51,641\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 09:15:26,475\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 09:17:10,819\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 09:18:41,386\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 09:20:36,826\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 09:21:13,569\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 09:24:45,703\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 09:27:36,156\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 09:27:41,982\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 09:29:28,158\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 09:31:48,506\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 09:33:03,117\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 09:34:08,405\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 09:37:03,660\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 09:39:51,780\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 09:39:57,560\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 09:41:45,312\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 09:44:55,482\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 09:46:00,497\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '93632d75')}\n",
      "2024-06-05 09:46:47,620\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 09:47:22,826\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 09:50:09,843\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 09:50:54,790\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 09:52:00,905\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 09:52:04,115\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 09:54:07,327\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 09:59:53,013\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 10:00:27,535\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 10:03:18,645\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 10:04:17,693\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 10:06:28,222\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 10:06:54,775\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 10:10:37,334\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 10:12:13,194\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 10:17:08,532\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 10:17:47,140\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 10:18:56,887\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 10:20:10,215\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 10:23:05,602\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 10:23:06,821\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 10:26:20,922\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 10:26:52,140\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 10:31:07,072\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 10:31:20,620\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 10:35:19,695\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 10:36:56,137\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 10:39:53,620\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 10:39:58,241\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 10:43:30,707\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 10:43:35,014\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 10:43:54,522\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 10:44:14,873\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 10:46:30,608\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 10:47:32,875\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 10:52:04,394\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 10:52:32,031\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 10:53:04,759\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 10:55:50,606\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 10:59:37,387\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 10:59:46,711\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 11:00:29,627\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'd0c36a7a')}\n",
      "2024-06-05 11:03:07,286\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 11:04:14,191\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 11:08:04,807\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 11:08:20,594\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 11:09:40,796\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 11:10:30,258\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 11:12:45,108\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 11:13:35,851\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 11:20:22,738\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 11:20:33,053\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 11:23:37,677\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 11:24:14,531\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 11:25:16,156\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 11:25:48,163\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 11:26:43,060\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 11:28:33,880\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 11:32:21,659\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 11:36:11,601\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 11:37:15,406\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 11:37:36,877\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 11:41:13,123\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 11:43:27,322\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 11:46:01,411\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 11:48:20,293\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 11:48:27,873\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 11:52:31,761\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 11:57:04,051\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', 'de5d0d51')}\n",
      "2024-06-05 11:57:04,181\tINFO tune.py:1042 -- Total run time: 60675.92 seconds (60675.77 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_size': 128, 'num_layers': 1, 'dropout': 0.1, 'bidirectional': False, 'learning_rate': 0.01, 'basin': 'boysen_reservoir_inflow', 'test_year': 2006}\n",
      "Best configuration saved to: /data/Hydra_Work/Tuning/Config_Text/boysen_reservoir_inflow_best_config.txt\n"
     ]
    }
   ],
   "source": [
    "# Stehekin gives :True\t0.4\t64\t0.001\t3 Even looking at overall min, and for animas r at durango\n",
    "# T-tests suggests: Bidirectional good, dropout unimportant, 16 bad, 64 vs 128 unimportant. All models that imrpvoed loss wre bidirectional\n",
    "# Libby seemed to want an single layer\n",
    "# San Joaqin is just hard, score of 9.4: {'hidden_size': 64, 'num_layers': 1, 'dropout': 0.4, 'bidirectional': False, 'learning_rate': 1e-05}\n",
    "\n",
    "\n",
    "runs = 12\n",
    "# At weekly:\n",
    "# Animas has {'hidden_size': 128, 'num_layers': 3, 'dropout': 0.1, 'bidirectional': False, 'learning_rate': 1e-05}, 64,3,0.1. Results for 64, 1, 0.1, True identical\n",
    "def objective(config):   \n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.\n",
    "                      is_available() else 'cpu')\n",
    "    \n",
    "    #print('Device available is', device)\n",
    "    \n",
    "\n",
    "    score = train_model(config) # Have training loop in here that outputs loss of model\n",
    "    return {\"val_loss\": score}\n",
    "\n",
    "#basin = 'stehekin_r_at_stehekin'\n",
    "\n",
    "#, search_alg = optuna_search\n",
    "optuna_tune_config = tune.TuneConfig(scheduler=asha_scheduler)\n",
    "tune_config = tune.TuneConfig(scheduler=asha_scheduler)\n",
    "running_tune_config = tune.TuneConfig()\n",
    "\n",
    "run_config=train.RunConfig(stop= plateau_stopper)\n",
    "\n",
    "# Note using < 1gb per run stops pylance from crashing I think\n",
    "# Without Optun\n",
    "tuner = tune.Tuner(tune.with_resources(tune.with_parameters(objective), resources={\"cpu\": 15/runs, \"gpu\": 1/runs}), param_space=config_space, tune_config = tune_config, run_config = run_config) \n",
    "# With Optuna\n",
    "#tuner = tune.Tuner(tune.with_resources(tune.with_parameters(objective), resources={\"cpu\": 1, \"gpu\": 1/16}), param_space = optuna_config_space, tune_config = optuna_tune_config, run_config = run_config) \n",
    "\n",
    "results = tuner.fit()\n",
    "best_config = results.get_best_result(metric=\"val_loss\", mode=\"min\").config\n",
    "print(best_config)\n",
    "\n",
    "\n",
    "\n",
    "# Define the file path where you want to save the best configuration\n",
    "file_path = f\"/data/Hydra_Work/Tuning/Config_Text/{basin}_best_config.txt\"\n",
    "# Open the file in write mode and save the configuration\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(str(best_config))\n",
    "\n",
    "print(\"Best configuration saved to:\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss</th>\n",
       "      <th>config/basin</th>\n",
       "      <th>config/test_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.171296</td>\n",
       "      <td>pecos_r_nr_pecos</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.142887</td>\n",
       "      <td>merced_river_yosemite_at_pohono_bridge</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.295092</td>\n",
       "      <td>fontenelle_reservoir_inflow</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>-0.230011</td>\n",
       "      <td>owyhee_r_bl_owyhee_dam</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>-0.517411</td>\n",
       "      <td>pecos_r_nr_pecos</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>-0.274982</td>\n",
       "      <td>detroit_lake_inflow</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>-0.157572</td>\n",
       "      <td>hungry_horse_reservoir_inflow</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>-0.198643</td>\n",
       "      <td>pecos_r_nr_pecos</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>-0.367187</td>\n",
       "      <td>merced_river_yosemite_at_pohono_bridge</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>-0.403179</td>\n",
       "      <td>animas_r_at_durango</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     val_loss                            config/basin  config/test_year\n",
       "35  -0.171296                        pecos_r_nr_pecos              2000\n",
       "40  -0.142887  merced_river_yosemite_at_pohono_bridge              2000\n",
       "42  -0.295092             fontenelle_reservoir_inflow              2000\n",
       "67  -0.230011                  owyhee_r_bl_owyhee_dam              2002\n",
       "79  -0.517411                        pecos_r_nr_pecos              2002\n",
       "..        ...                                     ...               ...\n",
       "513 -0.274982                     detroit_lake_inflow              2022\n",
       "517 -0.157572           hungry_horse_reservoir_inflow              2022\n",
       "519 -0.198643                        pecos_r_nr_pecos              2022\n",
       "524 -0.367187  merced_river_yosemite_at_pohono_bridge              2022\n",
       "525 -0.403179                     animas_r_at_durango              2022\n",
       "\n",
       "[61 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = results.get_dataframe()\n",
    "results_df[results_df['val_loss'] < -0.1][['val_loss', 'config/basin', 'config/test_year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Safe_Basins = list(results_df[results_df['val_loss'] < -0.05]['config/basin'].values)\n",
    "Retrain_Basins = list(set(basins) - set(Safe_Basins))\n",
    "Retrain_Basins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "results_df = results.get_dataframe()\n",
    "columns_to_drop = ['timestamp', 'checkpoint_dir_name', 'done', 'training_iteration', \n",
    "                   'trial_id', 'date', 'time_this_iter_s', 'time_total_s', 'pid', \n",
    "                   'hostname', 'node_ip', 'time_since_restore', 'iterations_since_restore']\n",
    "\n",
    "# Drop the columns\n",
    "results_df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "val_loss_bidirectional_true = results_df[results_df['config/num_layers'] == 3]['val_loss']\n",
    "val_loss_bidirectional_false = results_df[results_df['config/num_layers'] == 1]['val_loss']\n",
    "\n",
    "# Perform a t-test\n",
    "t_statistic, p_value = stats.ttest_ind(val_loss_bidirectional_true, val_loss_bidirectional_false)\n",
    "\n",
    "# Print the results\n",
    "print(\"T-Statistic:\", t_statistic)\n",
    "print(\"P-Value:\", p_value)\n",
    "\n",
    "# Check if the difference in means is statistically significant\n",
    "alpha = 0.05  # Significance level\n",
    "if p_value < alpha:\n",
    "    print(\"The difference in mean val_loss is statistically significant.\")\n",
    "else:\n",
    "    print(\"The difference in mean val_loss is not statistically significant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading models\n",
    "Tuned_Models = {}\n",
    "for basin in basins:\n",
    "    Tuned_Models[basin] = torch.load(f'/data/Hydra_Work/Post_Rodeo_Work/Tuned_Single_Models/basin.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning General Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "static_size = np.shape(static_variables)[1]\n",
    "forecast_size = np.shape(seasonal_forecasts['american_river_folsom_lake_2000_apr'])[1]\n",
    "History_Fourier_in_forcings = 0 #2*3*(6 - 1)\n",
    "Climate_guess = 0 #3\n",
    "History_Statistics_in_forcings = 0 #5*2\n",
    "\n",
    "forecast_input_size = forecast_size + static_size + History_Fourier_in_forcings + History_Statistics_in_forcings  + Climate_guess + 3\n",
    "output_size, head_hidden_size, head_num_layers =  3, 64, 3\n",
    "hindcast_input_size = 8 #8 for no flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.stopper import TrialPlateauStopper\n",
    "\n",
    "# Fixed parameters\n",
    "total_epochs = 200\n",
    "n_epochs = 1 # Epochs between tests\n",
    "group_lengths = [7] # \n",
    "batch_size = 64\n",
    "copies = 1\n",
    "\n",
    "# parameters to tune\n",
    "# I tuned to 128,2,0.4,False,1e-3 \n",
    "hidden_sizes = [128]\n",
    "num_layers = [1]\n",
    "dropout = [0.1]\n",
    "bidirectional =  [False]\n",
    "learning_rate = [1e-3]\n",
    "\n",
    "config_space = {\n",
    "    \"hidden_size\": tune.grid_search(hidden_sizes),\n",
    "    \"num_layers\": tune.grid_search(num_layers),\n",
    "    \"dropout\": tune.grid_search(dropout),\n",
    "    \"bidirectional\": tune.grid_search(bidirectional),\n",
    "    \"learning_rate\": tune.grid_search(learning_rate),\n",
    "    'test_year': tune.grid_search(list(np.arange(2022,2024,2)) )\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "# Places to save info\n",
    "model_dir = '/data/Hydra_Work/Post_Rodeo_Work/Tuned_General_Model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "    years = list(np.arange(2000,2024,2))\n",
    "    test_year = 2000\n",
    "    val_years = [years[years.index(test_year)-1], years[years.index(test_year)-2]  ]\n",
    "    train_years = [year for year in years if year not in [test_year] + val_years]\n",
    "    \n",
    "    Test_Dates = All_Dates[All_Dates.year == test_year]\n",
    "    Val_Dates = [date for date in All_Dates if date.year in val_years]\n",
    "    Train_Dates = [date for date in All_Dates if date.year in train_years]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_general(config):\n",
    "    \n",
    "    All_Dates = ray.get(All_Dates_id)  \n",
    "    \n",
    "    years = list(np.arange(2000,2024,2))\n",
    "    test_year = config['test_year']\n",
    "    val_years = [years[years.index(test_year)-1], years[years.index(test_year)-2]  ]\n",
    "    train_years = [year for year in years if year not in [test_year] + val_years]\n",
    "    \n",
    "    Test_Dates = All_Dates[All_Dates.year == test_year]\n",
    "    Val_Dates = All_Dates[All_Dates.year.isin(val_years)]\n",
    "    Train_Dates = All_Dates[All_Dates.year.isin(train_years)]\n",
    "    \n",
    "    era5 = ray.get(era5_id)  \n",
    "    daily_flow = ray.get(daily_flow_id)  \n",
    "    climatological_flows = ray.get(climatological_flows_id)\n",
    "    climate_indices = ray.get(climate_indices_id)\n",
    "    seasonal_forecasts = ray.get(seasonal_forecasts_id)\n",
    "    Static_variables = ray.get(Static_variables_id)\n",
    "\n",
    "    copies = 1\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.\n",
    "                    is_available() else 'cpu')\n",
    "    \n",
    "    save_path = f'/data/Hydra_Work/No_Forecast_Validation_Models/{test_year}/General_LSTM_No_Flow_Model/General_LSTM.pth'\n",
    "    loss_path = f'/data/Hydra_Work/No_Forecast_Validation_Models/{test_year}/General_LSTM_No_Flow_Model/General_LSTM_loss.txt'\n",
    "\n",
    "    val_loss = 1000\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(loss_path):\n",
    "        # If the file does not exist, create it and write val_loss to it\n",
    "        with open(loss_path, 'w') as file:\n",
    "            file.write('%f' % val_loss)\n",
    "    \n",
    "  \n",
    "    models, params_to_optimize, optimizers, schedulers = define_models(hindcast_input_size, forecast_input_size,\n",
    "    config[\"hidden_size\"], config[\"num_layers\"], config[\"dropout\"],\n",
    "    config[\"bidirectional\"], config[\"learning_rate\"], copies=copies, device = device)\n",
    "\n",
    "    losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(total_epochs):\n",
    "\n",
    "        train_losses = {}\n",
    "        epoch_val_losses = {}\n",
    "\n",
    "        for copy in range(copies):\n",
    "\n",
    "             # Need to fix the outputs of No_Body_Model_Run\n",
    "            train_losses[copy], Climate_Loss = No_Body_Model_Run(Train_Dates, basins, models[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts,\n",
    "                Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper= None, n_epochs=n_epochs,\n",
    "                batch_size=batch_size, group_lengths=group_lengths, Train_Mode=True, device=device, specialised=False)\n",
    "            epoch_val_losses[copy], Climate_Loss = No_Body_Model_Run(Val_Dates, basins, models[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts,\n",
    "                Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper= None, n_epochs=n_epochs,\n",
    "                batch_size=batch_size, group_lengths=group_lengths, Train_Mode=False, device=device, specialised=False)\n",
    "\n",
    "        loss = np.mean(list(train_losses.values())) - Climate_Loss\n",
    "\n",
    "\n",
    "        candidate_val_loss = ((np.mean(list(epoch_val_losses.values())).mean() - Climate_Loss)[0])/np.mean(Climate_Loss)\n",
    "        val_loss = np.min([val_loss, candidate_val_loss ])\n",
    "        \n",
    "        # Check best loss so far for this model\n",
    "        with open(loss_path, 'r') as file:\n",
    "            # Read the entire contents of the file\n",
    "            Overall_Best_Val_Loss = float(file.read())\n",
    "\n",
    "        if val_loss < Overall_Best_Val_Loss:\n",
    "            torch.save(models[0], save_path)\n",
    "\n",
    "            with open(loss_path, 'w') as f:\n",
    "                f.write('%f' % val_loss)\n",
    "\n",
    "            \n",
    "               \n",
    "        ray.train.report({'val_loss' : val_loss})\n",
    "\n",
    "        losses.append(loss)\n",
    "        val_losses.append(candidate_val_loss)\n",
    "        #print(val_losses)\n",
    "        #print(candidate_val_loss)\n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 11:18:14,293\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "from ray import train, tune\n",
    "\n",
    "\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(runtime_env = { \"env_vars\":   {\"PYTHONPATH\": '/data/Hydra_Work/Competition_Functions/' } } )\n",
    "         \n",
    "All_Dates_id = ray.put(All_Dates)  \n",
    "Val_Dates_id = ray.put(Val_Dates)  \n",
    "era5_id = ray.put(era5)  \n",
    "daily_flow_id = ray.put(daily_flow)  \n",
    "climatological_flows_id = ray.put(climatological_flows)\n",
    "climate_indices_id = ray.put(climate_indices)\n",
    "seasonal_forecasts_id = ray.put(seasonal_forecasts)\n",
    "Static_variables_id = ray.put(static_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asha_scheduler = ASHAScheduler(\n",
    "#     time_attr='training_iteration',\n",
    "#     metric='val_loss',\n",
    "#     mode='min',\n",
    "#     max_t=100,\n",
    "#     grace_period=20,\n",
    "#     reduction_factor=2,\n",
    "#     brackets=1,\n",
    "# )\n",
    "\n",
    "\n",
    "plateau_stopper = TrialPlateauStopper(\n",
    "    metric=\"val_loss\",\n",
    "    num_results = 300,\n",
    "    grace_period=80,\n",
    "    mode=\"min\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-06-06 11:54:18</td></tr>\n",
       "<tr><td>Running for: </td><td>00:36:02.36        </td></tr>\n",
       "<tr><td>Memory:      </td><td>38.4/125.9 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 10.0/16 CPUs, 0.6666666666666666/1 GPUs (0.0/1.0 accelerator_type:A100D)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc                   </th><th>bidirectional  </th><th style=\"text-align: right;\">  dropout</th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  num_layers</th><th style=\"text-align: right;\">  test_year</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  val_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_78318_00000</td><td>RUNNING   </td><td>136.156.133.98:1011212</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">   189</td><td style=\"text-align: right;\">         2149.45</td><td style=\"text-align: right;\"> 0.358954 </td></tr>\n",
       "<tr><td>objective_78318_00002</td><td>RUNNING   </td><td>136.156.133.98:1011214</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">         0.0001</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">   189</td><td style=\"text-align: right;\">         2151.44</td><td style=\"text-align: right;\"> 0.0858477</td></tr>\n",
       "<tr><td>objective_78318_00001</td><td>TERMINATED</td><td>136.156.133.98:1011213</td><td>False          </td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       2022</td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">         1852.67</td><td style=\"text-align: right;\"> 0.0767424</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=1011213)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "\u001b[36m(objective pid=1011213)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 11:49:12,313\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'test_year': ('__ref_ph', '52467d93')}\n"
     ]
    }
   ],
   "source": [
    "# {'hidden_size': 256, 'num_layers': 3, 'dropout': 0.1, 'bidirectional': True, 'learning_rate': 0.001}\n",
    "# 7 Days:  128\t2\t0.4\tFalse\t0.001\n",
    "def objective(config):  \n",
    "    device = torch.device('cuda' if torch.cuda.\n",
    "                      is_available() else 'cpu')\n",
    "    \n",
    "    #print('Device available is', device)\n",
    "    \n",
    "\n",
    "    score = train_model_general(config) # Have training loop in here that outputs loss of model\n",
    "    return {\"val_loss\": score}\n",
    "\n",
    "\n",
    "#, search_alg = optuna_search\n",
    "# optuna_tune_config = tune.TuneConfig(scheduler=asha_scheduler)\n",
    "# tune_config = tune.TuneConfig(scheduler=asha_scheduler)\n",
    "run_config=train.RunConfig(stop= plateau_stopper)\n",
    "\n",
    "runs = 3\n",
    "# Without Optuna\n",
    "tuner = tune.Tuner(tune.with_resources(tune.with_parameters(objective), resources={\"cpu\": 15/runs , \"gpu\": 1/runs }), param_space=config_space, run_config = run_config) \n",
    "# With Optuna\n",
    "#tuner = tune.Tuner(tune.with_resources(tune.with_parameters(objective), resources={\"cpu\": 1, \"gpu\": 1/16}), param_space = optuna_config_space, tune_config = optuna_tune_config, run_config = run_config) \n",
    "\n",
    "results = tuner.fit()\n",
    "# try get_best_checkpoint, or change val to be maximum of current val_loss and previous ones\n",
    "best_config = results.get_best_result(metric=\"val_loss\", mode=\"min\").config\n",
    "print(best_config)\n",
    "file_path = f\"/data/Hydra_Work/Tuning/Config_Text/General_Model_best_config.txt\"\n",
    "\n",
    "# Open the file in write mode and save the configuration\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(str(best_config))\n",
    "\n",
    "print(\"Best configuration saved to:\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results.get_dataframe()\n",
    "results_df[results_df['val_loss'] < -0.15] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "General_Model = torch.load('/data/Hydra_Work/Post_Rodeo_Work/Tuned_General_Model/General_model.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Hydra Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_models_hydra(body_hindcast_input_size, body_forecast_input_size, body_output_size, body_hidden_size, body_num_layers, body_dropout,\n",
    "                        head_hidden_size, head_num_layers, head_forecast_output_size, head_dropout, bidirectional, basins,\n",
    "                        learning_rate_general_head, learning_rate_head, learning_rate_body, LR = 1e-3, \n",
    "                        additional_specific_head_hindcast_input_size = 1, additional_specific_head_forecast_input_size = 0,\n",
    "                        copies=1, device=None):\n",
    "    Hydra_Bodys = {}\n",
    "    Basin_Heads = {}\n",
    "    General_Heads = {}   \n",
    "    general_optimizers = {}\n",
    "    optimizers = {}\n",
    "    schedulers = {}\n",
    "    \n",
    "    body_forecast_output_size = body_output_size\n",
    "    body_hindcast_output_size = body_output_size\n",
    "    \n",
    "    # Define head hindcast size as head-forecast for simplicty\n",
    "    head_hindcast_output_size = head_forecast_output_size\n",
    "    specific_head_hindcast_output_size = head_forecast_output_size\n",
    "    specific_head_forecast_output_size = head_forecast_output_size\n",
    "    specific_head_hidden_size = head_hidden_size\n",
    "    specific_head_num_layers = head_num_layers\n",
    "    \n",
    "    # Head takes Body as inputs\n",
    "    #head_hindcast_input_size = body_hindcast_input_size \n",
    "    head_hindcast_input_size = body_hindcast_output_size\n",
    "    head_forecast_input_size = body_forecast_output_size\n",
    "    \n",
    "    # Specific input size\n",
    "    specific_head_hindcast_input_size = head_hindcast_input_size + additional_specific_head_hindcast_input_size\n",
    "    specific_head_forecast_input_size = head_forecast_input_size + additional_specific_head_forecast_input_size\n",
    "    \n",
    "    for copy in range(copies):\n",
    "        Hydra_Bodys[copy] = Google_Model_Block(body_hindcast_input_size, body_forecast_input_size, body_hindcast_output_size, body_forecast_output_size, body_hidden_size, body_num_layers, device, body_dropout, bidirectional)\n",
    "        General_Heads[copy] = Google_Model_Block(head_hindcast_input_size, head_forecast_input_size, head_hindcast_output_size, head_forecast_output_size, head_hidden_size, head_num_layers, device, head_dropout, bidirectional)\n",
    "        Basin_Heads[copy] = Specific_Heads(basins, specific_head_hindcast_input_size, specific_head_forecast_input_size, specific_head_hindcast_output_size, specific_head_forecast_output_size, specific_head_hidden_size, specific_head_num_layers, device, head_dropout, bidirectional)\n",
    "\n",
    "\n",
    "        specific_head_parameters = list()\n",
    "        for basin, model in Basin_Heads[copy].items():\n",
    "            specific_head_parameters += list(model.parameters())\n",
    "\n",
    "        optimizers[copy] = torch.optim.Adam(\n",
    "        # Extra LR is the global learning rate, not really important\n",
    "        [\n",
    "            {\"params\": General_Heads[copy].parameters(), \"lr\": learning_rate_general_head},\n",
    "            {\"params\": specific_head_parameters, \"lr\": learning_rate_head},\n",
    "            {\"params\": Hydra_Bodys[copy].parameters(), \"lr\": learning_rate_body},\n",
    "        ],\n",
    "        lr=LR, weight_decay = 1e-4) #1e-4 good so far, 3 not so food\n",
    "\n",
    "        general_optimizers[copy] = torch.optim.Adam(\n",
    "        # Extra LR is the global learning rate, not really important\n",
    "        [\n",
    "            {\"params\": General_Heads[copy].parameters(), \"lr\": learning_rate_general_head},\n",
    "            {\"params\": Hydra_Bodys[copy].parameters(), \"lr\": learning_rate_body},\n",
    "        ],\n",
    "        lr=LR, )\n",
    "        schedulers[copy] = lr_scheduler.StepLR(optimizers[copy], 1, gamma=0.99) #.CosineAnnealingLR(optimizers[copy], T_max= 100000, eta_min= 1e-4,)\n",
    "         #\n",
    "        \n",
    "    return Hydra_Bodys, General_Heads, Basin_Heads, optimizers, schedulers, general_optimizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "static_size = np.shape(static_variables)[1]\n",
    "forecast_size = np.shape(seasonal_forecasts['american_river_folsom_lake_2000_apr'])[1]\n",
    "History_Fourier_in_forcings = 0 #2*3*(6 - 1)\n",
    "Climate_guess = 0 #3\n",
    "History_Statistics_in_forcings = 0 # 5*2\n",
    "\n",
    "forecast_input_size = forecast_size + static_size + History_Fourier_in_forcings + History_Statistics_in_forcings  + Climate_guess + 3\n",
    "output_size, head_hidden_size, head_num_layers =  3, 64, 3\n",
    "body_hindcast_input_size = 8\n",
    "body_forecast_input_size = forecast_input_size\n",
    "\n",
    "\n",
    "Overall_Best_Val_Loss = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.stopper import TrialPlateauStopper\n",
    "\n",
    "# Fixed parameters\n",
    "total_epochs = 300\n",
    "n_epochs = 1 # Epochs between tests\n",
    "group_lengths = [7] #np.arange(180)\n",
    "\n",
    "copies = 1\n",
    "head_output_size = 3\n",
    "\n",
    "# parameters to tune\n",
    "# chose 128, 2, 0.1, 1e-3, 6, 32, 1, 0.4, 1e-3\n",
    "body_hidden_sizes =  [128]\n",
    "body_num_layers = [1]\n",
    "body_dropouts = [0.0] #[0.1, 0.4]\n",
    "body_learning_rates = [1e-4] \n",
    "body_outputs = [4, 6] # Say hindcast and forecasts have same outputrs body_hindcast_output_size\n",
    "\n",
    "\n",
    "head_hidden_sizes = [32]\n",
    "head_num_layers = [1]\n",
    "head_dropouts = [0.0] #[0.1, 0.4, 0.7]\n",
    "head_learning_rates = [1e-2, 1e-4]\n",
    "batch_size = [256]\n",
    "LR = 1e-3\n",
    "bidirectionals = [False]\n",
    "spec_multiplier = [1]\n",
    "\n",
    "config_space = {\n",
    "    \"body_hidden_size\": tune.grid_search(body_hidden_sizes),\n",
    "    \"body_num_layer\": tune.grid_search(body_num_layers),\n",
    "    \"body_dropout\": tune.grid_search(body_dropouts),\n",
    "    \"bidirectional\": tune.grid_search(bidirectionals),\n",
    "    \"body_output\": tune.grid_search(body_outputs),\n",
    "    \"body_learning_rate\": tune.grid_search(body_learning_rates),\n",
    "    \"head_hidden_size\": tune.grid_search(head_hidden_sizes),\n",
    "    \"head_num_layer\": tune.grid_search(head_num_layers),\n",
    "    \"head_dropout\": tune.grid_search(head_dropouts),\n",
    "    \"head_learning_rate\": tune.grid_search(head_learning_rates),\n",
    "    \"spec_multiplier\": tune.grid_search(spec_multiplier),\n",
    "    'batch_size': tune.grid_search(batch_size),\n",
    "    'test_year': tune.grid_search([2010] ) #list(np.arange(2000,2024,2))\n",
    "\n",
    "    #\"general_head_learning_rate\": tune.grid_search(head_learning_rates),\n",
    "}\n",
    "\n",
    "# Places to save info\n",
    "model_dir = '/data/Hydra_Work/Post_Rodeo_Work/Tuned_Hydra_Model/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_hydra(config):\n",
    "\n",
    "    All_Dates = ray.get(All_Dates_id)  \n",
    "    \n",
    "    \n",
    "    years = list(np.arange(2000,2024,2))\n",
    "    test_year = config['test_year']\n",
    "    val_years = [years[years.index(test_year)-1], years[years.index(test_year)-2]  ]\n",
    "    train_years = [year for year in years if year not in [test_year] + val_years]\n",
    "    \n",
    "    Test_Dates = All_Dates[All_Dates.year == test_year]\n",
    "    Val_Dates = All_Dates[All_Dates.year.isin(val_years)]\n",
    "    Train_Dates = All_Dates[All_Dates.year.isin(train_years)]\n",
    "\n",
    "    \n",
    "    era5 = ray.get(era5_id)  \n",
    "    daily_flow = ray.get(daily_flow_id)  \n",
    "    climatological_flows = ray.get(climatological_flows_id)\n",
    "    climate_indices = ray.get(climate_indices_id)\n",
    "    seasonal_forecasts = ray.get(seasonal_forecasts_id)\n",
    "    Static_variables = ray.get(Static_variables_id)  \n",
    "  \n",
    "                        \n",
    "    body_save_path = f'/data/Hydra_Work/No_Forecast_Validation_Models/{test_year}/General_Head_Model/Hydra_Body_LSTM.pth'\n",
    "    head_save_path = f'/data/Hydra_Work/No_Forecast_Validation_Models/{test_year}/General_Head_Model/Hydra_Head_LSTM.pth'\n",
    "    basin_heads_save_path = f'/data/Hydra_Work/No_Forecast_Validation_Models/{test_year}/Basin_Head_Model'\n",
    "    \n",
    "    loss_path = f'/data/Hydra_Work/No_Forecast_Validation_Models/{test_year}/General_Head_Model/Hydra_LSTM_loss.txt'\n",
    "\n",
    "    val_loss = 1000\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(loss_path):\n",
    "        # If the file does not exist, create it and write val_loss to it\n",
    "        with open(loss_path, 'w') as file:\n",
    "            file.write('%f' % val_loss)\n",
    "\n",
    "\n",
    "    copies = 1\n",
    "    warmup = 4\n",
    "    best_val_loss = 999\n",
    "    device = torch.device('cuda' if torch.cuda.\n",
    "                    is_available() else 'cpu')\n",
    "   \n",
    "\n",
    "    general_head_learning_rate = config['body_learning_rate']\n",
    "    Hydra_Bodys, General_Hydra_Heads, model_heads, optimizers, schedulers, general_optimizers  = define_models_hydra(body_hindcast_input_size, body_forecast_input_size, config['body_output'],\n",
    "                                config['body_hidden_size'], config['body_num_layer'], config['body_dropout'], \n",
    "                                config['head_hidden_size'], config['head_num_layer'], 3, config['head_dropout'], config['bidirectional'], basins,\n",
    "                                general_head_learning_rate, config['head_learning_rate'], config['body_learning_rate'], LR, device = device\n",
    "                                )\n",
    "     \n",
    "\n",
    "    batch_size = config['batch_size']\n",
    "                                                \n",
    "    general_losses, specific_losses, general_val_losses, specific_val_losses, val_losses = [], [], [], [], []\n",
    "\n",
    "    # Initialise, with dummy scheduler\n",
    "    for copy in range(copies):\n",
    "        # Initialise\n",
    "        dummy_scheduler = lr_scheduler.StepLR(optimizers[copy],step_size = warmup, gamma = 0.8)\n",
    "\n",
    "        Model_Run(All_Dates, basins, Hydra_Bodys[copy], General_Hydra_Heads[copy], model_heads[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts,\n",
    "            Static_variables, general_optimizers[copy], dummy_scheduler, criterion, early_stopper= None, n_epochs= warmup,\n",
    "            batch_size=batch_size, group_lengths=group_lengths, Train_Mode=True, device=device, feed_forcing = False)\n",
    "\n",
    "            \n",
    "    for epoch in range(total_epochs):\n",
    "        train_general_losses = {}\n",
    "        train_specific_losses = {}\n",
    "        train_climate_losses = {}\n",
    "        epoch_val_general_losses = {}\n",
    "        epoch_val_specific_losses = {}\n",
    "        climate_losses = {}\n",
    "        \n",
    "        for copy in range(copies):\n",
    "                        \n",
    "\n",
    "            # Full Training\n",
    "            train_general_losses[copy], train_specific_losses[copy], train_climate_losses[copy] = Model_Run(Train_Dates, basins, Hydra_Bodys[copy], General_Hydra_Heads[copy], model_heads[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts,\n",
    "                Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper= None, n_epochs= n_epochs,\n",
    "                batch_size=batch_size, group_lengths=group_lengths, Train_Mode=True, device=device, feed_forcing = False, spec_multiplier = config[\"spec_multiplier\"])\n",
    "            epoch_val_general_losses[copy], epoch_val_specific_losses[copy], climate_losses[copy] = Model_Run(Val_Dates, basins, Hydra_Bodys[copy], General_Hydra_Heads[copy], model_heads[copy], era5, daily_flow, climatological_flows, climate_indices, seasonal_forecasts,\n",
    "                Static_variables, optimizers[copy], schedulers[copy], criterion, early_stopper= None, n_epochs= n_epochs,\n",
    "                batch_size=batch_size, group_lengths=group_lengths, Train_Mode=False, device=device, feed_forcing = False)\n",
    "\n",
    "        general_loss = np.mean(list(train_general_losses.values()))\n",
    "        specific_loss = np.mean(list(train_specific_losses.values()))\n",
    "        train_climate_loss = np.mean(list(train_climate_losses.values()))\n",
    "        climate_loss = np.mean(list(climate_losses.values()))\n",
    "        \n",
    "        epoch_val_general_loss = np.mean(list(epoch_val_general_losses.values())).mean()\n",
    "        epoch_val_specific_loss = np.mean(list(epoch_val_specific_losses.values())).mean()\n",
    "        \n",
    "        \n",
    "        general_losses.append(general_loss)\n",
    "        specific_losses.append(specific_loss)\n",
    "        specific_val_losses.append(epoch_val_specific_loss)\n",
    "        general_val_losses.append(epoch_val_general_loss)\n",
    "\n",
    "        val_loss = 0.5*(epoch_val_general_loss + epoch_val_specific_loss)\n",
    "        candidate_val_loss = ((val_loss.mean() - climate_loss))/np.mean(climate_loss)\n",
    "        best_val_loss = np.min([best_val_loss, candidate_val_loss ])\n",
    "         \n",
    "        with open(loss_path, 'r') as file:\n",
    "            # Read the entire contents of the file\n",
    "            Overall_Best_Val_Loss = float(file.read())\n",
    "\n",
    "        if best_val_loss < Overall_Best_Val_Loss:\n",
    "            with open(loss_path, 'w') as f:\n",
    "                f.write('%f' % best_val_loss)\n",
    "\n",
    "            torch.save(Hydra_Bodys[0], body_save_path)\n",
    "            torch.save(General_Hydra_Heads[0], head_save_path)\n",
    "            for basin in basins:\n",
    "                torch.save(model_heads[0][basin], f\"{basin_heads_save_path}/{basin}.path\")\n",
    "                \n",
    "            \n",
    "               \n",
    "        ray.train.report({'val_loss' : best_val_loss})\n",
    "        #print('Validation Loss', val_losses)\n",
    "        #print('Training Loss', general_losses/train_climate_loss )\n",
    "        print(candidate_val_loss)\n",
    "        val_losses.append(candidate_val_loss)\n",
    "\n",
    "    return best_val_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 07:33:55,008\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "from ray import train, tune\n",
    "\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(runtime_env = { \"env_vars\":   {\"PYTHONPATH\": '/data/Hydra_Work/Competition_Functions/' } } )\n",
    "         \n",
    "All_Dates_id = ray.put(All_Dates)  \n",
    "era5_id = ray.put(era5)  \n",
    "daily_flow_id = ray.put(daily_flow)  \n",
    "climatological_flows_id = ray.put(climatological_flows)\n",
    "climate_indices_id = ray.put(climate_indices)\n",
    "seasonal_forecasts_id = ray.put(seasonal_forecasts)\n",
    "Static_variables_id = ray.put(static_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asha_scheduler = ASHAScheduler(\n",
    "    time_attr='training_iteration',\n",
    "    metric='val_loss',\n",
    "    mode='min',\n",
    "    max_t=100,\n",
    "    grace_period=20,\n",
    "    reduction_factor=3,\n",
    "    brackets=1,\n",
    ")\n",
    "\n",
    "\n",
    "plateau_stopper = TrialPlateauStopper(\n",
    "    metric=\"val_loss\",\n",
    "    num_results = 800,\n",
    "    grace_period=20,\n",
    "    mode=\"min\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-06-06 07:37:58</td></tr>\n",
       "<tr><td>Running for: </td><td>00:03:53.43        </td></tr>\n",
       "<tr><td>Memory:      </td><td>42.3/125.9 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 15.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A100D)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  batch_size</th><th>bidirectional  </th><th style=\"text-align: right;\">  body_dropout</th><th style=\"text-align: right;\">  body_hidden_size</th><th style=\"text-align: right;\">  body_learning_rate</th><th style=\"text-align: right;\">  body_num_layer</th><th style=\"text-align: right;\">  body_output</th><th style=\"text-align: right;\">  head_dropout</th><th style=\"text-align: right;\">  head_hidden_size</th><th style=\"text-align: right;\">  head_learning_rate</th><th style=\"text-align: right;\">  head_num_layer</th><th style=\"text-align: right;\">  spec_multiplier</th><th style=\"text-align: right;\">  test_year</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  val_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_26ce5_00000</td><td>RUNNING </td><td>136.156.133.98:996294</td><td style=\"text-align: right;\">         256</td><td>False          </td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">               128</td><td style=\"text-align: right;\">              0.0001</td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">            4</td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">                32</td><td style=\"text-align: right;\">              0.1   </td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         224.87 </td><td style=\"text-align: right;\">  0.20507 </td></tr>\n",
       "<tr><td>objective_26ce5_00001</td><td>RUNNING </td><td>136.156.133.98:996295</td><td style=\"text-align: right;\">         256</td><td>False          </td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">               128</td><td style=\"text-align: right;\">              0.0001</td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">            4</td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">                32</td><td style=\"text-align: right;\">              0.01  </td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         224.531</td><td style=\"text-align: right;\">  0.255037</td></tr>\n",
       "<tr><td>objective_26ce5_00002</td><td>RUNNING </td><td>136.156.133.98:996296</td><td style=\"text-align: right;\">         256</td><td>False          </td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">               128</td><td style=\"text-align: right;\">              0.0001</td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">            4</td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">                32</td><td style=\"text-align: right;\">              0.0001</td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">       2010</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         224.891</td><td style=\"text-align: right;\">  0.212736</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=996294)\u001b[0m /home/gbmc/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "\u001b[36m(objective pid=996294)\u001b[0m   warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(objective pid=996294)\u001b[0m 0.9528351140947744\n",
      "\u001b[36m(objective pid=996294)\u001b[0m 2.7010988372894116\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=996294)\u001b[0m 0.20506976474633834\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=996294)\u001b[0m 0.4268740338164566\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=996294)\u001b[0m 1.8464422703766754\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=996294)\u001b[0m 0.5746116993298312\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=996294)\u001b[0m 0.8285865921395753\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=996294)\u001b[0m 1.0315079504550688\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=996294)\u001b[0m 0.21510097065456504\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=996294)\u001b[0m 2.829953433962464\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=996294)\u001b[0m 1.3664495013943334\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=996294)\u001b[0m 1.3245506511754634\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=996294)\u001b[0m 0.4505570774690395\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=996294)\u001b[0m 1.6656495727761906\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(objective pid=996294)\u001b[0m 0.8459236524229086\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 07:37:58,211\tWARNING tune.py:186 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2024-06-06 07:38:04,734\tINFO tune.py:1042 -- Total run time: 239.97 seconds (233.43 seconds for the tuning loop).\n",
      "2024-06-06 07:38:04,735\tWARNING tune.py:1057 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: Tuner.restore(path=\"/home/gbmc/ray_results/objective_2024-06-06_07-34-04\", trainable=...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'body_hidden_size': 128, 'body_num_layer': 1, 'body_dropout': 0.0, 'bidirectional': False, 'body_output': 4, 'body_learning_rate': 0.0001, 'head_hidden_size': 32, 'head_num_layer': 1, 'head_dropout': 0.0, 'head_learning_rate': 0.1, 'spec_multiplier': 1, 'batch_size': 256, 'test_year': 2010}\n",
      "Best configuration saved to: /data/Hydra_Work/Tuning/Config_Text/Hydral_Model_best_config.txt\n"
     ]
    }
   ],
   "source": [
    "runs_per_iteration = 4\n",
    "def objective(config):  \n",
    "    device = torch.device('cuda' if torch.cuda.\n",
    "                      is_available() else 'cpu')\n",
    "    \n",
    "\n",
    "    score = train_model_hydra(config) # Have training loop in here that outputs loss of model\n",
    "    return {\"val_loss\": score}\n",
    "\n",
    "\n",
    "run_config=train.RunConfig(stop= plateau_stopper)\n",
    "# Can use fractions of GPU\n",
    "tuner = tune.Tuner(tune.with_resources(tune.with_parameters(objective), resources={\"cpu\": 15/runs_per_iteration, \"gpu\": 1/(runs_per_iteration)}), param_space=config_space, run_config = run_config) \n",
    "\n",
    "results = tuner.fit()\n",
    "best_config = results.get_best_result(metric=\"val_loss\", mode=\"min\").config\n",
    "print(best_config)\n",
    "file_path = f\"/data/Hydra_Work/Tuning/Config_Text/Hydral_Model_best_config.txt\"\n",
    "\n",
    "# Open the file in write mode and save the configuration\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(str(best_config))\n",
    "\n",
    "print(\"Best configuration saved to:\", file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results.get_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results_df[\u001b[43mresults_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.1\u001b[39m]\u001b[38;5;66;03m#[['val_loss', 'config/body_output']]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/pandas/core/frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniforge3/envs/Hydra_Code/lib/python3.11/site-packages/pandas/core/indexes/range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "results_df[results_df['val_loss'] < -0.1]#[['val_loss', 'config/body_output']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hydra_Code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
